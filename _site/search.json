[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "website",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Beck’s research focuses around the question of what personality is. Definitions have big consequences for how we measure personality, what those measures predict both short- and long-term, how personality is thought to change, and more. One way of doing this is to focus on different levels of aggregation. Thus, she studies how to understand the personality of an individual relative to only themself, relative to some others, and relative to all others.\nTo do so, she uses a mix of methods, including experience sampling methods, passive sensing, survey data, panel data, cognitive tests, and more measured across time intervals from moments to years along with an array of statistical approaches, including time series analysis, multilevel / hierarchical modeling, machine learning, network psychometrics, structural equation modeling, and more. For example, Dr. Beck has been working to build personalized machine learning prediction of behaviors, experiences, and more, finding that we can predict behaviors and experiences better when we don’t assume that people have the same antecedents of the behaviors and experiences. Instead, people have unique antecedents, which could have consequences for how to change or intervene upon behaviors and experiences.\nIn other work, Dr. Beck uses longitudinal panel data across multiple continents to answer questions about what personality traits predict over time. For example, she recently examined personality trait and well-being predictors of later dementia diagnoses and neuropathology measures after death, finding that personality traits are strong predictors of dementia diagnosis but have a much more complex relationship with neuropathology measures."
  },
  {
    "objectID": "about.html#dr.-emorie-d.-beck-phd",
    "href": "about.html#dr.-emorie-d.-beck-phd",
    "title": "About",
    "section": "Dr. Emorie D. Beck, PhD",
    "text": "Dr. Emorie D. Beck, PhD\nDr. Beck’s research focuses around the question of what personality is. Definitions have big consequences for how we measure personality, what those measures predict both short- and long-term, how personality is thought to change, and more. One way of doing this is to focus on different levels of aggregation. Thus, she studies how to understand the personality of an individual relative to only themself, relative to some others, and relative to all others.\nTo do so, she uses a mix of methods, including experience sampling methods, passive sensing, survey data, panel data, cognitive tests, and more measured across time intervals from moments to years along with an array of statistical approaches, including time series analysis, multilevel / hierarchical modeling, machine learning, network psychometrics, structural equation modeling, and more. For example, Dr. Beck has been working to build personalized machine learning prediction of behaviors, experiences, and more, finding that we can predict behaviors and experiences better when we don’t assume that people have the same antecedents of the behaviors and experiences. Instead, people have unique antecedents, which could have consequences for how to change or intervene upon behaviors and experiences.\nIn other work, Dr. Beck uses longitudinal panel data across multiple continents to answer questions about what personality traits predict over time. For example, she recently examined personality trait and well-being predictors of later dementia diagnoses and neuropathology measures after death, finding that personality traits are strong predictors of dementia diagnosis but have a much more complex relationship with neuropathology measures."
  },
  {
    "objectID": "index.html#dr.-emorie-d.-beck-phd",
    "href": "index.html#dr.-emorie-d.-beck-phd",
    "title": "Emorie D. Beck, PhD",
    "section": "Dr. Emorie D. Beck, PhD",
    "text": "Dr. Emorie D. Beck, PhD\n\nAssistant Professor, Department of Psychology\n\nUniversity of California, Davis\nDr. Beck’s research focuses around the question of what personality is. Definitions have big consequences for how we measure personality, what those measures predict both short- and long-term, how personality is thought to change, and more. One way of doing this is to focus on different levels of aggregation. Thus, she studies how to understand the personality of an individual relative to only themself, relative to some others, and relative to all others."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "PSC 162 - Introduction to Personality\nThis one-quarter course introduces the scientific study of personality. Personality refers to the relatively enduring patterns in people’s thoughts, feelings, and behaviors over time and across situations. The course will involve a combination of lectures, exams, in-class activities, and a written assignment.\n\nWinter 2023, UC Davis\n\n\n\nData Visualization in R\nIn scholarly writing, a figure can be worth 1000 words. Data visualization is a key part of the scientific enterprise, yet most students are taught only a small range of visualizations that are most frequently used in their fields and for the types of methods they use. Such standard procedure can limit the reach and scope of scientific work in an era of rapid digital technological innovation. This course will be designed around themes rather than types of visualization. The beginning of the course will cover best practices in data visualization from the perspective of a growing literature on cognitive perceptions of data visualization and will orient students to visualizing data in R using ggplot2. Next, we will cover several broad topic areas, including visualizing probability, differences, and uncertainty. We will conclude with interactive and animated graphics in R and Shiny. Students are highly encouraged to bring their own data from ongoing or completed projects, but this is not required.\n\nFall 2022, UC Davis\n\n\n\nData Management and Wrangling in R\nComing soon!\n\nFall 2023, UC Davis\n\n\n\nCurrent Topics in Personality\nEach term, we read a series of thematically related papers and discuss them. These topics may range from statistical methods to theoretical topics and papers, past and present. The goal of these discussions is to diversify our understandings of what personality is, how we measure it, and more. Where possible, terms end with project proposals related to the term’s thematic theme.\n\nFall 2022, UC Davis\n\nWinter 2023, UC Davis"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Cardinal States\nThe study aims to understand how individual lives unfold in everyday life. Much work on understanding personality involves taking a standard battery of questions and administering them to participants one time. More recently, personality researchers have turned to the experience sampling method in order to capture small slices of individuals’ daily behaviors and experiences. To do so, researchers generally take the same standard batteries meant to describe personality in general and change the wording to make sense in daily life (e.g., “I am often outgoing” becomes “I am outgoing”). However, participants frequently report that (1) these items aren’t applicable in their daily lives or (2) fail to assess things that are actually applicable to their daily lives. This study aims to understand which and how unique experiences unfold in participants’ everyday life.\nData collection for this study are ongoing. The preregistration of the data collection can be found here:\nAim 1: Determining unique content of daily life Participants will be prompted to describe their typical thoughts, feelings, behaviors, and experiences. Then, from this, they’ll be asked to generate a list of 15-20 things (or cardinal states) they think are most important to be assessed to understand their everyday experiences.\nAim 2: The utility of unique content of daily life Participants uniquely provided items will be provided to them in experience sampling method surveys sent to them via text message five times per day for three weeks. In addition, we will have them respond to standard personality batteries and report contextual information (e.g., in class).\nPlanned studies  Separate pre-registrations linked to the study pre-registration will be submitted for each: - Item content of unique items - Predicting outcomes (e.g., loneliness, procrastination, studying) using unique v shared items - Psychometric properties of unique items - Association among emotions, emotion regulation strategies, and emotion regulation goals using change as outcome or other systems models - Using people’s free response as folk theories of their own behavior and testing their accuracy - Do person-specific models out-predict hybrid and group-level models in predicting momentary behaviors / experiences\n\n\nMomentary Cognitive Function"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Use the left-hand navigation bar to peruse R tutorials I have written."
  },
  {
    "objectID": "index.html#assistant-professor",
    "href": "index.html#assistant-professor",
    "title": "Dr. Emorie D. Beck, PhD",
    "section": "Assistant Professor",
    "text": "Assistant Professor\n\nDepartment of Psychology\n\nUniversity of California, Davis\nDr. Beck’s research focuses around the question of what personality is. Definitions have big consequences for how we measure personality, what those measures predict both short- and long-term, how personality is thought to change, and more. One way of doing this is to focus on different levels of aggregation. Thus, she studies how to understand the personality of an individual relative to only themself, relative to some others, and relative to all others."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "*Beck, E. D. and *Christensen, A. P. (2021). Understanding personality as a dynamic cybernetic system (of systems). European Journal of Personality. Manuscript under review.\n*Jayawickreme, E., *Fleeson, W., *Beck, E. D., *Baumert, A., and *Adler, J. (2021). Personality dynamics. Personality Science. 2\nBeck, E. D. and Jackson, J. J. (2020a). Idiographic traits: A return to Allportian approaches to personality. Current Directions in Psychological Science, 29(3):301–308.\nNosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T., Mellor, D., and Vazire, S. (2019). Preregistation is hard, and worthwhile. Trends in Cognitive Science."
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2023",
    "text": "2023\nGraham, E. K., Beck, E. D., Ong, A., Jackson, K., Willroth, E. C., and Mroczek, D. K. (in press). Advancing the study of loneliness and health in older adults through the use of integrative data analysis. Journal of Personality and Social Psychology.\nHawks, Z. W., Strong, R., Jung, L., Beck, E. D., Passell, E., Grinspoon, E., Sliwinski, M., and Germine, L. (in press). Accurate prediction of cognitive reaction time from intraindividual, age, and circadian variables. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging."
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\nBeck, E. D. and Jackson, J. J. (2022a). Idiographic prediction of loneliness and procrastination. Psychological Science. 33(10), 1767–1782.\nBeck, E. D., Jackson, J. J., and Condon, D. M. (2022). Interindividual differences in personality network structure. European Journal of Personality.\n*Beck, E. D., *Workman, C. I., and *Christensen, A. P. (2022). CRediT where credit is due: A comment on Leising et al., (2021). Personality Science.\nBeck, E. D. and Jackson, J. J. (2022b). A mega-analysis of personality prediction: Robustness and boundary conditions. Journal of Personality and Social Psychology, 122(3):523–553."
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\nBeck, E. D. and Jackson, J. J. (2021a). Detecting idiographic personality change. Journal of Personality Assessment, pages 1–17.\nBollich-Ziegler, K. L., Beck, E. D., Hill, P., and Jackson, J. J. (2021). Do correctional facilities correct our youth?: Effects of incarceration and court-ordered community service on personality development. Journal of Personality and Social Psychology, 121:894–913.\nYoneda, T., Marroig, A., Graham, E. K., Willroth, E. C., Watermeyer, T., Beck, E. D., Zelinski, E. M., Reynolds, C. A., Pedersen, N. L., Hofer, S. M., Mroczek, D. K., and Muniz-Terrera, G. (2022). Personality predictors of cognitive dispersion: A coordinated analysis of data from seven international studies of older adults. Neuropsychology, 36(2):103–115.\nMalle, B. F., Austerweil, J. L., Chi, V. B., Kennet, Y. N., Beck, E. D., Thapa, S., and Allaham, M. (2021). Cognitive properties of norm representations. Cog Sci Proceedings.\nBeck, E. D. and Jackson, J. J. (2021b). Idiographic personality coherence: A quasi experimental longitudinal ESM study. European Journal of Personality, 0:08902070211017746.\nSaef, R., Beck, E. D., and Jackson, J. J. (2021). Examining the dynamic nature of worker subjective well-being: The application of idiographic approaches. Research in Occupational Stress and Well-Being, 19:179–200.\nHill, P. L., Beck, E. D., and Jackson, J. J. (2021). Maintaining sense of purpose following health adversity in older adulthood: A propensity score matching examination. The Journals of Gerontology: Series B, 76:1574–1579.\nJackson, J. J. and Beck, E. D. (2021a). Using idiographic models to distinguish personality and psychopathology. Journal of Personality, 0:1–18.\nJackson, J. J. and Beck, E. D. (2021b). Personality development beyond the mean: Do life events shape personality variability, structure, and ipsative continuity? The Journals of Gerontology: Series B, 76(1):20–30.\nFrumkin, M. R., Piccirillo, M. L., Beck, E. D., Grossman, J. T., and Rodebaugh, T. L. (2021). Feasibility and utility of idiographic models in the clinic: a pilot study. Psychotherapy Research, 31(4):520–534."
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "2020",
    "text": "2020\nBeck, E. D. and Jackson, J. J. (2020b). Consistency and change in idiographic personality: A longitudinal ESM network study. Journal of Personality and Social Psychology, 118(5):1080–1100."
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications",
    "section": "2019",
    "text": "2019\nPiccirillo, M. L., Beck, E. D., and Rodebaugh, T. L. (2019). A clinician’s primer for idiographic research: considerations and recommendations. Behavior Therapy, 50(5):938–951."
  },
  {
    "objectID": "dplyr.html",
    "href": "dplyr.html",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "",
    "text": "library(psych)\nlibrary(dplyr)"
  },
  {
    "objectID": "dplyr.html#section",
    "href": "dplyr.html#section",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "1. %>%",
    "text": "1. %>%\nThe pipe %>% is wonderful. It makes coding intuitive. Often in coding, you need to use so-called nested functions. For example, you might want to round a number after taking the square of 43.\n\nsqrt(43)\n\n[1] 6.557439\n\nround(sqrt(43), 2)\n\n[1] 6.56\n\n\nThe issue with this comes whenever we need to do a series of operations on a data set or other type of object. In such cases, if we run it in a single call, then we have to start in the middle and read our way out.\nThe pipe solves this by allowing you to read from left to right (or top to bottom). The easiest way to think of it is that each call of %>% reads and operates as “and then.” So with the rounded square root of 43, for example:\n\nsqrt(43) %>%\n  round(2)\n\n[1] 6.56\n\n\nAs you can see, the two results are the same but the second is slightly easier to follow. And, as you’ll see below, this becomes even more intuitive when you start using it in conjunction with dplyr functions."
  },
  {
    "objectID": "dplyr.html#filter",
    "href": "dplyr.html#filter",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "2. filter()",
    "text": "2. filter()\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\nSay for example, that you’re interested personality change in adolescence, but you just opened a survey up online. So when you actually download and examine your data, you realize that you have an age range of something like 3-86, not 12-18. In this case, you want to get rid of the people over 18 – that is, filter() them out.\n\ndata(bfi) # grab the bfi data from the psych package\nbfi <- bfi %>% as_tibble()\n\nsummary(bfi$age) # get age descriptives\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.00   20.00   26.00   28.78   35.00   86.00 \n\nbfi2 <- bfi %>% # see a pipe!\n  filter(age <= 18) # filter to age up to 18\n\nsummary(bfi2$age) # summary of the new data \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    3.0    16.0    17.0    16.3    18.0    18.0 \n\n\nBut this isn’t quite right. We still have folks below 12. But, the beauty of filter() is that you can do sequence of OR and AND statements when there is more than one condition, such as up to 18 AND at least 12.\n\nbfi2 <- bfi %>%\n  filter(age <= 18 & age >= 12) # filter to age up to 18 and at least 12\n\nsummary(bfi2$age) # summary of the new data \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    16.0    17.0    16.4    18.0    18.0 \n\n\nGot it!\nBut filter works for more use cases than just conditional <, >, <=, and >=. It can also be used for cases where we want a single values to match cases with text. Before I demonstrate that, though, I need to convert one of the variables in the bfi data frame to a string. So let’s change gender (1 = male, 2 = female) to text (we’ll get into factors later).\n\nbfi$education <- plyr::mapvalues(bfi$education, 1:5, c(\"Below HS\", \"HS\", \"Some College\", \"College\", \"Higher Degree\"))\n\nNow let’s try a few things:\n1. Create a data set with only individuals with some college (==).\n\nbfi2 <- bfi %>% \n  filter(education == \"Some College\")\nunique(bfi2$education)\n\n[1] \"Some College\"\n\n\n2. Create a data set with only people age 18 (==).\n\nbfi2 <- bfi %>%\n  filter(age == 18)\nsummary(bfi2$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     18      18      18      18      18      18 \n\n\n3. Create a data set with individuals with some college or above (%in%).\n\nbfi2 <- bfi %>%\n  filter(education %in% c(\"Some College\", \"College\", \"Higher Degree\"))\nunique(bfi2$education)\n\n[1] \"Some College\"  \"Higher Degree\" \"College\"      \n\n\nThe %in% operator is wonderful. Instead of comparing a column to a single value, you can compare it to several. So above, when we wanted ages between 12 and 18, we could have done:\n\nbfi2 <- bfi %>%\n  filter(age %in% 12:18)\nsummary(bfi2$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    16.0    17.0    16.4    18.0    18.0 \n\n\nI’ve been using dplyr for nearly five years, and I still have to remind myself that when you want to remove rows, you use filter()."
  },
  {
    "objectID": "dplyr.html#select",
    "href": "dplyr.html#select",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "3. select()",
    "text": "3. select()\nIf filter() is for pulling certain observations (rows), then select() is for pulling certain variables (columns). Almost without fail, any data that are received for collected are going to have some variables that are not used, not useful, extraneous, etc. In such cases, it’s good practice to remove these columns to stop your environment from becoming cluttered and eating up your RAM.\nIn our bfi data, most of these have been pre-removed, so instead, we’ll imagine we don’t want to use any indicators of Agreeableness (A1-A5) and that we aren’t interested in gender.\nWith select(), there are few ways choose variables. We can bare quote name the ones we want to keep, bare quote names we want to remove, or use any of a number of select() helper functions.\n1. Bare quote columns we want to keep:\n\nbfi %>%\n  select(C1, C2, C3, C4, C5)\n\n# A tibble: 2,800 × 5\n      C1    C2    C3    C4    C5\n   <int> <int> <int> <int> <int>\n 1     2     3     3     4     4\n 2     5     4     4     3     4\n 3     4     5     4     2     5\n 4     4     4     3     5     5\n 5     4     4     5     3     2\n 6     6     6     6     1     3\n 7     5     4     4     2     3\n 8     3     2     4     2     4\n 9     6     6     3     4     5\n10     6     5     6     2     1\n# … with 2,790 more rows\n\n\nI’m going to stop there because I don’t want to name the additional 17 columns we want to keep. Instead we’ll use : to grab a range of columns.\n\nbfi %>%\n  select(C1:O5, education, age)\n\n# A tibble: 2,800 × 22\n      C1    C2    C3    C4    C5    E1    E2    E3    E4    E5    N1    N2    N3\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     2     3     3     4     4     3     3     3     4     4     3     4     2\n 2     5     4     4     3     4     1     1     6     4     3     3     3     3\n 3     4     5     4     2     5     2     4     4     4     5     4     5     4\n 4     4     4     3     5     5     5     3     4     4     4     2     5     2\n 5     4     4     5     3     2     2     2     5     4     5     2     3     4\n 6     6     6     6     1     3     2     1     6     5     6     3     5     2\n 7     5     4     4     2     3     4     3     4     5     5     1     2     2\n 8     3     2     4     2     4     3     6     4     2     1     6     3     2\n 9     6     6     3     4     5     5     3    NA     4     3     5     5     2\n10     6     5     6     2     1     2     2     4     5     5     5     5     5\n# … with 2,790 more rows, and 9 more variables: N4 <int>, N5 <int>, O1 <int>,\n#   O2 <int>, O3 <int>, O4 <int>, O5 <int>, education <chr>, age <int>\n\n\n2. Bare quote columns we don’t want to keep:\n\nbfi %>% \n  select(-(A1:A5), -gender)\n\n# A tibble: 2,800 × 22\n      C1    C2    C3    C4    C5    E1    E2    E3    E4    E5    N1    N2    N3\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     2     3     3     4     4     3     3     3     4     4     3     4     2\n 2     5     4     4     3     4     1     1     6     4     3     3     3     3\n 3     4     5     4     2     5     2     4     4     4     5     4     5     4\n 4     4     4     3     5     5     5     3     4     4     4     2     5     2\n 5     4     4     5     3     2     2     2     5     4     5     2     3     4\n 6     6     6     6     1     3     2     1     6     5     6     3     5     2\n 7     5     4     4     2     3     4     3     4     5     5     1     2     2\n 8     3     2     4     2     4     3     6     4     2     1     6     3     2\n 9     6     6     3     4     5     5     3    NA     4     3     5     5     2\n10     6     5     6     2     1     2     2     4     5     5     5     5     5\n# … with 2,790 more rows, and 9 more variables: N4 <int>, N5 <int>, O1 <int>,\n#   O2 <int>, O3 <int>, O4 <int>, O5 <int>, education <chr>, age <int>\n\n\nNote the () around the columns. That is necessary when you want to remove a range of columns.\n3. Add or remove using select() helper functions.\n\nstarts_with(): matches names that begin with quoted argument. For example, if we wanted all the Conscientiousness items, we could call the following:\n\n\nbfi %>%\n  select(starts_with(\"C\"))\n\n# A tibble: 2,800 × 5\n      C1    C2    C3    C4    C5\n   <int> <int> <int> <int> <int>\n 1     2     3     3     4     4\n 2     5     4     4     3     4\n 3     4     5     4     2     5\n 4     4     4     3     5     5\n 5     4     4     5     3     2\n 6     6     6     6     1     3\n 7     5     4     4     2     3\n 8     3     2     4     2     4\n 9     6     6     3     4     5\n10     6     5     6     2     1\n# … with 2,790 more rows\n\n\n\nends_with(): matches names that end with quoted argument. For example, if we wanted the first item in each Big Five scale, we could call:\n\n\nbfi %>% \n  select(ends_with(\"1\"))\n\n# A tibble: 2,800 × 5\n      A1    C1    E1    N1    O1\n   <int> <int> <int> <int> <int>\n 1     2     2     3     3     3\n 2     2     5     1     3     4\n 3     5     4     2     4     4\n 4     4     4     5     2     3\n 5     2     4     2     2     3\n 6     6     6     2     3     4\n 7     2     5     4     1     5\n 8     4     3     3     6     3\n 9     4     6     5     5     6\n10     2     6     2     5     5\n# … with 2,790 more rows\n\n\n\ncontains(): matches names that contain quote material. This can be any subset of a string, which makes it very useful for a number of contexts we’ll see later. But for now, if I wanted to be lazy or couldn’t remember the name of th education variable, I could call:\n\n\nbfi %>% \n  select(contains(\"edu\"))\n\n# A tibble: 2,800 × 1\n   education   \n   <chr>       \n 1 <NA>        \n 2 <NA>        \n 3 <NA>        \n 4 <NA>        \n 5 <NA>        \n 6 Some College\n 7 <NA>        \n 8 HS          \n 9 Below HS    \n10 <NA>        \n# … with 2,790 more rows\n\n\n\nmatches(): selects variables that match a regular expression (regex). Regex is tricky. I tend to end up referencing online documentation when I need to use this beyond a few basic expressions that I use very regularly. We’ll start with a simple one, keeping only those variables that either have or do not have numbers:\n\n\n# contains numbers\nbfi %>%\n  select(matches(\"[0-9]\")) \n\n# A tibble: 2,800 × 25\n      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2    E3\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     2     4     3     4     4     2     3     3     4     4     3     3     3\n 2     2     4     5     2     5     5     4     4     3     4     1     1     6\n 3     5     4     5     4     4     4     5     4     2     5     2     4     4\n 4     4     4     6     5     5     4     4     3     5     5     5     3     4\n 5     2     3     3     4     5     4     4     5     3     2     2     2     5\n 6     6     6     5     6     5     6     6     6     1     3     2     1     6\n 7     2     5     5     3     5     5     4     4     2     3     4     3     4\n 8     4     3     1     5     1     3     2     4     2     4     3     6     4\n 9     4     3     6     3     3     6     6     3     4     5     5     3    NA\n10     2     5     6     6     5     6     5     6     2     1     2     2     4\n# … with 2,790 more rows, and 12 more variables: E4 <int>, E5 <int>, N1 <int>,\n#   N2 <int>, N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>,\n#   O4 <int>, O5 <int>\n\n# does not contain numbers\nbfi %>%\n  select(!matches(\"[0-9]\")) \n\n# A tibble: 2,800 × 3\n   gender education      age\n    <int> <chr>        <int>\n 1      1 <NA>            16\n 2      2 <NA>            18\n 3      2 <NA>            17\n 4      2 <NA>            17\n 5      1 <NA>            17\n 6      2 Some College    21\n 7      1 <NA>            18\n 8      1 HS              19\n 9      1 Below HS        19\n10      2 <NA>            17\n# … with 2,790 more rows\n\n\n\nnum_range(): Given a stem and a range of numbers, this selects items in a sequence. This is especially useful when variables of your data set may not be in order.\n\n\n# select first 2 Extraversion items\nbfi %>%\n  select(num_range(\"E\", 1:2))\n\n# A tibble: 2,800 × 2\n      E1    E2\n   <int> <int>\n 1     3     3\n 2     1     1\n 3     2     4\n 4     5     3\n 5     2     2\n 6     2     1\n 7     4     3\n 8     3     6\n 9     5     3\n10     2     2\n# … with 2,790 more rows\n\n\n\none_of(): select any of a subset of items from a vector. This is one of my favorites, for reasons we’ll see in my tutorial on workflow and data documentation. But for now, let’s say I thought there were six items in each personality when there are actually five. So when I call the following, one_of() will be forgiving and ignore the fact that I messed up.\n\n\nbfi %>% \n  select(one_of(paste0(\"E\", 1:6)))\n\n# A tibble: 2,800 × 5\n      E1    E2    E3    E4    E5\n   <int> <int> <int> <int> <int>\n 1     3     3     3     4     4\n 2     1     1     6     4     3\n 3     2     4     4     4     5\n 4     5     3     4     4     4\n 5     2     2     5     4     5\n 6     2     1     6     5     6\n 7     4     3     4     5     5\n 8     3     6     4     2     1\n 9     5     3    NA     4     3\n10     2     2     4     5     5\n# … with 2,790 more rows\n\n\n\nall_of(): select all of a subset of items from a vector. Unlike one_of(), all_of() is less forgiving and will throw an error if we try to call for 6 Extraversion items.\n\n\nbfi %>%\n  select(all_of(paste0(\"E\", 1:6)))\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `E6` doesn't exist.\n\n\nOops. In this case, we’d then need to modify the code to reflect the correct number of items.\n\nbfi %>%\n  select(all_of(paste0(\"E\", 1:5)))\n\n# A tibble: 2,800 × 5\n      E1    E2    E3    E4    E5\n   <int> <int> <int> <int> <int>\n 1     3     3     3     4     4\n 2     1     1     6     4     3\n 3     2     4     4     4     5\n 4     5     3     4     4     4\n 5     2     2     5     4     5\n 6     2     1     6     5     6\n 7     4     3     4     5     5\n 8     3     6     4     2     1\n 9     5     3    NA     4     3\n10     2     2     4     5     5\n# … with 2,790 more rows"
  },
  {
    "objectID": "dplyr.html#arrange",
    "href": "dplyr.html#arrange",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "4. arrange()",
    "text": "4. arrange()\nSometimes, either in order to get a better sense of our data or in order to well, order our data, we want to sort it. Although there is a base R sort() function, the arrange() function is tidyverse version that plays nicely with other tidyverse functions.\nSo in our previous examples, we could also arrange() our data by age or education, rather than simply filtering. (Or as we’ll see later, we can do both!)\n\n# sort by age\nbfi %>% \n  select(gender:age) %>%\n  arrange(age)\n\n# A tibble: 2,800 × 3\n   gender education       age\n    <int> <chr>         <int>\n 1      1 Higher Degree     3\n 2      2 <NA>              9\n 3      2 Some College     11\n 4      2 <NA>             11\n 5      2 <NA>             11\n 6      2 <NA>             12\n 7      2 <NA>             12\n 8      2 <NA>             12\n 9      2 <NA>             12\n10      1 <NA>             12\n# … with 2,790 more rows\n\n# sort by education\nbfi %>%\n  select(gender:age) %>%\n  arrange(education)\n\n# A tibble: 2,800 × 3\n   gender education   age\n    <int> <chr>     <int>\n 1      1 Below HS     19\n 2      1 Below HS     21\n 3      1 Below HS     17\n 4      1 Below HS     18\n 5      1 Below HS     18\n 6      2 Below HS     18\n 7      2 Below HS     43\n 8      1 Below HS     32\n 9      1 Below HS     18\n10      2 Below HS     18\n# … with 2,790 more rows\n\n\nWe can also arrange by multiple columns, like if we wanted to sort by gender then education:\n\nbfi %>%\n  select(gender:age) %>%\n  arrange(gender, education)\n\n# A tibble: 2,800 × 3\n   gender education   age\n    <int> <chr>     <int>\n 1      1 Below HS     19\n 2      1 Below HS     21\n 3      1 Below HS     17\n 4      1 Below HS     18\n 5      1 Below HS     18\n 6      1 Below HS     32\n 7      1 Below HS     18\n 8      1 Below HS     18\n 9      1 Below HS     53\n10      1 Below HS     18\n# … with 2,790 more rows"
  },
  {
    "objectID": "dplyr.html#group_by",
    "href": "dplyr.html#group_by",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "5. group_by()",
    "text": "5. group_by()\nThe group_by() function is the “split” of the method. It basically implicitly breaks the data set into chunks by whatever bare quoted column(s)/variable(s) are supplied as arguments.\nSo imagine that we wanted to group_by() education levels to get average ages at each level. We would simply call:\n\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education)\n\n# A tibble: 2,800 × 8\n# Groups:   education [6]\n      C1    C2    C3    C4    C5   age gender education   \n   <int> <int> <int> <int> <int> <int>  <int> <chr>       \n 1     2     3     3     4     4    16      1 <NA>        \n 2     5     4     4     3     4    18      2 <NA>        \n 3     4     5     4     2     5    17      2 <NA>        \n 4     4     4     3     5     5    17      2 <NA>        \n 5     4     4     5     3     2    17      1 <NA>        \n 6     6     6     6     1     3    21      2 Some College\n 7     5     4     4     2     3    18      1 <NA>        \n 8     3     2     4     2     4    19      1 HS          \n 9     6     6     3     4     5    19      1 Below HS    \n10     6     5     6     2     1    17      2 <NA>        \n# … with 2,790 more rows\n\n\nWe can now see that it tells us that we have a tibble with 2,800 rows and 8 columns as well as Groups:   education [6]\nImportantly, once you group, you must ungroup() or your data frame will remain “split” and cause you problems. In other words, you must “combine” your data frame back together. This is super easy with the ungroup() function:\n\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  ungroup()\n\n# A tibble: 2,800 × 8\n      C1    C2    C3    C4    C5   age gender education   \n   <int> <int> <int> <int> <int> <int>  <int> <chr>       \n 1     2     3     3     4     4    16      1 <NA>        \n 2     5     4     4     3     4    18      2 <NA>        \n 3     4     5     4     2     5    17      2 <NA>        \n 4     4     4     3     5     5    17      2 <NA>        \n 5     4     4     5     3     2    17      1 <NA>        \n 6     6     6     6     1     3    21      2 Some College\n 7     5     4     4     2     3    18      1 <NA>        \n 8     3     2     4     2     4    19      1 HS          \n 9     6     6     3     4     5    19      1 Below HS    \n10     6     5     6     2     1    17      2 <NA>        \n# … with 2,790 more rows\n\n\nYou can also overwrite groups by calling group_by() more than once. We’ll touch more on that in future tutorials, but for now, notice what happens when I call group_by() twice sequentially:\n\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  group_by(gender, age)\n\n# A tibble: 2,800 × 8\n# Groups:   gender, age [115]\n      C1    C2    C3    C4    C5   age gender education   \n   <int> <int> <int> <int> <int> <int>  <int> <chr>       \n 1     2     3     3     4     4    16      1 <NA>        \n 2     5     4     4     3     4    18      2 <NA>        \n 3     4     5     4     2     5    17      2 <NA>        \n 4     4     4     3     5     5    17      2 <NA>        \n 5     4     4     5     3     2    17      1 <NA>        \n 6     6     6     6     1     3    21      2 Some College\n 7     5     4     4     2     3    18      1 <NA>        \n 8     3     2     4     2     4    19      1 HS          \n 9     6     6     3     4     5    19      1 Below HS    \n10     6     5     6     2     1    17      2 <NA>        \n# … with 2,790 more rows\n\n\nNote that the resulting data frame is not grouped by education at all, but by gender and age (i.e. it is not cumulative)."
  },
  {
    "objectID": "dplyr.html#mutate",
    "href": "dplyr.html#mutate",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "6. mutate()",
    "text": "6. mutate()\nThe mutate() function is one of a few options for how to “apply” (a) function(s) to your split (i.e. group_by()) data frame. When you use mutate(), the resulting data frame will have the same number of rows you started with (which will not be true with summarize() / summarise()). One way to remember is this is that you are directly mutating the existing data frame, either modifying existing columns or creating new ones.\nSo to continue with the example above, if we were to add a column that indicated average age levels within each age group, we would call:\n\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  group_by(education) %>% \n  mutate(age_by_edu = mean(age, na.rm = T))\n\n# A tibble: 2,800 × 9\n# Groups:   education [6]\n      C1    C2    C3    C4    C5   age gender education age_by_edu\n   <int> <int> <int> <int> <int> <int>  <int> <chr>          <dbl>\n 1     6     6     3     4     5    19      1 Below HS        25.1\n 2     4     3     5     3     2    21      1 Below HS        25.1\n 3     5     5     5     2     2    17      1 Below HS        25.1\n 4     5     5     4     1     1    18      1 Below HS        25.1\n 5     4     5     4     3     3    18      1 Below HS        25.1\n 6     3     2     3     4     6    18      2 Below HS        25.1\n 7     3     6     3     1     3    43      2 Below HS        25.1\n 8     5     3     2     4     6    32      1 Below HS        25.1\n 9     5     5     4     3     4    18      1 Below HS        25.1\n10     4     5     5     2     3    18      2 Below HS        25.1\n# … with 2,790 more rows\n\n\nAs you can see in the resulting data frame, each person (row) with the same education level has the same value in the new age_by_edu column I just added.\nmutate() is also super useful even when you aren’t grouping. For example, if I wanted to recode gender so that 1 = “male” and 2 = “female,” we could do that like:\n\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  mutate(gender_cat = plyr::mapvalues(gender, c(1,2), c(\"Male\", \"Female\")))\n\n# A tibble: 2,800 × 9\n      C1    C2    C3    C4    C5   age gender education    gender_cat\n   <int> <int> <int> <int> <int> <int>  <int> <chr>        <chr>     \n 1     2     3     3     4     4    16      1 <NA>         Male      \n 2     5     4     4     3     4    18      2 <NA>         Female    \n 3     4     5     4     2     5    17      2 <NA>         Female    \n 4     4     4     3     5     5    17      2 <NA>         Female    \n 5     4     4     5     3     2    17      1 <NA>         Male      \n 6     6     6     6     1     3    21      2 Some College Female    \n 7     5     4     4     2     3    18      1 <NA>         Male      \n 8     3     2     4     2     4    19      1 HS           Male      \n 9     6     6     3     4     5    19      1 Below HS     Male      \n10     6     5     6     2     1    17      2 <NA>         Female    \n# … with 2,790 more rows\n\n\nWe could also just write over the original gender category like:\n\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  mutate(gender = plyr::mapvalues(gender, c(1,2), c(\"Male\", \"Female\")))\n\n# A tibble: 2,800 × 8\n      C1    C2    C3    C4    C5   age gender education   \n   <int> <int> <int> <int> <int> <int> <chr>  <chr>       \n 1     2     3     3     4     4    16 Male   <NA>        \n 2     5     4     4     3     4    18 Female <NA>        \n 3     4     5     4     2     5    17 Female <NA>        \n 4     4     4     3     5     5    17 Female <NA>        \n 5     4     4     5     3     2    17 Male   <NA>        \n 6     6     6     6     1     3    21 Female Some College\n 7     5     4     4     2     3    18 Male   <NA>        \n 8     3     2     4     2     4    19 Male   HS          \n 9     6     6     3     4     5    19 Male   Below HS    \n10     6     5     6     2     1    17 Female <NA>        \n# … with 2,790 more rows"
  },
  {
    "objectID": "dplyr.html#summarize-summarise",
    "href": "dplyr.html#summarize-summarise",
    "title": "Data Manipulation: Intro to dplyr",
    "section": "7. summarize() / summarise()",
    "text": "7. summarize() / summarise()\nThe summarize() / summarise() functions (choose your spelling as you will) is another of the options for how to “apply” (a) function(s) to your split (i.e. group_by()) data frame. When you use summarize() (I made by choice), the resulting data frame will have the number of rows equal to the number of group_by() categories you provide. So if you provided education, that will be 6, and if you provided none, it would be one.\n\n# group_by() education\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  group_by(education) %>% \n  summarize(age_by_edu = mean(age, na.rm = T))  \n\n# A tibble: 6 × 2\n  education     age_by_edu\n  <chr>              <dbl>\n1 Below HS            25.1\n2 College             33.0\n3 Higher Degree       35.3\n4 HS                  31.5\n5 Some College        27.2\n6 <NA>                18.0\n\n# no groups  \nbfi %>% \n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  summarize(age_by_edu = mean(age, na.rm = T))  \n\n# A tibble: 1 × 1\n  age_by_edu\n       <dbl>\n1       28.8\n\n\nFrom this, for example, it becomes clear that all the NAs in the education variable were because they reflected 18 year olds who had not yet completed high school as well as that the sample is pretty young overall."
  },
  {
    "objectID": "purrr.html",
    "href": "purrr.html",
    "title": "Intro to purrr",
    "section": "",
    "text": "Download .Rmd (won’t work in Safari or IE)\nSee GitHub Repository\n#purrr In my opinion, purrr is one of the most underrated and under-utilized R packages.\n#Background: iteration Iteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the \\(\\Sigma\\) symbol, then you’ve seen (and probably used) iteration.\nIt’s also incredibly useful. Anytime you have to repeat some sort of action many times, iteration is your best friend. In psychology, this often means reading in a bunch of individual data files from an experiment, repeating an analysis with a series of different predictors or outcomes, or creating a series of figures.\nEnter for loops. for loops are the “OG” form of iteration in computer science. The basic syntax is below. Basically, we can use a for loop to loop through and print a series of things.\nIn psychology, we deal with all sorts of weird sorts of data frames. From longitudinal data with separate files for each year to experimental data with separate data for each participant (if you’re “lucky,” you might even get both!), data are often stored as separate files. THe good news is that for loops are here to save you from:\nAssuming you have all the data in a single folder and the format is reasonably similar, you have the following basic syntax:\nThe loop above defines the path of the data, reads all the files in that path, creates an empty list to store the data files, loops through each of the files individually and saves them into the list, and combines each of the read data files into a single data frame.\nThis is all well and good and would work just fine. But what happens if you have multiple data files for different subjects if, say, they complete a writing task and a memory task? Or maybe you work with longitudinal data, like I do, and frequently have multiple data files for a given year for different categories (e.g. health, psychological, etc.). In that case, the loop above might not work. The files might have different properties or be stored in different locations (for your own sanity).\nIn this case, it’s a little more complicated. First, our method for loading each of the files into a list doesn’t work nicely here because we are iterating through 2 variables. As a result, we have to save each file into an object called “tmp” that then must be joined with data from previous iterations. The downside of this is that we have to use a sort of “brute-force” method to do so, which is not ideal. Things go wrong with data collection quite often, meaning that files are likely to have different columns. Third, when a loop fails, it refuses to continue. And you are left with a couple of variables (i and k) and have to try to work backward to figure out what is going wrong.\nSound annoying? Enter purrr.\n#The purrr solution purrr is my favorite alternative to iteration. (There’s also the whole apply family, which is definitely worth learning – even I still use it – but I find purrr to be much more useful in the long run.) purrr keeps me organized by keeping everything together in a single object. It works nicely with functions like possibly() and safely() that catch and handle errors.\npurrr can be used for many more things than I will talk about here. If you want to know more, you can check out Hadley Wickham’s R for Data Science or purrr documentation. I’m going to focus on how I I use purrr: reading data, cleaning, running models, making tables, and making plots.\nHere’s some purrr syntax I won’t explain now. It’s set up to mirror the demonstration above. It will make way more sense soon, I promise:\nBeyond “WTF”, your initial response may be “this is not more efficient than the nested for loop you showed us above.” You are partially correct. The rest of the tutorial will be showing you why being wrong now will sooooo right later!"
  },
  {
    "objectID": "purrr.html#nested-data-frames",
    "href": "purrr.html#nested-data-frames",
    "title": "Intro to purrr",
    "section": "Nested Data Frames",
    "text": "Nested Data Frames\nBefore we can learn how to use purrr, we need to understand what a nested data frame is. If you’ve ever worked with a list in R, you are halfway there. Basically a nested data frame takes the normal data frame you are probably familiar with and adds some new features. It still has columns, rows, and cells, but what makes up those cells isn’t restrictred to numbers, strings, or logicals. Instead, you can put essentially anything you want: lists, models, data frames, plots, etc!\nIf that freaks you out a bit, imagine this. Imagine you are me: you work with personality data and want to use each of the Big 5 to individually predict some outcomes, like health and life satisfaction.\n\nipip50 <- read.csv(url(\"https://media.githubusercontent.com/media/emoriebeck/R-tutorials/master/05_purrr/ipip50_sample.csv\"), stringsAsFactors = F)\n\n# let's recode the exercise variable (exer)\n# 0 = \"veryRarelyNever\"; 1 = \"less1mo\"; 2 = \"less1wk\"; 3 = \"1or2wk\"; 4 = \"3or5wk\"; 5 = \"more5wk\"\nipip50 <- ipip50 %>% \n  mutate(exer = mapvalues(exer, unique(exer), c(3,4,0,5,2,1)))\n\nThe really bad solution would be to write the code to model these data, make a table of the results, and make a plot. Then you would copy and paste that code 9 times to do the same steps for the other trait-outcome pairs, changing the key variables. Sometime later, you could run those individually.\nA better solution would be a loop, where you use a nested loop to complete the steps for each trait-outcome pair. How you store these values can be a little wonky and often involes a bunch of different lists or a cluttered global environment with losts of objects.\nBut the best solution is purrr. What does this look like? Well, we start with a nested data frame. To do that, we need to make sure our data is ready. I’ve found that the easiest way to work with data with purrr is to first convert your data to long form, where we want to have columns for all the variable we would want to iterate through in a loop. To help you understand what that means and looks like, I think it’s useful to start with a non-nested data frame created by the crossing() function from the dplyr package.\nBasically, crossing() takes what you give it and returns a data frame with all combinations of the variables. There is no limit to the number of columns this can have. Here, we feed it “Trait”, which contains a vector of the Big 5, and “Outcome”, which contains a vector of our outcomes, which results in a data frame with 2 columns and 10 rows.\n\n(df <- expand.grid(\n  Trait = c(\"E\", \"A\", \"C\", \"N\", \"O\"),\n  Outcome = c(\"BMI\", \"logMedInc\", \"exer\")\n)) \n\n   Trait   Outcome\n1      E       BMI\n2      A       BMI\n3      C       BMI\n4      N       BMI\n5      O       BMI\n6      E logMedInc\n7      A logMedInc\n8      C logMedInc\n9      N logMedInc\n10     O logMedInc\n11     E      exer\n12     A      exer\n13     C      exer\n14     N      exer\n15     O      exer\n\n\nOne cool thing this will allow us to do is to use consistent variable names in formulas and functions and to feed the correct data into different programs using the dplyr helper function filter(). You can use expand grid with purrr functions without nesting any data in the data frame (in fact, I do this a lot because I work with large data sets and lots of combinations), but I’m going to show you the nested data frame route in this case and refer you to my GitHub for when and how you would use the crossing() approach.\nBack to nested data frames. We want to end up with a data frame that has the same columns as the crossing() data frame except that we want an additional column that holds the data for each trait-outcome pair. To do this, we need to have a column that indexes both trait and outcome. To get this, we change our data to the “tidy” format using gather() in the tidyr package.\nSo let’s take our Big 5 data and do that.\n\n# Let's make the trait data long and create composites\n(ipip50_composites <- ipip50 %>%\n  gather(key = item, value = value, A_1:O_10) %>%\n  separate(item, c(\"Trait\", \"item\"), sep = \"_\") %>%\n  group_by(RID, gender, age, BMI, exer, logMedInc, Trait) %>%\n  summarise(t.value = mean(value, na.rm = T)))\n\n# A tibble: 5,000 × 8\n# Groups:   RID, gender, age, BMI, exer, logMedInc [1,000]\n       RID gender   age   BMI exer  logMedInc Trait t.value\n     <int> <chr>  <int> <dbl> <chr>     <dbl> <chr>   <dbl>\n 1 3078429 female    27  28.0 3          10.7 A        5.41\n 2 3078429 female    27  28.0 3          10.7 C        4.77\n 3 3078429 female    27  28.0 3          10.7 E        6.03\n 4 3078429 female    27  28.0 3          10.7 N        5.65\n 5 3078429 female    27  28.0 3          10.7 O        4.01\n 6 4310942 female    35  28.4 4          10.7 A        4.63\n 7 4310942 female    35  28.4 4          10.7 C        4.77\n 8 4310942 female    35  28.4 4          10.7 E        5.00\n 9 4310942 female    35  28.4 4          10.7 N        3.11\n10 4310942 female    35  28.4 4          10.7 O        6.78\n# … with 4,990 more rows\n\n# Now let's make the outcomes long\nipip50_composites <- ipip50_composites %>%\n  gather(key = Outcome, value = o.value, BMI:logMedInc) \n\nNow that our data is in long format, we have a couple of options. The first is to use the nest() function from the tidyr package to chunk our data by trait and outcome. This will result in a data frame with 3 columns: 1 that indexes the Trait, one that indexes the Outcome, and one that indexes the data for that trait and outcome combination.\n\n(ipip50_nested <- ipip50_composites %>% \n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup())\n\n# A tibble: 15 × 3\n   Trait Outcome   data                \n   <chr> <chr>     <list>              \n 1 A     BMI       <tibble [1,000 × 5]>\n 2 C     BMI       <tibble [1,000 × 5]>\n 3 E     BMI       <tibble [1,000 × 5]>\n 4 N     BMI       <tibble [1,000 × 5]>\n 5 O     BMI       <tibble [1,000 × 5]>\n 6 A     exer      <tibble [1,000 × 5]>\n 7 C     exer      <tibble [1,000 × 5]>\n 8 E     exer      <tibble [1,000 × 5]>\n 9 N     exer      <tibble [1,000 × 5]>\n10 O     exer      <tibble [1,000 × 5]>\n11 A     logMedInc <tibble [1,000 × 5]>\n12 C     logMedInc <tibble [1,000 × 5]>\n13 E     logMedInc <tibble [1,000 × 5]>\n14 N     logMedInc <tibble [1,000 × 5]>\n15 O     logMedInc <tibble [1,000 × 5]>\n\n\nBasically, instead of the cells in the “data” column being a single numeric, logical, or character value, each cell is a data frame! Note that the class of the “data” column is a list (hence the name “list column”) and the class of each cell is a tibble. o_O Pretty cool, huh? Here’s why: by putting a data frame of the data for each trait / outcome combination in a single cell, we can operate on each cell like we would a cell in a normal data frame. Sort of."
  },
  {
    "objectID": "purrr.html#the-map-functions",
    "href": "purrr.html#the-map-functions",
    "title": "Intro to purrr",
    "section": "The map() Functions",
    "text": "The map() Functions\nI say sort of because we need another function, this time from the purrr package (yay!) called map(). Now my purpose here isn’t to go through every possible way you can use this. If you want to learn more of the ins and outs see http://r4ds.had.co.nz/many-models.html. My goal is to show you how I, a psychology grad student, uses purrr every. single. day. in my research.\nWhat we want to do is to run a model using personality to predict our outcomes for each combination of trait and outcome. Now that we have a data frame for each nested in a data frame, we’re ready to do that. Here’s how.\n\nmap()\n\nstart_time <- Sys.time()\n(ipip50_nested <- ipip50_nested %>%\n  mutate(model = map(data, ~lm(o.value ~ t.value, data = .))))\n\n# A tibble: 15 × 4\n   Trait Outcome   data                 model \n   <chr> <chr>     <list>               <list>\n 1 A     BMI       <tibble [1,000 × 5]> <lm>  \n 2 C     BMI       <tibble [1,000 × 5]> <lm>  \n 3 E     BMI       <tibble [1,000 × 5]> <lm>  \n 4 N     BMI       <tibble [1,000 × 5]> <lm>  \n 5 O     BMI       <tibble [1,000 × 5]> <lm>  \n 6 A     exer      <tibble [1,000 × 5]> <lm>  \n 7 C     exer      <tibble [1,000 × 5]> <lm>  \n 8 E     exer      <tibble [1,000 × 5]> <lm>  \n 9 N     exer      <tibble [1,000 × 5]> <lm>  \n10 O     exer      <tibble [1,000 × 5]> <lm>  \n11 A     logMedInc <tibble [1,000 × 5]> <lm>  \n12 C     logMedInc <tibble [1,000 × 5]> <lm>  \n13 E     logMedInc <tibble [1,000 × 5]> <lm>  \n14 N     logMedInc <tibble [1,000 × 5]> <lm>  \n15 O     logMedInc <tibble [1,000 × 5]> <lm>  \n\nend_time <- Sys.time()\nprint(end_time - start_time)\n\nTime difference of 0.09020615 secs\n\n\nWhat’s going on there? Well, we’re using mutate() from dplyr to create a new column in our data frame called “model.” Then, we use the map() function to tell it that we want to take each of the cells in the “data” column and run a linear model predicting our outcomes (o.value) from personality (t.value). The “data = .” part follows because we are within a dplyr pipe.\nAs you can see, this results in a new column called “model.” As with the data column, the class of the “model” column is a list, and the class of any individual cell in the column is the S3 class “lm”, which just means linear model.\nNow, this is definitely a fast way to run a lot of models, but thus far, this isn’t better than a for loop. But our nested data frame can way outperform a for loop. We don’t run models just for the sake of doing so. We want to extract the information and report it, often in either a table or figure. With map() and purrr, we can create a table and figure for each model and store it in our data frame. No more dealing with clunky lists whose contents is hard to access or seemingly infinite numbers of objects cluttering your environment. It’s all stored in ONE DATA FRAME. (Sorry to shout, but I think the advantages of this cannot be overstated.)\nWatch:\n\n(ipip50_nested <- ipip50_nested %>%\n  mutate(tidy = map(model, broom::tidy)))\n\n# A tibble: 15 × 5\n   Trait Outcome   data                 model  tidy            \n   <chr> <chr>     <list>               <list> <list>          \n 1 A     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 2 C     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 3 E     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 4 N     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 5 O     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 6 A     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 7 C     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 8 E     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n 9 N     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n10 O     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n11 A     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n12 C     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n13 E     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n14 N     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n15 O     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]>\n\n\n\n\nplyr alternative\nTo be fair, there are other alternative, like dlply() in the plyr package. I’ll demonstrate it below then make a case for why not to do this.\nSo if we start by taking our long format data frame, we can use a very similar format to map to create a list of models.\n\nmodels <- dlply(ipip50_composites, .(Trait, Outcome), function(x) lm(o.value ~ t.value, data = x))\n\nThen we could again use tidy() from broom to get summaries.\n\ntidies <- llply(models, broom::tidy)\n\nAnd then use combine() from dplyr to merge them. BUT, we have a problem. (1) We have a weird, nested list, and (2) combine() doesn’t index our grouping variables like our nested data frame + map().\n\n\nThe for Loop Alternative\nOkay, but we can do this with a for loop, so why not? My rationale is that it makes my brain hurt to write a loop that is half as functional the purrr solution. Watch:\n\nTraits <- c(\"E\", \"A\", \"C\", \"N\", \"O\")\nOutcomes <- c(c(\"BMI\", \"logMedInc\", \"exer\"))\n\nipip50_loop <- list()\ncounter <- 1\nstart_time <- Sys.time()\nfor (trait in Traits){\n  for (outcome in Outcomes){\n    df <- ipip50_composites %>% \n      filter(Trait == trait & Outcome == outcome)\n    tmp <- tibble(Trait = trait, Outcome = outcome)\n    tmp$model <- list(lm(o.value ~ t.value, data = df))\n    ipip50_loop[[counter]] <- tmp\n    counter <- counter + 1\n  }\n}\nend_time <- Sys.time()\nprint(end_time - start_time)\n\nTime difference of 0.4015951 secs\n\n\nThis took a lot more lines of code and also took longer. For a few models with a small data set, this doesn’t matter much. But when you work with hundreds of models with 10’s or 100’s of thousands of observations, this adds up.\nSo the lesson here is just use purrr. Please."
  },
  {
    "objectID": "purrr.html#unnesting",
    "href": "purrr.html#unnesting",
    "title": "Intro to purrr",
    "section": "Unnesting",
    "text": "Unnesting\nUsing the tidy() function from the broom package, we now have another column. Again, the column’s class is “list” but the cell’s class is “data.frame”. How can we use this? Well, the nest() function we used earlier has a sibling called unnest(). It does the opposite of nest(); it takes our list columns and expands it. So, since we have 2 x 5 data frame in each cell of the “tidy” column, when we unnest it, there will be rows for each Trait and outcome combination (where there was only 1 in the nested data frame). This will make more sense with a demonstration:\n\nipip50_nested %>%\n  select(Trait, Outcome, tidy) %>%\n  unnest(tidy)\n\n# A tibble: 30 × 7\n   Trait Outcome term        estimate std.error statistic   p.value\n   <chr> <chr>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n 1 A     BMI     (Intercept)  25.3        1.10    23.0    2.11e- 94\n 2 A     BMI     t.value       0.0627     0.229    0.274  7.84e-  1\n 3 C     BMI     (Intercept)  26.2        0.928   28.3    1.54e-129\n 4 C     BMI     t.value      -0.152      0.220   -0.691  4.89e-  1\n 5 E     BMI     (Intercept)  25.7        0.667   38.5    1.34e-199\n 6 E     BMI     t.value      -0.0228     0.172   -0.133  8.94e-  1\n 7 N     BMI     (Intercept)  25.9        0.667   38.8    2.28e-201\n 8 N     BMI     t.value      -0.0763     0.181   -0.423  6.73e-  1\n 9 O     BMI     (Intercept)  25.5        0.947   26.9    1.55e-120\n10 O     BMI     t.value       0.0167     0.199    0.0840 9.33e-  1\n# … with 20 more rows\n\n\nPretty neat, huh? From here, we may want to do a bunch of different things. And purrr is our friend for all of them. I’m going to do a few below, just to show you your options."
  },
  {
    "objectID": "purrr.html#create-a-table",
    "href": "purrr.html#create-a-table",
    "title": "Intro to purrr",
    "section": "Create a Table",
    "text": "Create a Table\nWhen we have multiple predictors and outcomes, we typically want to smash all this info into a single table, with predictors as different rows of the table and outcomes as different columns (or vice versa). We typically include both an estimate and a confidence interval or standard error for each term in the model (in our case Intercept and t.value).\nLet’s create a table with different columns for each of the outcomes, and different rows for each trait:\n\n(tab <- ipip50_nested %>%\n  select(Trait, Outcome, tidy) %>%\n  unnest(tidy) %>%\n  select(Trait:std.error) %>%\n  rename(b = estimate, SE = std.error) %>%\n  gather(key = tmp, value = value, b, SE) %>%\n  unite(tmp, Outcome, tmp, sep = \".\") %>%\n  spread(key = tmp, value = value))\n\n# A tibble: 10 × 8\n   Trait term          BMI.b BMI.SE  exer.b exer.SE logMedInc.b logMedInc.SE\n   <chr> <chr>         <dbl>  <dbl>   <dbl>   <dbl>       <dbl>        <dbl>\n 1 A     (Intercept) 25.3     1.10   2.11    0.282    10.9            0.0711\n 2 A     t.value      0.0627  0.229  0.122   0.0589    0.00765        0.0148\n 3 C     (Intercept) 26.2     0.928  2.02    0.238    10.9            0.0601\n 4 C     t.value     -0.152   0.220  0.162   0.0566   -0.000788       0.0143\n 5 E     (Intercept) 25.7     0.667  2.40    0.171    10.9            0.0431\n 6 E     t.value     -0.0228  0.172  0.0749  0.0442   -0.00260        0.0111\n 7 N     (Intercept) 25.9     0.667  3.20    0.171    10.9            0.0432\n 8 N     t.value     -0.0763  0.181 -0.146   0.0463    0.00565        0.0117\n 9 O     (Intercept) 25.5     0.947  2.92    0.244    10.8            0.0612\n10 O     t.value      0.0167  0.199 -0.0506  0.0513    0.0207         0.0129\n\n\nWe aren’t quite done yet. This table would never make it in a publication. Enter kable() + kableExtra.\ntab %>% select(-Trait) %>%\n  kable(., \"html\", booktabs = T, escape = F, digits = 2,\n        col.names = c(\"Term\", rep(c(\"b\", \"SE\"), times = 3))) %>%\n  kable_styling(full_width = F) %>%\n  column_spec(2:7, width = \"2cm\") %>%\n  kableExtra::group_rows(\"Agreeableness\",1,2) %>%\n  kableExtra::group_rows(\"Conscientiousness\",3,4) %>%\n  kableExtra::group_rows(\"Extraversion\",5,6) %>%\n  kableExtra::group_rows(\"Neuroticism\",7,8) %>%\n  kableExtra::group_rows(\"Openness\",9,10) %>%\n  add_header_above(c(\" \" = 1, \"BMI\" = 2, \"Exercise\" = 2, \"Log Median Income\" = 2))\n\n\n\n \n\n\nBMI\nExercise\nLog Median Income\n\n  \n    Term \n    b \n    SE \n    b \n    SE \n    b \n    SE \n  \n \n\n  Agreeableness\n\n    (Intercept) \n    25.29 \n    1.10 \n    2.11 \n    0.28 \n    10.89 \n    0.07 \n  \n  \n    t.value \n    0.06 \n    0.23 \n    0.12 \n    0.06 \n    0.01 \n    0.01 \n  \n  Conscientiousness\n\n    (Intercept) \n    26.21 \n    0.93 \n    2.02 \n    0.24 \n    10.93 \n    0.06 \n  \n  \n    t.value \n    -0.15 \n    0.22 \n    0.16 \n    0.06 \n    0.00 \n    0.01 \n  \n  Extraversion\n\n    (Intercept) \n    25.67 \n    0.67 \n    2.40 \n    0.17 \n    10.94 \n    0.04 \n  \n  \n    t.value \n    -0.02 \n    0.17 \n    0.07 \n    0.04 \n    0.00 \n    0.01 \n  \n  Neuroticism\n\n    (Intercept) \n    25.86 \n    0.67 \n    3.20 \n    0.17 \n    10.91 \n    0.04 \n  \n  \n    t.value \n    -0.08 \n    0.18 \n    -0.15 \n    0.05 \n    0.01 \n    0.01 \n  \n  Openness\n\n    (Intercept) \n    25.51 \n    0.95 \n    2.92 \n    0.24 \n    10.83 \n    0.06 \n  \n  \n    t.value \n    0.02 \n    0.20 \n    -0.05 \n    0.05 \n    0.02 \n    0.01 \n  \n\n\n\n\nNow I would usually get fancy and bold or flag significant values. I would also use confidence intervals rather than standard errors and add some additional rows with some model summary terms (e.g. \\(R^2\\)). If you want to see that, I’ll refer you to my github."
  },
  {
    "objectID": "purrr.html#plots",
    "href": "purrr.html#plots",
    "title": "Intro to purrr",
    "section": "Plots",
    "text": "Plots\n\nOne Big Plot\nSometimes, we want one big plot that shows all our results. What kind of plot? You have choice. Line graphs are popular, but they are perhaps overly simple for these simple linear relationships. We’d also have to go back and get predicted values, which is helpful, but again I’ll refer you to my github for more on that. Instead, we’re going to create a forest plot, which is useful for determining which terms are different than 0 and how those relate to other terms. We can do this for both our model terms (Intercept and t.value), but I’m going to restrict us to t.value, which tells us how a 1 point increase in a personality characteristic is associated with an outcome.\n\nipip50_nested %>%\n  select(Trait, Outcome, tidy) %>%\n  unnest(tidy) %>%\n  filter(term == \"t.value\") %>%\n  ggplot(aes(x = Trait, y = estimate)) +\n    geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error),\n                  width = .1) +\n    geom_point(aes(color = Trait), size = 3) +\n    coord_flip() +\n    facet_wrap(~Outcome, scale = \"free\") +\n    theme_classic() +\n    theme(legend.position = \"none\")\n\n\n\n\nPretty cool. We see that personality predicts exercise pretty much across the board, but that it does not predict BMI. Only Openness predicts log Median Income.\nBut here’s where we pat ourselves on the back. We got from nesting our data to the plot above in FIFTEEN LINES OF CODE. Without purrr, it would take us that many lines just to run our models. Then we’d still need to tidy them, join them back together, and plot. I don’t want to do that. Or we could use a loop, and create a weird series of lists or a cluttered environment. No thank you on all accounts.\n\n\nPredicted Values\nBut you will encounter times when you want to do predicted values. There are a number of ways to go about this (and both of these ignore that you can just use geom_smooth() in the ggplot2 package with method = “lm” for simple linear models). I’m going to show you 2 purrrfect ways. Because demonstrations.\n\nSingle Plots\nFirst, let’s get predicted values for each model. We’ll use expand.grid() to get the full range of values for each personality traits (1 to 7) and then use the predict() function to get the predicted values, setting the “newdata” argument to the newly created range of personality values.\nTo do this, I’m also going to introduce something that is central to purrr programming: local functions. As a general rule, if you ever have to do something multiple times, write a function. Save yourself. Please. When writing functions for purrr, the basic mindframe I use is to make the inputs of the data frame either the “data” column of the nested data frame or the individual columns of interest. So in the function below, I want to get predicted values, so I take a model object as input and output a data frame of predicted values.\n\npred_fun <- function(mod){\n  crossing(\n    t.value = seq(1,7,.25)\n  ) %>%\n    mutate(pred = predict(mod, newdata = .))\n}\n\n(ipip50_nested <- ipip50_nested %>%\n  mutate(pred = map(model, pred_fun)))\n\n# A tibble: 15 × 6\n   Trait Outcome   data                 model  tidy             pred    \n   <chr> <chr>     <list>               <list> <list>           <list>  \n 1 A     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 2 C     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 3 E     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 4 N     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 5 O     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 6 A     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 7 C     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 8 E     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n 9 N     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n10 O     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n11 A     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n12 C     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n13 E     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n14 N     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n15 O     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble>\n\n\nNow, let’s take those predicted values and use them to make individual plots.\n\nplot_fun <- function(df, trait, outcome){\n  df %>%\n    ggplot(aes(x = t.value, y = pred)) +\n      geom_line() +\n      labs(x = trait, y = outcome) +\n      theme_classic()\n}\n\n(ipip50_nested <- ipip50_nested %>%\n  mutate(plot = pmap(list(pred, Trait, Outcome), plot_fun)))\n\n# A tibble: 15 × 7\n   Trait Outcome   data                 model  tidy             pred     plot  \n   <chr> <chr>     <list>               <list> <list>           <list>   <list>\n 1 A     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 2 C     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 3 E     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 4 N     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 5 O     BMI       <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 6 A     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 7 C     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 8 E     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n 9 N     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n10 O     exer      <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n11 A     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n12 C     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n13 E     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n14 N     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n15 O     logMedInc <tibble [1,000 × 5]> <lm>   <tibble [2 × 5]> <tibble> <gg>  \n\n\nLet’s take a look at how this plot actually looks:\n\nipip50_nested$plot[[1]]\n\n\n\n\nMeh, not a fan. Let’s do better and combine prediction lines across traits within outcomes. We’ll do this two ways: (1) separately for each outcome and (2) using facets across all outcomes.\n\nipip50_nested %>%\n  unnest(pred) %>%\n  ggplot(aes(x = t.value, y = pred, color = Trait)) +\n    geom_line(size = 2) +\n    facet_wrap(~Outcome, scale = \"free\") +\n    theme_classic() +\n    theme(legend.position = \"bottom\")\n\n\n\n\nMeh, this is fine, but the scale’s off. This is what I’d call a “wow graph” because it exaggerates the differences. I could write some code to put each graph on a realistic scale for the outcome, but for now, I won’t."
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "Introduction to ggplot2",
    "section": "",
    "text": "Codelibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()"
  },
  {
    "objectID": "ggplot2.html#what-is-ggplot2-trying-to-do",
    "href": "ggplot2.html#what-is-ggplot2-trying-to-do",
    "title": "Introduction to ggplot2",
    "section": "What is ggplot2 trying to do?",
    "text": "What is ggplot2 trying to do?\n\nCreate a grammar of graphics\nAims to help draw connections across diverse plots\nCreate order in the chaos of complicated plots\n\nFrom Wickham (2010):\n\nA grammar of graphics is a tool that enables us to concisely describe the components of a graphic."
  },
  {
    "objectID": "ggplot2.html#what-are-the-core-elements-of-ggplot2-grammar",
    "href": "ggplot2.html#what-are-the-core-elements-of-ggplot2-grammar",
    "title": "Introduction to ggplot2",
    "section": "What are the core elements of ggplot2 grammar?",
    "text": "What are the core elements of ggplot2 grammar?\n\n\nMappings: base layer\n\nScales: control and modify your mappings\n\nGeoms: plot elements\n\nFacets: panel your plot\n\nGrobs: things that aren’t geoms that we want to layer on like text, arrows, other things\n\nThemes: style your figure"
  },
  {
    "objectID": "ggplot2.html#but-first-our-data",
    "href": "ggplot2.html#but-first-our-data",
    "title": "Introduction to ggplot2",
    "section": "But first, our data",
    "text": "But first, our data\n\nThese are some Experience Sampling Method data I collected during my time in graduate school\nSpecifically, these include data from Beck & Jackson (2022)\nIn that paper I built personalized machine learning models of behaviors and experiences from sets of:\n\npsychological\nsituational\nand time variables\n\n\n\n\nCodeload(url(\"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/02-week2-ggplot2/01-data/ipcs_data.RData\"))\nipcs_data \n\n\n\n  \n\n\n\n\nCodeipcs_data <- ipcs_data %>%\n  group_by(SID) %>%\n  mutate(beep = 1:n()) %>%\n  ungroup()"
  },
  {
    "objectID": "ggplot2.html#continuous",
    "href": "ggplot2.html#continuous",
    "title": "Introduction to ggplot2",
    "section": "continuous",
    "text": "continuous\n\nLet’s try the continuous scale with our y mapping\nWe’ll use the following three arguments\n\n\nlimits: vector length 2\n\nbreaks: vector of any length\n\nlabels: numeric or character vector\n\n\n\n\nCodeggplot(\n  data = ipcs_data\n  , mapping = aes(x = linear, y = happy)\n) + \n  scale_y_continuous(\n    limits = c(1, 5)\n    , breaks = seq(1, 5, by = 2)\n    , labels = c(\"one\", \"three\", \"five\")\n  )"
  },
  {
    "objectID": "ggplot2.html#geom_jitter",
    "href": "ggplot2.html#geom_jitter",
    "title": "Introduction to ggplot2",
    "section": "geom_jitter()",
    "text": "geom_jitter()\n\nSometimes we have data that have lots of repeating values, especially with ordinal response scales where the variables aren’t composited / latent\njitter adds random noise to the point to allow you to see more of the points\n\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(\n    x = purposeful\n    , y = happy\n    )) + \n    geom_jitter() + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\nThis may be too much jitter\n\nSometimes we have data that have lots of repeating values, especially with ordinal response scales where the variables aren’t composited / latent\njitter adds random noise to the point to allow you to see more of the points\n\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(\n    x = purposeful\n    , y = happy\n    )) + \n    geom_jitter(width = .1, height = .1) + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\nalpha\nAlpha can help us understand how many points are stacked when using jitter (or other overlapping data)\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy, alpha = .25)) + \n    geom_jitter(\n      width = .1\n      , height = .1\n      # , alpha = .25\n      ) + \n    theme_classic() # I just hate grey backgrounds"
  },
  {
    "objectID": "ggplot2.html#geom_smooth",
    "href": "ggplot2.html#geom_smooth",
    "title": "Introduction to ggplot2",
    "section": "geom_smooth()",
    "text": "geom_smooth()\n\n\ngeom_smooth() allows you to apply statistical functions to your data\nThere are other ways to do this that we won’t cover today\nCore arguments are:\n\n\nmethod: “loess”, “lm”, “glm”, “gam”\n\nformula: e.g., y ~ x or y ~ poly(x, 2)\n\n\nse: display standard error of estimate (T/ F)\n\naes() wrapped aesthetics or directly mapped aesthetics\n\n\n\nRemember: it’s a LAYERED grammar of graphics, so let’s layer!\nse = F\nmethod = \"lm\"\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"lm\"\n      , formula = y ~ x\n      , se = F\n      , color = \"blue\"\n    ) + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\nmethod = \"loess\"\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"loess\"\n      , formula = y ~ x\n      , se = F\n      , color = \"blue\"\n    ) + \n    theme_classic() # I just hate grey backgrounds\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : pseudoinverse used at 4.015\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : neighborhood radius 1.015\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : reciprocal condition number 0\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : There are other near singularities as well. 1\n\n\n\n\n\n\n\n\nse=T\nAnd we can add standard error ribbons\nmethod = \"lm\"\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"lm\"\n      , formula = y ~ x\n      , se = T\n      , color = \"blue\"\n    ) + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\nmethod = \"loess\"\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"loess\"\n      , formula = y ~ x\n      , se = T\n      , color = \"blue\"\n    ) + \n    theme_classic() # I just hate grey backgrounds\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : pseudoinverse used at 4.015\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : neighborhood radius 1.015\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : reciprocal condition number 0\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric =\nparametric, : There are other near singularities as well. 1\n\n\nWarning in predLoess(object$y, object$x, newx = if\n(is.null(newdata)) object$x else if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : pseudoinverse used at\n4.015\n\n\nWarning in predLoess(object$y, object$x, newx = if\n(is.null(newdata)) object$x else if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : neighborhood radius\n1.015\n\n\nWarning in predLoess(object$y, object$x, newx = if\n(is.null(newdata)) object$x else if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : reciprocal condition\nnumber 0\n\n\nWarning in predLoess(object$y, object$x, newx = if\n(is.null(newdata)) object$x else if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : There are other near\nsingularities as well. 1"
  },
  {
    "objectID": "ggplot2.html#geom_hlinegeom_vline",
    "href": "ggplot2.html#geom_hlinegeom_vline",
    "title": "Introduction to ggplot2",
    "section": "\ngeom_hline()/geom_vline()\n",
    "text": "geom_hline()/geom_vline()\n\n\nSometimes, we will want to place lines at various intercepts\nWe’ll get into specific use cases as the course progresses\n\ngeom_hline(): horizontal lines have yintercept mappings\n\ngeom_vline(): vertical lines have xintercept mappings\n\ngeom_hline()\nHorizontal lines have yintercept mappings\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_hline(\n      aes(yintercept = mean(happy, na.rm = T))\n      , linetype = \"dashed\"\n      ) + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, color = \"blue\") + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\ngeom_vline()\nVertical lines have xintercept mappings\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_vline(\n      aes(xintercept = mean(purposeful, na.rm = T))\n      , linetype = \"dashed\"\n      ) + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, color = \"blue\") + \n    theme_classic() # I just hate grey backgrounds"
  },
  {
    "objectID": "ggplot2.html#geom_bar",
    "href": "ggplot2.html#geom_bar",
    "title": "Introduction to ggplot2",
    "section": "geom_bar()",
    "text": "geom_bar()\n\nBar graphs can be useful for showing relative differences\nMy hot take is that they are rarely that useful\n\n(This is mostly because of how we perceive errorbars and differences, which we’ll talk more about in a few weeks!)\n\n\nBut let’s look at using them for frequency and means / se’s\n\nFrequency\nHow often did our participant have an argument, interact with others, study, and feel tired?\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>% \n  select(SID, Full_Date, argument, interacted, study, tired) %>%\n  pivot_longer(\n    cols = argument:tired\n    , names_to = \"item\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(item) %>%\n  summarize(value = sum(value == 1)) %>%\n  ggplot(aes(x = item, fill = item, y = value)) + \n    geom_col(color = \"black\") + \n    theme_classic()\n\n\n\n\n\n\n\nMean differences\nWere there mean-level in our continuous variables?\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\")) %>%\n  select(SID, Full_Date, happy, purposeful, afraid, attentive) %>%\n  pivot_longer(\n    cols = c(-SID, -Full_Date)\n    , names_to = \"item\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(item) %>%\n  summarize(\n    mean = mean(value)\n    , ci = 1.96*(sd(value)/sqrt(n()))\n    ) %>%\n  ggplot(aes(x = item, fill = item, y = mean)) + \n    geom_col(color = \"black\") + \n    geom_errorbar(\n      aes(ymin = mean - ci, ymax = mean + ci)\n      , position = position_dodge(width = .1)\n      , width = .1\n      , stat = \"identity\"\n    ) + \n    theme_classic()"
  },
  {
    "objectID": "ggplot2.html#geom_boxplot",
    "href": "ggplot2.html#geom_boxplot",
    "title": "Introduction to ggplot2",
    "section": "geom_boxplot()",
    "text": "geom_boxplot()\n\nSometimes called box and whisker plots\nA method for summarizing a distribution of data without showing raw data\nBox instead shows 25th, 50th, and 75th percentile (quartiles)\nWhiskers show 1.5 * interquartile range (75%tile-25%tile)\nMore fun when we want to compare distributions across variables (IMO)\n\nOne boxplot\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(aes(y = SID, x = happy)) + \n    geom_boxplot(width = .5) + \n    theme_classic()\n\n\n\n\n\n\n\nMultiple boxplots\nMultiple Participants\n\nLater, we’ll also talk about how to order the boxplots (and other axes) by means, medians, etc.\n\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\", \"211\", \"174\", \"150\", \"171\")) %>%\n  ggplot(aes(\n    y = SID, x = happy\n    , fill = SID\n    )) + \n    geom_boxplot(width = .5) + \n    theme_classic()\n\n\n\n\n\n\n\nMultiple Variables\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\")) %>%\n  select(SID, Full_Date, happy, purposeful, afraid, attentive) %>%\n  pivot_longer(\n    cols = c(-SID, -Full_Date)\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) %>%\n  ggplot(aes(\n    y = item\n    , x = value\n    , fill = item\n    )) + \n    geom_boxplot(width = .5) + \n    theme_classic()\n\n\n\n\n\n\n\nAdvanced!\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    scale_x_continuous(limits = c(1,7), breaks = seq(1,5,2)) + \n    scale_y_continuous(limits = c(1,7), breaks = seq(1,5,2)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_boxplot(aes(\n      x = 6\n      , y = happy\n      )) + \n    geom_boxplot(aes(\n      y = 6\n      , x = purposeful\n      )) +\n    geom_smooth(\n      method = \"lm\"\n      , formula = y ~ x\n      , se = F\n      , color = \"blue\"\n      ) + \n    theme_classic() # I just hate grey backgrounds\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "ggplot2.html#geom_histogram-geom_density",
    "href": "ggplot2.html#geom_histogram-geom_density",
    "title": "Introduction to ggplot2",
    "section": "\ngeom_histogram() & geom_density()\n",
    "text": "geom_histogram() & geom_density()\n\n\nUseful for showing raw / smoothed distributions of data\n\nHistogram\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(aes(y = happy)) + \n    geom_histogram(\n      fill = \"grey\"\n      , color = \"black\"\n    ) + \n    coord_flip() +\n    theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nDensity Distribution\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(aes(x = happy, y = ..density..)) + \n    geom_histogram(\n      fill = \"grey\"\n      , color = \"black\"\n    ) + \n    theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nMultiple histograms / density distributions\n\nWe can compare multiple participants\n\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\", \"211\", \"174\", \"150\", \"171\")) %>%\n  ggplot(aes(\n     y = happy\n    , fill = SID\n    )) + \n    geom_density(alpha = .2) + \n    coord_flip() +\n    theme_classic()"
  },
  {
    "objectID": "ggplot2.html#facets",
    "href": "ggplot2.html#facets",
    "title": "Introduction to ggplot2",
    "section": "Facets",
    "text": "Facets\n\nOften, we have lots of other reasons we need to reproduce the same plot multiple times\n\nmultiple variables\nmultiple people\nmultiple conditions\netc.\n\n\nThere are more ways to do this than we’ll cover today, like piecing plots together and more\n\n\n\n\n\n\n\n\n\n\nThe core of directly faceting within ggplot is that you have to facet according to variables in your data set\nThis is part of why we covered moving your data to long\nSay that you want to facet by variable, for example, but your data is in wide form\n\nFacets couldn’t handle that\n\n\n\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  select(SID, beep, afraid:content) %>%\n  pivot_longer(\n    cols = afraid:content\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) \n\n\n\n  \n\n\n\nIn ggplot2, there are two core faceting functions * facet_grid() + snaps figures in a grid; no wrapping + especially useful for 1-2 faceting variables * facet_wrap() + treats each facet a separate + wraps according to nrow and ncol arguments\nfacet_grid()\nCore arguments:\n\n\nrows, cols: list of variables or formula, e.g., x ~ y\n\n\nscales: same x or y scale on all facets?\n\nspace: same space for unequal length x or y facets?\n\nswitch: move labels from left to right or top to bottom?\n\ndrop: drop unused factor levels\n\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  select(SID, beep, afraid:content) %>%\n  pivot_longer(\n    cols = afraid:content\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) %>%\n  ggplot(aes(x = beep, y = value, group = item)) + \n    geom_point() + \n    geom_line() + \n    facet_grid(item~.) +\n    theme_classic()\n\n\n\n\n\n\n\nfacet_wrap()\nCore arguments:\n\n\nfacets: barequoted or one-sided formula, e.g., ~ x + y\n\n\nnrow / ncol: number of rows and columns\n\nscales: same x or y scale on all facets?\n\nswitch: move labels from left to right or top to bottom?\n\ndrop: drop unused factor levels\n\ndir: horizontal or vertical\n\nstrip.position: where to put the labels\n\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  select(SID, beep, afraid:content) %>%\n  pivot_longer(\n    cols = afraid:content\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) %>%\n  ggplot(aes(x = beep, y = value, group = item)) + \n    geom_point() + \n    geom_line() + \n    facet_wrap(\n      ~item\n      , ncol = 1\n      , strip.position = \"right\"\n      ) +\n    theme_classic()\n\n\n\n\n\n\n\nChange scale and space\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  select(SID, beep, afraid:content) %>%\n  pivot_longer(\n    cols = afraid:content\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) %>%\n  ggplot(aes(x = beep, y = value, group = item)) + \n    geom_point() + \n    geom_line() + \n    facet_grid(\n      item ~ . \n      , scales = \"free_y\"\n      , space = \"free_y\"\n      ) +\n    theme_classic()"
  },
  {
    "objectID": "ggplot2.html#labels-titles",
    "href": "ggplot2.html#labels-titles",
    "title": "Introduction to ggplot2",
    "section": "Labels & Titles",
    "text": "Labels & Titles\n\nAPA style says titles are bad\nCommon sense says titles help understanding\nAsk for forgiveness, not permission\n\nRemember this?\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"lm\"\n      , formula = y ~ x\n      , se = T\n      , color = \"blue\"\n    ) + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\nWe can add labels and a title\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"lm\"\n      , formula = y ~ x\n      , se = T\n      , color = \"blue\"\n    ) + \n    labs(\n      x = \"Momentary Purpose (1-5)\"\n      , y = \"Momentary Happiness (1-5)\"\n      , title = \"Zero-Order Associations \n                  Between Momentary Happiness and Purpose\"\n    ) + \n    theme_classic() # I just hate grey backgrounds\n\n\n\n\n\n\n\nLabels also apply to other mappings like color\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\", \"211\", \"174\", \"150\", \"171\")) %>%\n  ggplot(aes(\n     y = happy\n    , fill = SID\n    )) + \n    geom_density(alpha = .2) + \n    coord_flip() +\n    labs(\n      x = \"Momentary Happiness\"\n      , y = \"Smoothed Density\"\n      , fill = \"Participant\\nID\"\n    ) + \n    theme_classic()\n\n\n\n\n\n\n\nYou can also use labels to remove axis labels\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\")) %>%\n  select(SID, Full_Date, happy, purposeful, afraid, attentive) %>%\n  pivot_longer(\n    cols = c(-SID, -Full_Date)\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) %>%\n  ggplot(aes(\n    y = item\n    , x = value\n    , fill = item\n    )) +\n    geom_boxplot(width = .5) + \n    labs(\n      x = \"Momentart Rating (1-5)\"\n      , y = NULL\n      , fill = \"Item\"\n    ) + \n    theme_classic()"
  },
  {
    "objectID": "ggplot2.html#themes",
    "href": "ggplot2.html#themes",
    "title": "Introduction to ggplot2",
    "section": "Themes",
    "text": "Themes\nBasic, Built-in Themes\n\nThere are lots of themes you can use in ggplot that are pre-built into the package\nTry tying theme_ into your R console, and look at the functions that pop up\nSome stand-out ones are:\n\n\ntheme_classic() (what we’ve been using)\ntheme_bw()\n\ntheme_minimal() (but is there a theme_maximal?)\ntheme_void\n\n\nAdvanced Themes\n\nCustom themes are one of the best ways to “hack” your ggplots\nYou will not remember all of them\nYou will have to google them all time\nHere’s the site: https://ggplot2.tidyverse.org/reference/theme.html\n\nRather than give details on a bunch of these, I’m going to demonstrate theme modifications I often use\n\nSmoothed Regression Line\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  ggplot(mapping = aes(x = purposeful, y = happy)) + \n    geom_jitter(width = .1, height = .1, alpha = .25) + \n    geom_smooth(\n      method = \"lm\"\n      , formula = y ~ x\n      , se = T\n      , color = \"blue\"\n    ) + \n    labs(\n      x = \"Momentary Purpose (1-5)\"\n      , y = \"Momentary Happiness (1-5)\"\n      , title = \"Zero-Order Associations Between Momentary Happiness and Purpose\"\n    ) + \n    theme_classic() + \n    theme(\n      plot.title = element_text(\n        face = \"bold\"\n        , size = rel(1.1)\n        , hjust = .5\n        )\n      , axis.title = element_text(\n        face = \"bold\"\n        , size = rel(1.1)\n        )\n      , axis.text = element_text(\n        face = \"bold\"\n        , size = rel(1.2)\n        )\n    )\n\n\n\n\n\n\n\nBar Chart\n\nCodeipcs_data %>%\n  filter(SID %in% c(\"216\")) %>%\n  select(SID, Full_Date, happy, purposeful, afraid, attentive) %>%\n  pivot_longer(\n    cols = c(-SID, -Full_Date)\n    , names_to = \"item\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(item) %>%\n  summarize(\n    mean = mean(value)\n    , ci = 1.96*(sd(value)/sqrt(n()))\n    ) %>%\n  ggplot(aes(x = item, fill = item, y = mean)) + \n    geom_col(color = \"black\") + \n    geom_errorbar(\n      aes(ymin = mean - ci, ymax = mean + ci)\n      , position = position_dodge(width = .1)\n      , width = .1\n      , stat = \"identity\"\n    ) + \n    labs(\n      x = NULL\n      , y = \"Mean Momentary Rating (CI)\\n[Range 1-5]\"\n      , title = \"Descriptive Statistics of Momentary Emotion Ratings\"\n    ) + \n    theme_classic() + \n    theme(\n      legend.position = \"none\"\n      , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n      , axis.title = element_text(face = \"bold\", size = rel(1.1))\n      , axis.text = element_text(face = \"bold\", size = rel(1.2))\n  )\n\n\n\n\n\n\n\nTime Series\n\nCodeipcs_data %>%\n  filter(SID == \"216\") %>%\n  select(SID, beep, afraid:content) %>%\n  pivot_longer(\n    cols = afraid:content\n    , names_to = \"item\"\n    , values_to = \"value\"\n  ) %>%\n  ggplot(aes(x = beep, y = value, group = item)) + \n    geom_line(aes(color = item)) + \n    geom_point(size = 1) + \n    facet_grid(item~.) +\n    labs(\n      x = \"ESM Beep (#)\"\n      , y = \"Rated Momentary Value (1-5)\"\n      , title = \"Time Series of Four Momentary Emotion Items for Participant 216\"\n      , color = NULL\n      ) + \n    theme_classic() + \n    theme(\n      legend.position = \"bottom\"\n      , legend.text = element_text(face = \"bold\", size = rel(1.1))\n      , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n      , axis.title = element_text(face = \"bold\", size = rel(1.1))\n      , axis.text = element_text(face = \"bold\", size = rel(1.2))\n      , strip.background = element_rect(color = \"black\", fill = \"cornflowerblue\")\n      , strip.text = element_text(face = \"bold\", size = rel(1.2), color = \"white\")\n    )"
  },
  {
    "objectID": "publications.html#section-5",
    "href": "publications.html#section-5",
    "title": "Publications",
    "section": "2017",
    "text": "2017\nBeck, E. D. and Jackson, J. J. (2017). The search for a bridge: Idiographic personality networks. European Journal of Personality, 31:530–532."
  },
  {
    "objectID": "about.html#awards",
    "href": "about.html#awards",
    "title": "About",
    "section": "Awards",
    "text": "Awards\n2023 | Walter G. Klopfer Award for the Best Empirical Paper, Journal of Personality Assessment\n2022 | Tanaka Dissertation Award Finalist, Association for Research in Personality\n2021 | Emerging Scholar Award, Association for Research in Personality\n2021 | Dissertation Award Finalist, European Association of Personality Psychology\n2018 | Departmental Teaching Award, Psychological and Brain Sciences, Washington University in St. Louis\n2018 | Summer School in Personality, European Association of Personality Psychology\n2017 | Scholarship and Travel Award, The Society of Young Network Scientists\n2017 | Student Travel Award, Society for Personality and Social Psychology\n2016 | Muriel Fain Sher Premium in Psychology, Brown University\n2016 | Kling Premium for Excellence in Teaching, Brown University"
  },
  {
    "objectID": "about.html#grants",
    "href": "about.html#grants",
    "title": "About",
    "section": "Grants",
    "text": "Grants\n2018 | Visionary Grant, American Psychological Foundation, Co-PI ($19,268)\n2018 | Inside the Grant Panel, Society for Personality and Social Psychology ($5000)\n2018 | Predoctoral NRSA, National Institutes of Aging T32 AG00030-3 ($49,140)\n2015 | Undergraduate Teaching and Research Award, Brown University ($3,500)\n2014 | Linking Internships and Knowledge, Brown University ($3,000)"
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Codebooks and Workflow",
    "section": "",
    "text": "Download .Rmd (won’t work in Safari or IE)\nSee GitHub Repository"
  },
  {
    "objectID": "workflow.html#packages",
    "href": "workflow.html#packages",
    "title": "Codebooks and Workflow",
    "section": "Packages",
    "text": "Packages\nPackages seems like the most basic step, but it is actually very important. ALWAYS LOAD YOUR PACKAGES IN A VERY INTENTIONAL ORDER AT THE BEGINNING OF YOUR SCRIPT. Package conflicts suck, so it needs to be shouted.\nFor this tutorial, we are going to quite simple. We will load the psych package for data descriptives, some options for cleaning and reverse coding, and some evaluations of our scales. The plyr package is the predecessor of the dplyr package, which is a core package of the tidyverse, which you will become quite familiar with in these tutorials. I like the plyr package because it contains a couple of functions (e.g. mapvalues()) that I find quite useful. Finally, we load the tidyverse package, which is actually a complilation of 8 packages. Some of these we will use today and some we will use in later tutorials. All are very useful and are arguably some of the most powerful tools R offers.\n\n# load packages\nlibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "workflow.html#codebook",
    "href": "workflow.html#codebook",
    "title": "Codebooks and Workflow",
    "section": "Codebook",
    "text": "Codebook\nThe second step is a codebook. Arguably, this is the first step because you should create the codebook long before you open R and load your data.\nIn this case, we are going to using some data from the German Socioeconomic Panel Study (GSOEP), which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset, which I, as a contracted SOEP user, can use for teaching purposes. These data represent select cases from the full data set and should not be used for the purpose of publication. The full data are available for free at https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html.\nFor this tutorial, I created the codebook for you (Download (won’t work in Safari or IE)), and included what I believe are the core columns you may need. Some of these columns may not be particularly helpful for every dataset.\nHere are my core columns that are based on the original data:\n1. dataset: this column indexes the name of the dataset that you will be pulling the data from. This is important because we will use this info later on (see purrr tutorial) to load and clean specific data files. Even if you don’t have multiple data sets, I believe consistency is more important and suggest using this.\n2. old_name: this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to select() variables from the original data file and rename them something that is more useful to us.\n3. item_text: this column is the original text that participants saw or a description of the item.\n4. category: broad categories that different variables can be put into. I’m a fan of naming them things like “outcome”, “predictor”, “moderator”, “demographic”, “procedural”, etc. but sometimes use more descriptive labels like “Big 5” to indicate the model from which the measures are derived.\n5. label: label is basically one level lower than category. So if the category is Big 5, the label would be, or example, “A” for Agreeableness, “SWB” for subjective well-being, etc. This column is most important and useful when you have multiple items in a scales, so I’ll typically leave this blank when something is a standalone variable (e.g. sex, single-item scales, etc.).\n6. item_name: This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be “kind” for Agreebleness or “sex” for the demographic biological sex variable.\n7. new_name: This is a column that brings together much of the information we’ve already collected. It’s purpose is to be the new name that we will give to the variable that is more useful and descriptive to us. This is a constructed variable that brings together others. I like to make it a combination of “category”, “label”, “item_name”, and year using varying combos of “_” and “.” that we can use later with tidyverse functions. I typically construct this variable in Excel using the CONCATENATE() function, but it could also be done in R. The reason I do it in Excel is that it makes it easier for someone who may be reviewing my codebook.\n8. scale: this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc. This is helpful for knowing the plausible range. 9. recode: sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I’ll do to a variable for transparency.\nHere are additional columns that will make our lives easier or are applicable to some but not all data sets:\n10. reverse: this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse) for reasons that will become clear later.\n11. mini: this column represents the minimum value of scales that are numeric. Leave blank otherwise.\n12. maxi: this column represents the maximumv alue of scales that are numeric. Leave blank otherwise.\n13. year: for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it’s important to note to which wave an item belongs. You can do this by noting the wave (e.g. 1, 2, 3), but I prefer the actual year the data were collected (e.g. 2005, 2009, etc.)\n15. meta: Some datasets have a meta name, which essentially means a name that variable has across all waves to make it clear which variables are the same. They are not always useful as some data sets have meta names but no great way of extracting variables using them. But they’re still typically useful to include in your codebook regardless.\n\nKey Takeaways from Codebook Use\n\nReproducibility and Transparency\n\nCreating Composites\n\nKeeping Track of Classes of Variables\n\nEasier Reverse-Coding\n\nEasier Recoding\n\nYour future self will thank you.\n\nBelow, I will demonstrate each of these.\n\n\nRead in the Codebook\nBelow, I’ll load in the codebook we will use for this study, which will include all of the above columns.\n\n# set the path\nwd <- \"https://github.com/emoriebeck/R-tutorials/blob/master/06_workflow\"\n\ndownload.file(\n  url      = sprintf(\"%s/data/codebook.csv?raw=true\", wd), \n  destfile = sprintf(\"%s/data/codebook.csv\", data_path)\n  )\n\n# load the codebook\n(codebook <- sprintf(\"%s/data/codebook.csv\", data_path) %>% \n    read_csv(.) %>%\n    mutate(old_name = str_to_lower(old_name)))\n\n# A tibble: 153 × 13\n   dataset old_name item_text  scale categ…¹ label item_…²  year new_n…³ reverse\n   <chr>   <chr>    <chr>      <chr> <chr>   <chr> <chr>   <dbl> <chr>     <dbl>\n 1 <NA>    persnr   Never Cha…  <NA> Proced… <NA>  SID         0 Proced…       1\n 2 <NA>    hhnr     household…  <NA> Proced… <NA>  househ…     0 Proced…       1\n 3 ppfad   gebjahr  Year of B… \"num… Demogr… <NA>  DOB         0 Demogr…       1\n 4 ppfad   sex      Sex        \"\\n1… Demogr… <NA>  Sex         0 Demogr…       1\n 5 vp      vp12501  Thorough …  <NA> Big 5   C     thorou…  2005 Big 5_…       1\n 6 zp      zp12001  Thorough …  <NA> Big 5   C     thorou…  2009 Big 5_…       1\n 7 bdp     bdp15101 Thorough …  <NA> Big 5   C     thorou…  2013 Big 5_…       1\n 8 vp      vp12502  Am commun…  <NA> Big 5   E     commun…  2005 Big 5_…       1\n 9 zp      zp12002  Am commun…  <NA> Big 5   E     commun…  2009 Big 5_…       1\n10 bdp     bdp15102 Am commun…  <NA> Big 5   E     commun…  2013 Big 5_…       1\n# … with 143 more rows, 3 more variables: mini <dbl>, maxi <dbl>, recode <chr>,\n#   and abbreviated variable names ¹​category, ²​item_name, ³​new_name"
  },
  {
    "objectID": "workflow.html#data",
    "href": "workflow.html#data",
    "title": "Codebooks and Workflow",
    "section": "Data",
    "text": "Data\nFirst, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel Study, which is a longitudinal study of German households that has been conducted since 1984. We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.\nNote: we will be using the teaching set of the GSOEP data set. I will not be pulling from the raw files as a result of this. I will also not be mirroring the format that you would usually load the GSOEP from because that is slightly more complicated and somethng we will return to in a later tutorial on purrr (link) after we have more skills. I’ve left that code for now, but it won’t make a lot of sense right now.\n\npath <- \"~/Box/network/other projects/PCLE Replication/data/sav_files\"\nref <- sprintf(\"%s/cirdef.sav\", path) %>% haven::read_sav(.) %>% select(hhnr, rgroup20)\nread_fun <- function(Year){\n  vars <- (codebook %>% filter(year == Year | year == 0))$old_name\n  set <- (codebook %>% filter(year == Year))$dataset[1]\n  sprintf(\"%s/%s.sav\", path, set) %>% haven::read_sav(.) %>%\n    full_join(ref) %>%\n    filter(rgroup20 > 10) %>%\n    select(one_of(vars)) %>%\n    gather(key = item, value = value, -persnr, -hhnr, na.rm = T)\n}\n\nvars <- (codebook %>% filter(year == 0))$old_name\ndem <- sprintf(\"%s/ppfad.sav\", path) %>% \n  haven::read_sav(.) %>%\n  select(vars)\n  \ntibble(year = c(2005:2015)) %>%\n  mutate(data = map(year, read_fun)) %>%\n  select(-year) %>% \n  unnest(data) %>%\n  distinct() %>% \n  filter(!is.na(value)) %>%\n  spread(key = item, value = value) %>%\n  left_join(dem) %>%\n  write.csv(., file = \"~/Documents/Github/R-tutorials/ALDA/week_1_descriptives/data/week_1_data.csv\", row.names = F)\n\nThis code below shows how I would read in and rename a wide-format data set using the codebook I created.\n\n# download the file\ndownload.file(\n  url      = sprintf(\"%s/data/workflow_data.csv?raw=true\", wd), \n  destfile = sprintf(\"%s/data/workflow_data.csv\", data_path)\n  )\n\nold.names <- codebook$old_name # get old column names\nnew.names <- codebook$new_name # get new column names\n\n(soep <- sprintf(\"%s/data/workflow_data.csv\", data_path) %>% # path to data\n  read_csv(.) %>% # read in data\n  select(old.names) %>% # select the columns from our codebook\n  setNames(new.names)) # rename columns with our new names\n\n# A tibble: 28,290 × 153\n   Procedural_…¹ Proce…² Demog…³ Demog…⁴ Big 5…⁵ Big 5…⁶ Big 5…⁷ Big 5…⁸ Big 5…⁹\n           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1           901      94    1951       2       6      -1       7       4       4\n 2          1202     124    1913       2       7      NA      NA       6      NA\n 3          2301     230    1946       1       7       6       6       6       4\n 4          2302     230    1946       2       6       7       7       5       5\n 5          2304     230    1978       1       5      NA      NA       7      NA\n 6          2305     230    1946       2      NA      NA      NA      NA      NA\n 7          4601     469    1933       2       6      NA      NA       6      NA\n 8          4701     477    1919       2       6       6      NA       6       5\n 9          4901     493    1925       2       5       6       6       7       6\n10          5201     523    1955       1       7       7       7       5       6\n# … with 28,280 more rows, 144 more variables: `Big 5__E_communic.2013` <dbl>,\n#   `Big 5__A_coarse.2005` <dbl>, `Big 5__A_coarse.2009` <dbl>,\n#   `Big 5__A_coarse.2013` <dbl>, `Big 5__O_original.2005` <dbl>,\n#   `Big 5__O_original.2009` <dbl>, `Big 5__O_original.2013` <dbl>,\n#   `Big 5__N_worry.2005` <dbl>, `Big 5__N_worry.2009` <dbl>,\n#   `Big 5__N_worry.2013` <dbl>, `Big 5__A_forgive.2005` <dbl>,\n#   `Big 5__A_forgive.2009` <dbl>, `Big 5__A_forgive.2013` <dbl>, …"
  },
  {
    "objectID": "workflow.html#recode-variables",
    "href": "workflow.html#recode-variables",
    "title": "Codebooks and Workflow",
    "section": "Recode Variables",
    "text": "Recode Variables\nMany of the data we work with have observations that are missing for a variety of reasons. In R, we treat missing values as NA, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit NA values.\nIn the GSOEP, -1 to -7 indicate various types of missing values, so we will recode these to NA. To do this, we will use one of my favorite functions, mapvalues(), from the plyr package. In later tutorials where we read in and manipulate more complex data sets, we will use mapvalues() a lot. Basically, mapvalues takes 4 key arguments: (1) the variable you are recoding, (2) a vector of initial values from which you want to (3) recode your variable to using a vector of new values in the same order as the old values, and (4) a way to turn off warnings if some levels are not in your data (warn_missing = F).\n\n(soep <- soep %>%\n  mutate_all(~as.numeric(mapvalues(., from = seq(-1,-7, -1), # recode negative \n                to = rep(NA, 7), warn_missing = F)))) # values to NA\n\n# A tibble: 28,290 × 153\n   Procedural_…¹ Proce…² Demog…³ Demog…⁴ Big 5…⁵ Big 5…⁶ Big 5…⁷ Big 5…⁸ Big 5…⁹\n           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1           901      94    1951       2       6      NA       7       4       4\n 2          1202     124    1913       2       7      NA      NA       6      NA\n 3          2301     230    1946       1       7       6       6       6       4\n 4          2302     230    1946       2       6       7       7       5       5\n 5          2304     230    1978       1       5      NA      NA       7      NA\n 6          2305     230    1946       2      NA      NA      NA      NA      NA\n 7          4601     469    1933       2       6      NA      NA       6      NA\n 8          4701     477    1919       2       6       6      NA       6       5\n 9          4901     493    1925       2       5       6       6       7       6\n10          5201     523    1955       1       7       7       7       5       6\n# … with 28,280 more rows, 144 more variables: `Big 5__E_communic.2013` <dbl>,\n#   `Big 5__A_coarse.2005` <dbl>, `Big 5__A_coarse.2009` <dbl>,\n#   `Big 5__A_coarse.2013` <dbl>, `Big 5__O_original.2005` <dbl>,\n#   `Big 5__O_original.2009` <dbl>, `Big 5__O_original.2013` <dbl>,\n#   `Big 5__N_worry.2005` <dbl>, `Big 5__N_worry.2009` <dbl>,\n#   `Big 5__N_worry.2013` <dbl>, `Big 5__A_forgive.2005` <dbl>,\n#   `Big 5__A_forgive.2009` <dbl>, `Big 5__A_forgive.2013` <dbl>, …"
  },
  {
    "objectID": "workflow.html#reverse-scoring",
    "href": "workflow.html#reverse-scoring",
    "title": "Codebooks and Workflow",
    "section": "Reverse-Scoring",
    "text": "Reverse-Scoring\nMany scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.\nThere are a few ways to do this in R. Below, I’ll demonstrate how to do so using the reverse.code() function in the psych package in R. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).\nBefore we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook. We will talk more about what’s happening here in later tutorials on tidyr, so for now, just bear with me.\n\n(soep_long <- soep %>%\n  gather(key = item, value = value, -contains(\"Procedural\"), # change to long format\n         -contains(\"Demographic\"), na.rm = T) %>%\n  left_join(codebook %>% select(item = new_name, reverse, mini, maxi)) %>% # bring in codebook\n  separate(item, c(\"type\", \"item\"), sep = \"__\") %>% # separate category\n  separate(item, c(\"item\", \"year\"), sep = \"[.]\") %>% # seprate year\n  separate(item, c(\"item\", \"scrap\"), sep = \"_\") %>% # separate scale and item\n  mutate(value = as.numeric(value), # change to numeric\n         value = ifelse(reverse == -1, \n            reverse.code(-1, value, mini = mini, maxi = maxi), value)))\n\n# A tibble: 471,722 × 12\n   Procedu…¹ Proce…² Demog…³ Demog…⁴ type  item  scrap year  value reverse  mini\n       <dbl>   <dbl>   <dbl>   <dbl> <chr> <chr> <chr> <chr> <dbl>   <dbl> <dbl>\n 1       901      94    1951       2 Big 5 C     thor… 2005      6       1     1\n 2      1202     124    1913       2 Big 5 C     thor… 2005      7       1     1\n 3      2301     230    1946       1 Big 5 C     thor… 2005      7       1     1\n 4      2302     230    1946       2 Big 5 C     thor… 2005      6       1     1\n 5      2304     230    1978       1 Big 5 C     thor… 2005      5       1     1\n 6      4601     469    1933       2 Big 5 C     thor… 2005      6       1     1\n 7      4701     477    1919       2 Big 5 C     thor… 2005      6       1     1\n 8      4901     493    1925       2 Big 5 C     thor… 2005      5       1     1\n 9      5201     523    1955       1 Big 5 C     thor… 2005      7       1     1\n10      5202     523    1956       2 Big 5 C     thor… 2005      6       1     1\n# … with 471,712 more rows, 1 more variable: maxi <dbl>, and abbreviated\n#   variable names ¹​Procedural__SID, ²​Procedural__household, ³​Demographic__DOB,\n#   ⁴​Demographic__Sex"
  },
  {
    "objectID": "workflow.html#create-composites",
    "href": "workflow.html#create-composites",
    "title": "Codebooks and Workflow",
    "section": "Create Composites",
    "text": "Create Composites\nNow that we have reverse coded our items, we can create composites.\n\nBFI-S\nWe’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.\nHere’s the simplest way, which is also the long way because you’d have to do it for each scale in each year, which I don’t recommend.\n\nsoep$C.2005 <- with(soep, rowMeans(cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, \n                          `Big 5__C_efficient.2005`), na.rm = T)) \n\nBut personally, I don’t have a desire to do that 15 times, so we can use our codebook and dplyr to make our lives a whole lot easier.\n\nsoep <- soep %>% select(-C.2005) # get rid of added column\n\n(b5_soep_long <- soep_long %>%\n  filter(type == \"Big 5\") %>% # keep Big 5 variables\n  group_by(Procedural__SID, item, year) %>% # group by person, construct, & year\n  summarize(value = mean(value, na.rm = T)) %>% # calculate means\n  ungroup() %>% # ungroup\n  left_join(soep_long %>% # bring demographic info back in \n    select(Procedural__SID, DOB = Demographic__DOB, Sex = Demographic__Sex) %>%\n    distinct()))\n\n# A tibble: 151,186 × 6\n   Procedural__SID item  year  value   DOB   Sex\n             <dbl> <chr> <chr> <dbl> <dbl> <dbl>\n 1             901 A     2005   4.67  1951     2\n 2             901 A     2009   5     1951     2\n 3             901 A     2013   4.67  1951     2\n 4             901 C     2005   5     1951     2\n 5             901 C     2009   5     1951     2\n 6             901 C     2013   5.67  1951     2\n 7             901 E     2005   3.67  1951     2\n 8             901 E     2009   3.67  1951     2\n 9             901 E     2013   3.67  1951     2\n10             901 N     2005   3.33  1951     2\n# … with 151,176 more rows\n\n\n\n\nLife Events\nWe also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).\n\n(events_long <- soep_long %>%\n  filter(type == \"Life Event\") %>% # keep only life events\n  group_by(Procedural__SID, item) %>% # group by person and event\n  summarize(le_value = sum(value, na.rm = T), # sum up whether they experiened the event at all\n            le_value = ifelse(le_value > 1, 1, 0)) %>% # if more than once 1, otherwise 0\n   ungroup())\n\n# A tibble: 15,061 × 3\n   Procedural__SID item      le_value\n             <dbl> <chr>        <dbl>\n 1             901 MomDied          1\n 2            2301 MoveIn           0\n 3            2301 PartDied         1\n 4            2305 MoveIn           0\n 5            4601 PartDied         0\n 6            5201 ChldMvOut        1\n 7            5201 DadDied          0\n 8            5202 ChldMvOut        1\n 9            5203 MoveIn           1\n10            5303 MomDied          0\n# … with 15,051 more rows"
  },
  {
    "objectID": "workflow.html#create-combination-data-set",
    "href": "workflow.html#create-combination-data-set",
    "title": "Codebooks and Workflow",
    "section": "Create Combination Data Set",
    "text": "Create Combination Data Set\nFor the analyses I’d want to do with this data set, I’d want to combine every combination of personality traits with every combination of event we have data for each person for. In other words, I’m looking for a data frame that looks like this:\n\n\n\nSID\nyear\nEvent\nle_value\nTrait\np_value\n\n\n\n\n\n\n# Create combined data frame  \n(combined_long <- b5_soep_long %>% # take Big Five Data \n  spread(item, value) %>% # Spread it wide \n  full_join(events_long %>% rename(Event = item)) %>% # Join it with Events\n  filter(!is.na(Event)) %>% # take out people who don't have event data\n  gather(key = Trait, value = p_value, A:O)) # change to doubly long\n\n# A tibble: 138,820 × 8\n   Procedural__SID year    DOB   Sex Event    le_value Trait p_value\n             <dbl> <chr> <dbl> <dbl> <chr>       <dbl> <chr>   <dbl>\n 1             901 2005   1951     2 MomDied         1 A        4.67\n 2             901 2009   1951     2 MomDied         1 A        5   \n 3             901 2013   1951     2 MomDied         1 A        4.67\n 4            2301 2005   1946     1 MoveIn          0 A        5   \n 5            2301 2005   1946     1 PartDied        1 A        5   \n 6            2301 2009   1946     1 MoveIn          0 A        5.67\n 7            2301 2009   1946     1 PartDied        1 A        5.67\n 8            2301 2013   1946     1 MoveIn          0 A        6   \n 9            2301 2013   1946     1 PartDied        1 A        6   \n10            4601 2005   1933     2 PartDied        0 A        6.33\n# … with 138,810 more rows\n\n\n\nSave Your Data\n\n  write.csv(\n    x = combined_long\n    , file = sprintf(\"%s/data/clean_data_%s.csv\", data_path, Sys.Date())\n    , row.names = F\n    )"
  },
  {
    "objectID": "workflow.html#big-five-descriptives",
    "href": "workflow.html#big-five-descriptives",
    "title": "Codebooks and Workflow",
    "section": "Big Five Descriptives",
    "text": "Big Five Descriptives\nThere are lots of ways to create great tables of descriptives. My favorite way is using dplyr, but we will save that for a later lesson on creating great APA style tables in R. For now, we’ll use a wonderfully helpful function from the psych package called describe() in conjunction with a small amount of tidyr to reshape the data.\n\nb5_soep_long  %>%\n  unite(tmp, item, year, sep = \"_\") %>% # make new column that joins item and year\n  spread(tmp, value) %>% # make wide because that helps describe\n  describe(.) %>% # call describe \n  data.frame %>%\n  rownames_to_column(var = \"V\")\n\n                 V vars     n         mean           sd       median\n1  Procedural__SID    1 16719 8.321023e+06 1.067773e+07 3.105002e+06\n2              DOB    2 16719 1.960026e+03 1.847643e+01 1.960000e+03\n3              Sex    3 16719 1.524314e+00 4.994234e-01 2.000000e+00\n4           A_2005    4 10419 5.458281e+00 9.836785e-01 5.666667e+00\n5           A_2009    5 10294 5.348536e+00 9.853567e-01 5.333333e+00\n6           A_2013    6  9535 5.407726e+00 9.547886e-01 5.333333e+00\n7           C_2005    7 10412 5.899987e+00 9.559877e-01 6.000000e+00\n8           C_2009    8 10290 5.823275e+00 9.544427e-01 6.000000e+00\n9           C_2013    9  9530 5.839542e+00 9.146635e-01 6.000000e+00\n10          E_2005   10 10416 4.818756e+00 1.152964e+00 5.000000e+00\n11          E_2009   11 10291 4.772001e+00 1.147503e+00 4.666667e+00\n12          E_2013   12  9533 4.870642e+00 1.112361e+00 5.000000e+00\n13          N_2005   13 10413 4.046000e+00 1.226367e+00 4.000000e+00\n14          N_2009   14 10294 4.176753e+00 1.220681e+00 4.333333e+00\n15          N_2013   15  9534 4.249825e+00 1.210948e+00 4.333333e+00\n16          O_2005   16 10408 4.507286e+00 1.220377e+00 4.666667e+00\n17          O_2009   17 10287 4.398351e+00 1.215924e+00 4.333333e+00\n18          O_2013   18  9530 4.600437e+00 1.178126e+00 4.666667e+00\n        trimmed          mad  min      max    range        skew    kurtosis\n1  6.487615e+06 3735541.1688  901 35022002 35021101  1.50377658  0.55782194\n2  1.960218e+03      20.7564 1909     1995       86 -0.07556870 -0.84082302\n3  1.530388e+00       0.0000    1        2        1 -0.09736107 -1.99063988\n4  5.498161e+00       0.9884    1        7        6 -0.39523879 -0.16102753\n5  5.384086e+00       0.9884    1        7        6 -0.36634029 -0.18932735\n6  5.446367e+00       0.9884    1        7        6 -0.42492164  0.03645295\n7  6.007123e+00       0.9884    1        7        6 -0.95874147  0.86977466\n8  5.917861e+00       0.9884    1        7        6 -0.82052339  0.48848517\n9  5.920339e+00       0.9884    1        7        6 -0.71952576  0.18022243\n10 4.848152e+00       0.9884    1        7        6 -0.26500996 -0.16399435\n11 4.796712e+00       0.9884    1        7        6 -0.23569234 -0.16980025\n12 4.903523e+00       0.9884    1        7        6 -0.28970330 -0.17933079\n13 4.057816e+00       1.4826    1        7        6 -0.06916484 -0.31968532\n14 4.201190e+00       0.9884    1        7        6 -0.16849233 -0.29596620\n15 4.267239e+00       1.2355    1        7        6 -0.15031344 -0.30048828\n16 4.534302e+00       1.4826    1        7        6 -0.23155625 -0.17674732\n17 4.411250e+00       1.4826    1        7        6 -0.09636281 -0.29698790\n18 4.622661e+00       0.9884    1        7        6 -0.21091876 -0.20046540\n             se\n1  8.257980e+04\n2  1.428936e-01\n3  3.862458e-03\n4  9.636962e-03\n5  9.711837e-03\n6  9.777929e-03\n7  9.368827e-03\n8  9.408972e-03\n9  9.369467e-03\n10 1.129705e-02\n11 1.131162e-02\n12 1.139282e-02\n13 1.201801e-02\n14 1.203123e-02\n15 1.240190e-02\n16 1.196219e-02\n17 1.198842e-02\n18 1.206827e-02"
  },
  {
    "objectID": "workflow.html#big-five-distributions",
    "href": "workflow.html#big-five-distributions",
    "title": "Codebooks and Workflow",
    "section": "Big Five Distributions",
    "text": "Big Five Distributions\n\nb5_soep_long %>%\n  ggplot(aes(x = value)) + \n    geom_histogram() +\n    facet_grid(year ~ item) +\n    theme_classic()"
  },
  {
    "objectID": "workflow.html#life-events-descriptives",
    "href": "workflow.html#life-events-descriptives",
    "title": "Codebooks and Workflow",
    "section": "Life Events Descriptives",
    "text": "Life Events Descriptives\nFor count variables, like life events, we need to use something slightly different. We’re typically more interested in counts – in this case, how many people experienced each life event in the 10 years we’re considering?\nTo do this, we’ll use a little bit of dplyr rather than the base R function table() that is often used for count data. Instead, we’ll use a combination of group_by() and n() to get the counts by group. In the end, we’re left with a nice little table of counts.\n\nevents_long %>%\n  group_by(item, le_value) %>% \n  summarize(N = n()) %>%\n  ungroup() %>%\n  spread(le_value, N)\n\n# A tibble: 10 × 3\n   item        `0`   `1`\n   <chr>     <int> <int>\n 1 ChldBrth   1600   735\n 2 ChldMvOut  1555   830\n 3 DadDied     953   213\n 4 Divorce     414   122\n 5 Married    1646   331\n 6 MomDied     929   219\n 7 MoveIn     1403   419\n 8 NewPart    1207   420\n 9 PartDied    402    76\n10 SepPart    1172   415"
  },
  {
    "objectID": "workflow.html#life-event-plots",
    "href": "workflow.html#life-event-plots",
    "title": "Codebooks and Workflow",
    "section": "Life Event Plots",
    "text": "Life Event Plots\n\nevents_long %>%\n  mutate(le_value = mapvalues(le_value, 0:1, c(\"No Event\", \"Event\"))) %>%\n  ggplot(aes(x = le_value, fill = le_value)) +\n    scale_fill_manual(values = c(\"cornflowerblue\", \"gray\")) +\n    geom_bar(color = \"black\")  +\n    facet_wrap(~item, nrow = 2) +\n    theme_classic() +\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "workflow.html#scale-reliability",
    "href": "workflow.html#scale-reliability",
    "title": "Codebooks and Workflow",
    "section": "Scale Reliability",
    "text": "Scale Reliability\nWhen we work with scales, it’s often a good idea to check the internal consistency of your scale. If the scale isn’t performing how it should be, that could critically impact the inferences you make from your data.\nTo check the internal consistency of our Big 5 scales, we will use the alpha() function from the psych package, which will give us Cronbach’s as well as a number of other indicators of internal consistency.\nHere’s the way you may have seen / done this in the past.\n\nalpha.C.2005 <- with(soep, psych::alpha(x = cbind(`Big 5__C_thorough.2005`, \n                                  `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`)))\n\nSome items ( Big 5__C_lazy.2005 ) were negatively correlated with the total scale and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\nBut again, doing this 15 times would be quite a pain and would open you up to the possibility of a lot of copy and paste errors.\nSo instead, to do this, I’m going to use a mix of the tidyverse. At first glance, it may seem complex but as you move through other tutorials (particularly the purrr tutorial), it will begin to make much more sense.\n\n# short function to reshape data and run alpha\nalpha_fun <- function(df){\n  df %>% spread(scrap,value) %>% psych::alpha(.)\n}\n\n(alphas <- soep_long %>%\n  filter(type == \"Big 5\") %>% # filter out Big 5\n  select(Procedural__SID, item:value) %>% # get rid of extra columns\n  group_by(item, year) %>% # group by construct and year\n  nest() %>% # nest the data\n  mutate(alpha_res = map(data, alpha_fun), # run alpha\n         alpha = map(alpha_res, ~.$total[2])) %>% # get the alpha value\n  unnest(alpha)) # pull it out of the list column\n\n# A tibble: 15 × 5\n# Groups:   item, year [15]\n   item  year  data                  alpha_res std.alpha\n   <chr> <chr> <list>                <list>        <dbl>\n 1 C     2005  <tibble [31,117 × 3]> <psych>       0.515\n 2 C     2009  <tibble [30,728 × 3]> <psych>       0.494\n 3 C     2013  <tibble [28,496 × 3]> <psych>       0.482\n 4 E     2005  <tibble [31,188 × 3]> <psych>       0.540\n 5 E     2009  <tibble [30,770 × 3]> <psych>       0.530\n 6 E     2013  <tibble [28,532 × 3]> <psych>       0.544\n 7 A     2005  <tibble [31,184 × 3]> <psych>       0.418\n 8 A     2009  <tibble [30,796 × 3]> <psych>       0.410\n 9 A     2013  <tibble [28,529 × 3]> <psych>       0.401\n10 O     2005  <tibble [31,091 × 3]> <psych>       0.527\n11 O     2009  <tibble [30,722 × 3]> <psych>       0.510\n12 O     2013  <tibble [28,451 × 3]> <psych>       0.497\n13 N     2005  <tibble [31,162 × 3]> <psych>       0.473\n14 N     2009  <tibble [30,802 × 3]> <psych>       0.490\n15 N     2013  <tibble [28,536 × 3]> <psych>       0.480"
  },
  {
    "objectID": "workflow.html#zero-order-correlations",
    "href": "workflow.html#zero-order-correlations",
    "title": "Codebooks and Workflow",
    "section": "Zero-Order Correlations",
    "text": "Zero-Order Correlations\nFinally, we often want to look at the zero-order correlation among study variables to make sure they are performing as we think they should.\nTo run the correlations, we will need to have our data in wide format, so we’re going to do a little bit of reshaping before we do.\n\nb5_soep_long %>%\n  unite(tmp, item, year, sep = \"_\") %>%\n  spread(key = tmp, value = value) %>% \n  select(-Procedural__SID) %>%\n  cor(., use = \"pairwise\") %>%\n  round(., 2)\n\n         DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005 E_2009\nDOB     1.00  0.00  -0.08  -0.07  -0.06  -0.13  -0.12  -0.14   0.10   0.12\nSex     0.00  1.00   0.18   0.17   0.18   0.05   0.07   0.09   0.08   0.08\nA_2005 -0.08  0.18   1.00   0.50   0.50   0.32   0.20   0.19   0.10   0.06\nA_2009 -0.07  0.17   0.50   1.00   0.55   0.19   0.28   0.18   0.05   0.08\nA_2013 -0.06  0.18   0.50   0.55   1.00   0.18   0.19   0.29   0.04   0.06\nC_2005 -0.13  0.05   0.32   0.19   0.18   1.00   0.52   0.48   0.19   0.10\nC_2009 -0.12  0.07   0.20   0.28   0.19   0.52   1.00   0.55   0.12   0.16\nC_2013 -0.14  0.09   0.19   0.18   0.29   0.48   0.55   1.00   0.13   0.14\nE_2005  0.10  0.08   0.10   0.05   0.04   0.19   0.12   0.13   1.00   0.61\nE_2009  0.12  0.08   0.06   0.08   0.06   0.10   0.16   0.14   0.61   1.00\nE_2013  0.10  0.11   0.04   0.04   0.07   0.10   0.10   0.18   0.59   0.65\nN_2005  0.06 -0.18   0.10   0.06   0.02   0.09   0.06   0.03   0.18   0.10\nN_2009  0.03 -0.22   0.07   0.09   0.03   0.06   0.08   0.05   0.13   0.16\nN_2013  0.02 -0.21   0.06   0.06   0.10   0.04   0.06   0.08   0.10   0.10\nO_2005  0.11  0.06   0.12   0.09   0.07   0.17   0.12   0.08   0.40   0.29\nO_2009  0.10  0.05   0.05   0.11   0.07   0.06   0.14   0.08   0.26   0.36\nO_2013  0.05  0.07   0.08   0.09   0.13   0.07   0.08   0.15   0.24   0.28\n       E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013\nDOB      0.10   0.06   0.03   0.02   0.11   0.10   0.05\nSex      0.11  -0.18  -0.22  -0.21   0.06   0.05   0.07\nA_2005   0.04   0.10   0.07   0.06   0.12   0.05   0.08\nA_2009   0.04   0.06   0.09   0.06   0.09   0.11   0.09\nA_2013   0.07   0.02   0.03   0.10   0.07   0.07   0.13\nC_2005   0.10   0.09   0.06   0.04   0.17   0.06   0.07\nC_2009   0.10   0.06   0.08   0.06   0.12   0.14   0.08\nC_2013   0.18   0.03   0.05   0.08   0.08   0.08   0.15\nE_2005   0.59   0.18   0.13   0.10   0.40   0.26   0.24\nE_2009   0.65   0.10   0.16   0.10   0.29   0.36   0.28\nE_2013   1.00   0.11   0.13   0.15   0.26   0.28   0.35\nN_2005   0.11   1.00   0.55   0.53   0.09   0.08   0.06\nN_2009   0.13   0.55   1.00   0.60   0.06   0.07   0.07\nN_2013   0.15   0.53   0.60   1.00   0.05   0.05   0.05\nO_2005   0.26   0.09   0.06   0.05   1.00   0.58   0.55\nO_2009   0.28   0.08   0.07   0.05   0.58   1.00   0.61\nO_2013   0.35   0.06   0.07   0.05   0.55   0.61   1.00\n\n\nThis is a lot of values and a little hard to make sense of, so as a bonus, I’m going to give you a little bit of more complex code that makes this more readable (and publishable ).\n\nr <- b5_soep_long %>%\n  unite(tmp, item, year, sep = \"_\") %>%\n  spread(key = tmp, value = value) %>% \n  select(-Procedural__SID, -DOB, -Sex) %>%\n  cor(., use = \"pairwise\") \n\nr[upper.tri(r, diag = T)] <- NA\ndiag(r) <- (alphas %>% arrange(item, year))$std.alpha\n\nr %>% data.frame %>%\n  rownames_to_column(\"V1\") %>%\n  gather(key = V2, value = r, na.rm = T, -V1) %>%\n  separate(V1, c(\"T1\", \"Year1\"), sep = \"_\") %>%\n  separate(V2, c(\"T2\", \"Year2\"), sep = \"_\") %>%\n  mutate_at(vars(Year1), ~factor(., levels = c(2013, 2009, 2005))) %>%\n  ggplot(aes(x = Year2, y = Year1, fill = r)) +\n    geom_raster() + \n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n   name=\"Correlations\") +  \n  geom_text(aes(label = round(r,2))) +\n    facet_grid(T1 ~ T2) +\n    theme_classic()\n\n\n\n\nCorrelations among Personality Indicators. Values on the diagonal represent Chronbach’s alpha for each scale in each year. Within-trait correlations represent test-retest correlations."
  },
  {
    "objectID": "proportions.html#review",
    "href": "proportions.html#review",
    "title": "Visualizing Proportions",
    "section": "Review",
    "text": "Review\nWhat are the core elements of ggplot2 grammar?\nFrom last week: * Mappings: base layer + ggplot() and aes() * Scales: control and modify your mappings + e.g., scale_x_continuous() and scale_fill_manual() * Geoms: plot elements + e.g., geom_point() and geom_line()\n\n\nFacets: panel your plot\n\n\nfacet_wrap() and facet_grid()\n\n\n\n\nThemes: style your figure\n\nBuilt-in: e.g., theme_classic()\n\nManual: theme() (legend, strip, axis, plot, panel)"
  },
  {
    "objectID": "proportions.html#quick-review-1",
    "href": "proportions.html#quick-review-1",
    "title": "Visualizing Proportions",
    "section": "Quick Review",
    "text": "Quick Review\nColorblindness and accessible plots\n\nAdding in a colorblind-friendly palette from Wong (2011)\n\n\n\nCodecbsafe_pal <- tribble(\n  ~name, ~rgb\n  , \"black\", c(0, 0, 0)\n  , \"sky blue\", c(86, 180, 233)\n  , \"bluish green\", c(0, 158, 115)\n  , \"yellow\", c(240, 228, 66)\n  , \"orange\", c(230, 159, 0)\n  , \"blue\", c(0, 114, 178)\n  , \"vermillion\", c(213, 94, 0)\n  , \"reddish purple\", c(204, 121, 167)\n) %>%\n  mutate(hex = map_chr(rgb, function(x) rgb(x[1], x[2], x[3], maxColorValue = 255)))\ncbsafe_pal"
  },
  {
    "objectID": "proportions.html#basic-syntax",
    "href": "proportions.html#basic-syntax",
    "title": "Visualizing Proportions",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nCodegsoep %>%\n  filter(year == 2009 & !is.na(marital)) %>% # random\n  group_by(marital) %>%\n  tally() %>%\n  mutate(marital = factor(\n    marital\n    , 1:4\n    , c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\")\n    )) %>%\n  ggplot(aes(x = \"\", y = n, fill = marital)) + \n    geom_bar(stat = \"identity\", width = 1, color = \"white\") + \n    coord_polar(\"y\", start = 0) + \n    theme_void()"
  },
  {
    "objectID": "proportions.html#improvements-slice-labels-and-colors",
    "href": "proportions.html#improvements-slice-labels-and-colors",
    "title": "Visualizing Proportions",
    "section": "Improvements: Slice Labels and Colors",
    "text": "Improvements: Slice Labels and Colors\n\nCodegsoep %>%\n  filter(year == 2009 & !is.na(marital)) %>% # random\n  group_by(marital) %>%\n  tally() %>%\n  mutate(marital = factor(\n    marital\n    , 1:4\n    , c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\")\n    )) %>%\n  arrange(desc(marital)) %>%\n  mutate(prop = n / sum(n) * 100\n         , ypos = cumsum(prop)- 0.5*prop) %>%\n  ggplot(aes(x = \"\", y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", width = 1, color = \"white\") + \n    geom_text(\n      aes(y = ypos, label = marital)\n      , color = \"white\"\n      , size=4\n      ) +\n    scale_fill_manual(values = cbsafe_pal$hex[c(2, 8, 3, 4)]) + \n    coord_polar(\"y\", start = 0) + \n    theme_void() + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "proportions.html#more-improvements-title-and-story-congruent-colors",
    "href": "proportions.html#more-improvements-title-and-story-congruent-colors",
    "title": "Visualizing Proportions",
    "section": "More Improvements: Title and Story-Congruent Colors",
    "text": "More Improvements: Title and Story-Congruent Colors\n\nCodegsoep %>%\n  filter(year == 2009 & !is.na(marital)) %>% # random\n  group_by(marital) %>%\n  tally() %>%\n  mutate(marital = factor(marital, 1:4, c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\"))) %>%\n  arrange(desc(marital)) %>%\n  mutate(prop = n / sum(n) * 100\n         , ypos = cumsum(prop)- 0.5*prop) %>%\n  ggplot(aes(x = \"\", y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", width = 1, color = \"black\") + \n    geom_label(\n      aes(y = ypos, label = marital)\n      , color = \"white\"\n      , size = 6\n      , fontface = 2) +\n    scale_fill_manual(values = c(rev(brewer.pal(9,\"Greens\")[c(4,6,8)]), \"grey60\")) + \n    coord_polar(\"y\", start = 0) + \n    labs(\n      title = \"In 2009, the majority of GSOEP participants\\nwere or had been married/partnered\"\n    ) + \n    theme_void() + \n    theme(\n      legend.position = \"none\"\n      , plot.title = element_text(face = \"bold.italic\", size = rel(1.4), hjust = .5)\n      )"
  },
  {
    "objectID": "proportions.html#basic-syntax-1",
    "href": "proportions.html#basic-syntax-1",
    "title": "Visualizing Proportions",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nCodegsoep %>%\n  filter(age %in% 18:26 & !is.na(marital)) %>%\n  group_by(age, marital) %>%\n  tally() %>%\n  group_by(age) %>%\n  mutate(\n    marital = factor(\n      marital\n      , 1:4\n      , c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\")\n      )\n    , age = factor(age)\n    , prop = n/sum(n)\n    ) %>%\n  ggplot(aes(x = age, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\") + \n    theme_classic()"
  },
  {
    "objectID": "proportions.html#improvements-color",
    "href": "proportions.html#improvements-color",
    "title": "Visualizing Proportions",
    "section": "Improvements: Color",
    "text": "Improvements: Color\n\nCodegsoep %>%\n  filter(age %in% 18:26 & !is.na(marital)) %>%\n  group_by(age, marital) %>%\n  tally() %>%\n  group_by(age) %>%\n  mutate(marital = factor(marital, seq(4,1,-1), rev(c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\")))\n         , age = factor(age)\n         , prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\") + \n    scale_fill_manual(values = c(\"grey80\",brewer.pal(9,\"Greens\")[c(2,4,6)])) + \n    theme_classic()"
  },
  {
    "objectID": "proportions.html#improvements-label-scales",
    "href": "proportions.html#improvements-label-scales",
    "title": "Visualizing Proportions",
    "section": "Improvements: Label & Scales",
    "text": "Improvements: Label & Scales\n\nCodegsoep %>%\n  filter(age %in% 18:26 & !is.na(marital)) %>%\n  group_by(age, marital) %>%\n  tally() %>%\n  group_by(age) %>%\n  mutate(\n    marital = factor(\n      marital\n      , seq(4,1,-1)\n      , rev(c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\"))\n      )\n    , age = factor(age)\n    , prop = n/sum(n)\n    ) %>%\n  ggplot(aes(x = age, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\") + \n    scale_fill_manual(values = c(\"grey80\",brewer.pal(9,\"Greens\")[c(2,4,6)])) + \n    scale_y_continuous(\n      limits = c(0,1)\n      , breaks = seq(0, 1, .25)\n      , labels = c(\"0%\", \"25%\", \"50%\", \"75%\", \"100%\")\n      ) + \n    labs(\n      x = \"Age\"\n      , y = \"Percent of Sample\"\n      , title = \"Rates of relationships increase in emerging adulthood\"\n      , subtitle = \"But most remain unpartnered by 26\"\n      ) +\n    theme_classic()"
  },
  {
    "objectID": "proportions.html#improvements-legend",
    "href": "proportions.html#improvements-legend",
    "title": "Visualizing Proportions",
    "section": "Improvements: Legend",
    "text": "Improvements: Legend\n\nCodegsoep %>%\n  filter(age %in% 18:26 & !is.na(marital)) %>%\n  group_by(age, marital) %>%\n  tally() %>%\n  group_by(age) %>%\n  mutate(marital = factor(marital, seq(4,1,-1), rev(c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\")))\n         , age = factor(age)\n         , prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\") + \n    scale_fill_manual(values = c(\"grey80\",brewer.pal(9,\"Greens\")[c(2,4,6)])) + \n    scale_y_continuous(\n      limits = c(0,1)\n      , breaks = seq(0, 1, .25)\n      , labels = c(\"0%\", \"25%\", \"50%\", \"75%\", \"100%\")\n      ) + \n    annotate(\"text\", x = \"26\", y = .60, label = \"Never Married\", angle = 90) + \n    annotate(\"text\", x = \"26\", y = .13, label = \"Married\", angle = 90, color = \"white\") + \n    labs(\n      x = \"Age\"\n      , y = \"Percent of Sample\"\n      , title = \"Rates of relationships increase in emerging adulthood\"\n      , subtitle = \"But most remain unpartnered by 26\"\n      , fill = NULL\n      ) +\n    theme_classic() + \n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "proportions.html#improvements-theme-elements-exercise",
    "href": "proportions.html#improvements-theme-elements-exercise",
    "title": "Visualizing Proportions",
    "section": "Improvements: Theme Elements Exercise",
    "text": "Improvements: Theme Elements Exercise\n\nBold axis text and increase size\nBold axis titles and increase size\nBold title and subtitle and center (hint, you will also need to wrap the title text)\n\n(Answers)\n\nCodegsoep %>%\n  filter(age %in% 18:26 & !is.na(marital)) %>%\n  group_by(age, marital) %>%\n  tally() %>%\n  group_by(age) %>%\n  mutate(marital = factor(marital, seq(4,1,-1), rev(c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\")))\n         , age = factor(age)\n         , prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\") + \n    scale_fill_manual(values = c(\"grey80\",brewer.pal(9,\"Greens\")[c(2,4,6)])) + \n    scale_y_continuous(\n      limits = c(0,1)\n      , breaks = seq(0, 1, .25)\n      , labels = c(\"0%\", \"25%\", \"50%\", \"75%\", \"100%\")\n      ) + \n    annotate(\"text\", x = \"26\", y = .60, label = \"Never Married\", angle = 90) + \n    annotate(\"text\", x = \"26\", y = .13, label = \"Married\", angle = 90, color = \"white\") + \n    labs(\n      x = \"Age\"\n      , y = \"Percent of Sample\"\n      , title = \"Rates of relationships increase in\\nemerging adulthood\"\n      , subtitle = \"But most remain unpartnered by 26\"\n      , fill = NULL\n      ) +\n    theme_classic() + \n    theme(\n      legend.position = \"bottom\"\n      , axis.text = element_text(face = \"bold\", size = rel(1.1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.1))\n      , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n      , plot.subtitle = element_text(face = \"italic\", size = rel(1.1), hjust = .5)\n      )"
  },
  {
    "objectID": "proportions.html#basic-syntax-2",
    "href": "proportions.html#basic-syntax-2",
    "title": "Visualizing Proportions",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nCodegsoep %>%\n  filter(year %in% c(2000, 2005, 2010, 2015) & !is.na(marital)) %>% # random\n  group_by(year, marital) %>%\n  tally() %>%\n  mutate(marital = factor(marital, 1:4, c(\"Married\", \"Separated\", \"Widowed\", \"Never Married\"))) %>%\n  group_by(year) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = year, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\", position = \"dodge\") + \n    theme_classic()"
  },
  {
    "objectID": "proportions.html#improvements-order",
    "href": "proportions.html#improvements-order",
    "title": "Visualizing Proportions",
    "section": "Improvements: Order",
    "text": "Improvements: Order\n\nCodegsoep %>%\n  filter(year %in% c(2000, 2005, 2010, 2015) & !is.na(marital)) %>% # random\n  group_by(year, marital) %>%\n  tally() %>%\n  mutate(marital = factor(marital, c(1,4,2,3), c(\"Married\", \"Never Married\", \"Separated\", \"Widowed\"))) %>%\n  group_by(year) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = year, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\", position = \"dodge\") + \n    theme_classic()"
  },
  {
    "objectID": "proportions.html#improvements-labels",
    "href": "proportions.html#improvements-labels",
    "title": "Visualizing Proportions",
    "section": "Improvements: Labels",
    "text": "Improvements: Labels\nWe could label the bars, but let’s label the axes instead\n\nCodegsoep %>%\n  filter(year %in% c(2000, 2005, 2010, 2015) & !is.na(marital)) %>% # random\n  group_by(year, marital) %>%\n  tally() %>%\n  mutate(marital = factor(marital, c(1,4,2,3), c(\"Married\", \"Never Married\", \"Separated\", \"Widowed\"))) %>%\n  group_by(year) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = marital, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\", position = \"dodge\") + \n    scale_y_continuous(\n      limits = c(0,.7), breaks = seq(0,.7, .2), labels = c(\"0%\", \"20%\", \"40%\", \"60%\")\n    ) +\n    facet_grid(~year) + \n    labs(\n      x = NULL\n      , y = \"Percentage of Participants\"\n      , title = \"Marital Status Has Remained Consistent Throughout the 21st Century\"\n      ) + \n    theme_classic() + \n    theme(\n      legend.position = \"none\"\n      , axis.text.x = element_text(angle = 45, hjust = 1)\n      )"
  },
  {
    "objectID": "proportions.html#improvements-theme-elements",
    "href": "proportions.html#improvements-theme-elements",
    "title": "Visualizing Proportions",
    "section": "Improvements: Theme Elements",
    "text": "Improvements: Theme Elements\nLet’s label and improve the theme elements\n\nCodegsoep %>%\n  filter(year %in% c(2000, 2005, 2010, 2015) & !is.na(marital)) %>% # random\n  group_by(year, marital) %>%\n  tally() %>%\n  mutate(marital = factor(marital, c(1,4,2,3), c(\"Married\", \"Never Married\", \"Separated\", \"Widowed\"))) %>%\n  group_by(year) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = marital, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\", position = \"dodge\") + \n    scale_y_continuous(\n      limits = c(0,.7), breaks = seq(0,.7, .2), labels = c(\"0%\", \"20%\", \"40%\", \"60%\")\n    ) +\n    facet_grid(~year) + \n    labs(\n      x = NULL\n      , y = \"Percentage of Participants\"\n      , title = \"Marital Status Has Remained Consistent\\nThroughout the 21st Century\"\n      ) + \n    theme_classic() + \n    theme(\n      legend.position = \"none\"\n      , axis.text = element_text(face = \"bold\", size = rel(1.2))\n      , axis.text.x = element_text(angle = 45, hjust = 1, size = rel(1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.2))\n      , strip.background = element_rect(fill = \"grey90\", color = \"black\")\n      , strip.text = element_text(face = \"bold\", size = rel(1.2))\n      , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n      )"
  },
  {
    "objectID": "proportions.html#improvements-colors-exercise",
    "href": "proportions.html#improvements-colors-exercise",
    "title": "Visualizing Proportions",
    "section": "Improvements: Colors (Exercise)",
    "text": "Improvements: Colors (Exercise)\nExercise: * Improve the colors by making them: + Colorblind-friendly + Match the goal of the plot (see title)\n(Answers)\n\nCodegsoep %>%\n  filter(year %in% c(2000, 2005, 2010, 2015) & !is.na(marital)) %>% # random\n  group_by(year, marital) %>%\n  tally() %>%\n  mutate(marital = factor(marital, c(1,4,2,3), c(\"Married\", \"Never Married\", \"Separated\", \"Widowed\"))) %>%\n  group_by(year) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = marital, y = prop, fill = marital)) + \n    geom_bar(stat = \"identity\", color = \"black\", position = \"dodge\") + \n    scale_y_continuous(\n      limits = c(0,.7), breaks = seq(0,.7, .2), labels = c(\"0%\", \"20%\", \"40%\", \"60%\")\n    ) +\n    scale_fill_manual(values = cbsafe_pal$hex[2:5]) +\n    facet_grid(~year) + \n    labs(\n      x = NULL\n      , y = \"Percentage of Participants\"\n      , title = \"Marital Status Has Remained Consistent\\nThroughout the 21st Century\"\n      ) + \n    theme_classic() + \n    theme(\n      legend.position = \"none\"\n      , axis.text = element_text(face = \"bold\", size = rel(1.2))\n      , axis.text.x = element_text(angle = 45, hjust = 1, size = rel(1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.2))\n      , strip.background = element_rect(fill = \"grey90\", color = \"black\")\n      , strip.text = element_text(face = \"bold\", size = rel(1.2))\n      , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n      )"
  },
  {
    "objectID": "proportions.html#stacked-area-charts",
    "href": "proportions.html#stacked-area-charts",
    "title": "Visualizing Proportions",
    "section": "Stacked Area Charts",
    "text": "Stacked Area Charts\nBut first, remember stacked bar charts? Stacked area charts are sort of an extension of those:\n\nCodegsoep %>%\n  filter(age %in% c(20, 30, 40, 50, 60, 70, 80) & !is.na(SRhealth)) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, seq(5,1,-1), c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\"))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_bar(stat = \"identity\", color = \"black\") + \n    scale_fill_manual(values = cbsafe_pal$hex[2:6]) +\n    theme_classic()\n\n\n\n\nBut without the bars separating them.\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, seq(5,1,-1), c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\"))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_area() + \n    # scale_fill_manual(values = cbsafe_pal$hex[2:6]) +\n    theme_classic()\n\n\n\n\nImprovements: Color\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, seq(5,1,-1), c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\"))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_area(color = \"white\", alpha = .6) + \n    scale_fill_viridis_d() +\n    theme_classic()\n\n\n\n\nImprovements: Color Labels\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_area(color = \"white\", alpha = .6) + \n    annotate(\"text\", x = 85, y = .95, label = \"Bad\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 75, y = .80, label = \"Poor\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 62, y = .55, label = \"Satisfactory\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 43, y = .3, label = \"Good\", color = \"black\", fontface = 2) + \n    annotate(\"text\", x = 30, y = .07, label = \"Very Good\", color = \"black\", fontface = 2) + \n    scale_fill_viridis_d() +\n    theme_classic() + \n    theme(legend.position = \"none\")\n\n\n\n\nImprovements: Theme Elements\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_area(color = \"white\", alpha = .6) + \n    annotate(\"text\", x = 85, y = .95, label = \"Bad\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 75, y = .80, label = \"Poor\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 62, y = .55, label = \"Satisfactory\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 43, y = .3, label = \"Good\", color = \"black\", fontface = 2) + \n    annotate(\"text\", x = 30, y = .07, label = \"Very Good\", color = \"black\", fontface = 2) + \n    scale_fill_viridis_d() +\n    theme_classic() + \n    theme(legend.position = \"none\"\n          , axis.text = element_text(face = \"bold\", size = rel(1.1))\n          , axis.title = element_text(face = \"bold\", size = rel(1.1))\n          , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n    )\n\n\n\n\nImprovements: Labels and Title (Exercise)\nExercise:\n\nAdd plot title\nChange x and y scale labels and titles\n\n(Answers)\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_area(color = \"white\", alpha = .6) + \n    annotate(\"text\", x = 85, y = .95, label = \"Bad\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 75, y = .80, label = \"Poor\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 62, y = .55, label = \"Satisfactory\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 43, y = .3, label = \"Good\", color = \"black\", fontface = 2) + \n    annotate(\"text\", x = 30, y = .07, label = \"Very Good\", color = \"black\", fontface = 2) + \n    scale_fill_viridis_d() +\n    theme_classic() + \n    theme(legend.position = \"none\"\n          , axis.text = element_text(face = \"bold\", size = rel(1.1))\n          , axis.title = element_text(face = \"bold\", size = rel(1.1))\n          , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n    )\n\n\n\n\nImprovements: Labels and Title\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(prop = n/sum(n)) %>%\n  ggplot(aes(x = age, y = prop, fill = SRhealth)) + \n    geom_area(color = \"white\", alpha = .6) + \n    annotate(\"text\", x = 85, y = .95, label = \"Bad\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 75, y = .80, label = \"Poor\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 62, y = .55, label = \"Satisfactory\", color = \"white\", fontface = 2) + \n    annotate(\"text\", x = 43, y = .3, label = \"Good\", color = \"black\", fontface = 2) + \n    annotate(\"text\", x = 30, y = .07, label = \"Very Good\", color = \"black\", fontface = 2) + \n    scale_x_continuous(limits = c(18, 100), breaks = seq(20, 100, 10)) + \n    scale_y_continuous(limits = c(0,1), breaks = seq(0,1, .25), labels = c(\"0%\", \"25%\", \"50%\", \"75%\", \"100%\")) + \n    scale_fill_viridis_d() +\n    labs(\n      x = \"Age (Years)\"\n      , y = \"Percentage of Participants\"\n      , title = \"Levels of Self-Rated Health Decrease Across the Lifespan\"\n    ) + \n    theme_classic() + \n    theme(legend.position = \"none\"\n          , axis.text = element_text(face = \"bold\", size = rel(1.1))\n          , axis.title = element_text(face = \"bold\", size = rel(1.1))\n          , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n    )"
  },
  {
    "objectID": "proportions.html#total-density-plots",
    "href": "proportions.html#total-density-plots",
    "title": "Visualizing Proportions",
    "section": "Total Density Plots",
    "text": "Total Density Plots\n\nLet’s revisit these data but also demonstrating how sample size changes across the lifespan\nTo do this, we need two pieces of information:\n\nsample size in each self-rated health category at each age group\ntotal in each age group\n\n\n\nLet’s start by using stat_smooth() to get a smoothed geom_area() of the total sample size onto the figure\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(total_n = sum(n))  %>%\n  ggplot(aes(x = age, y = n)) + \n    stat_smooth(\n        aes(y = total_n)\n        , geom = 'area'\n        , method = 'loess'\n        , span = 1/3\n        , alpha = .8\n        , fill = \"grey\"\n        ) + \n    facet_grid(~SRhealth) + \n    theme_classic()\n\n\n\n\nThen add the area for each ordinal level of self-rated health.\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(total_n = sum(n))  %>%\n  ggplot(aes(x = age, y = n)) + \n    stat_smooth(aes(y = total_n), geom = 'area', method = 'loess'\n        , span = 1/3, alpha = .8, fill = \"grey\") + \n    stat_smooth(\n        aes(fill = SRhealth)\n        , geom = 'area'\n        , method = 'loess'\n        , span = 1/3\n        , alpha = .8\n        ) + \n    annotate(\"text\", x = 45, y = 3000, label = \"Total\") + \n    facet_grid(~SRhealth) + \n    theme_classic() + \n    theme(legend.position = \"none\")\n\n\n\n\nLet’s not belabor this too much.\n\nCodegsoep %>%\n  filter(!is.na(SRhealth) & age >= 18 & age <= 100) %>% # random\n  group_by(age, SRhealth) %>%\n  tally() %>%\n  mutate(SRhealth = factor(SRhealth, 1:5, rev(c(\"Very good\", \"Good\", \"Satisfactory\", \"Poor\", \"Bad\")))) %>%\n  group_by(age) %>%\n  mutate(total_n = sum(n))  %>%\n  ggplot(aes(x = age, y = n)) + \n    stat_smooth(aes(y = total_n), geom = 'area', method = 'loess'\n        , span = 1/3, alpha = .8, fill = \"grey\") + \n    stat_smooth(\n        aes(fill = SRhealth)\n        , geom = 'area'\n        , method = 'loess'\n        , span = 1/3\n        , alpha = .8\n        ) + \n    scale_x_continuous(limits = c(18, 100), breaks = seq(20, 100, 10)) + \n    scale_fill_viridis_d() +\n    annotate(\"text\", x = 45, y = 3000, label = \"Total\") + \n    labs(\n      x = \"Age (Years)\"\n      , y = \"Number of People\"\n      , title = \"Good Self-Rated Health Decreases Across the Lifespan\"\n      , subtitle = \"But bad decreases less, likely because all-cause sample drop-out\"\n      ) + \n    facet_grid(~SRhealth) + \n    theme_classic() + \n    theme(legend.position = \"none\"\n          , axis.text = element_text(face = \"bold\", size = rel(1.1))\n          , axis.title = element_text(face = \"bold\", size = rel(1.1))\n          , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n          , plot.subtitle = element_text(face = \"italic\", size = rel(1), hjust = .5)\n          , strip.background = element_rect(fill = \"grey90\", color = \"black\")\n          , strip.text = element_text(face = \"bold\", size = rel(1.2))\n          )"
  },
  {
    "objectID": "proportions.html#mosaic-plots",
    "href": "proportions.html#mosaic-plots",
    "title": "Visualizing Proportions",
    "section": "Mosaic Plots",
    "text": "Mosaic Plots\n\nUnlike bar charts, mosaic plots allow us to index relative areas, sizes, proportions, etc. relative to two dimensions (so not just amount)\nSo in our example, this will let us see relative differences within categories vertically and across categories horizontally\nTo build this, we will finally leave the basic ggplot2 package and use the ggmosaic package\nThere are other packages, but we’ll use this one because (1) it’s great and (2) it let’s us still use everything we’ve learned about ggplot\n\nBut first, the data:\n\nCodeif(!\"ggmosaic\" %in% installed.packages()) install.packages(\"ggmosaic\")\nlibrary(ggmosaic)\n\ngsoep_jobs <- gsoep %>%\n  mutate(age_gr = mapvalues(age, 20:99, rep(seq(20, 90, 10), each = 10))) %>%\n  filter(!is.na(age_gr) & age >= 20 & age < 100) %>%\n  group_by(SID) %>%\n  filter(!is.na(job)) %>%\n  filter(age_gr == max(age_gr)) %>%\n  group_by(SID, age_gr) %>%\n  summarize(job = max(job)) %>%\n  ungroup() %>%\n  rename(code = job) %>%\n  left_join(jobs %>% rename(code = old)) %>%\n  group_by(code) %>%\n  filter(n() / nrow(.) >= .02) %>%\n  ungroup() \ngsoep_jobs\n\n\n\n  \n\n\n\n\nLet’s say, for example, that we think that some professions may restrict certain age groups due to experience (younger age groups) or functional limitations (older age groups)\nWe could look at this simply as a stacked bar chart, but it wouldn’t clarify that there are different proportions of people in each job category\n\n\nCodegsoep_jobs %>%\n  ggplot() + \n    geom_mosaic(aes(x = product(age_gr), fill = cat)) + \n    theme_classic() + \n    theme(legend.position = \"none\")\n\n\n\n\nImprovements: Let’s polish it\n\nCodegsoep_jobs %>%\n  ggplot() + \n    geom_mosaic(aes(x = product(age_gr), fill = cat)) + \n    labs(\n      x = \"Age Group (Decades)\"\n      , title = \"There are small changes in category of professions across the lifespan\"\n      , subtitle = \"Younger adults are more likely to be be in service and sales positions\"\n      ) +\n    theme_classic() + \n    theme(\n      legend.position = \"none\"\n      , axis.text = element_text(face = \"bold\", size = rel(1.1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.2))\n      , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n      , plot.subtitle = element_text(face = \"italic\", size = rel(1.1), hjust = .5)\n      )"
  },
  {
    "objectID": "proportions.html#treemap",
    "href": "proportions.html#treemap",
    "title": "Visualizing Proportions",
    "section": "Treemap",
    "text": "Treemap\n\nMosaic plots are sort of just fancy stacked bar plots that let you also index by size\nTreemaps are helpful when we have nested categorical (and sometimes, to a lesser degree continuous) variables\nWe’ll use the example of our jobs data, but this could be used for lots of other types of variables\n\nCrossed conditions in an experiment\nIntergenerational data\nAverage scores on variables within categories\nBrain activation across broader and narrower brain regions\nPolitical affiliation across states, demographic groups, and more\n\n\n\n\nCodegsoep_tm <- gsoep %>%\n  group_by(SID) %>%\n  filter(!is.na(job)) %>%\n  group_by(SID) %>%\n  summarize(job = max(job)) %>%\n  ungroup() %>%\n  rename(code = job) %>%\n  left_join(jobs %>% rename(code = old)) %>%\n  group_by(code, cat, job) %>%\n  tally()  %>%\n  ungroup() %>%\n  filter(n/sum(n) > .02) %>%\n  mutate(job = str_wrap(job, 15))\n\n\nBasic Syntax\n\nCodeif(!\"treemapify\" %in% installed.packages()) install.packages(\"treemapify\")\nlibrary(treemapify)\ngsoep_tm %>%\n  arrange(cat, code) %>%\n  ggplot(aes(area = n, fill = cat, label = job, subgroup = cat)) +\n  geom_treemap(color = \"grey\", size = 3) \n\n\n\n\nImprovements: Remove Legend and Add Labels\n\nCodegsoep_tm %>%\n  arrange(cat, code) %>%\n  ggplot(aes(area = n, fill = cat, label = job, subgroup = cat)) +\n  geom_treemap() +\n  geom_treemap_text(\n    colour = \"white\"\n    , place = \"centre\"\n    , size = 15\n    , grow = FALSE\n    ) +\n  theme(legend.position = \"none\")\n\n\n\n\nImprovements: Add Subgroup Text\n\nCodegsoep_tm %>%\n  arrange(cat, code) %>%\n  ggplot(aes(area = n, fill = cat, label = job, subgroup = cat)) +\n  geom_treemap() +\n  geom_treemap_text(\n    colour = \"white\"\n    , place = \"centre\"\n    , size = 15\n    , grow = FALSE\n    ) +\n  geom_treemap_subgroup_text(\n    place = \"bottom\"\n    , grow = TRUE\n    , alpha = 0.4\n    , colour = \"white\"\n    , fontface = \"italic\"\n    ) +\n  scale_fill_viridis_d()  +\n  theme(legend.position = \"none\")\n\n\n\n\nImprovements: Color Palette\n\nCodegsoep_tm %>%\n  arrange(cat, code) %>%\n  ggplot(aes(area = n, fill = cat, label = job, subgroup = cat)) +\n  geom_treemap() +\n  geom_treemap_text(\n    colour = \"white\"\n    , place = \"centre\"\n    , size = 15\n    , grow = FALSE\n    ) +\n  geom_treemap_subgroup_text(\n    place = \"bottom\"\n    , grow = TRUE\n    , alpha = 0.4\n    , colour = \"white\"\n    , fontface = \"italic\"\n    ) +\n  scale_fill_viridis_d()  +\n  theme(legend.position = \"none\")\n\n\n\n\nImprovements: Group and Subgroup Borders + Text Color\n\nCodegsoep_tm %>%\n  arrange(cat, code) %>%\n  ggplot(aes(area = n, fill = cat, label = job, subgroup = cat)) +\n  geom_treemap(color = \"grey\", size = 3) +\n  geom_treemap_text(\n    colour = c(rep(\"white\", 11), rep(\"black\",4))\n    , place = \"centre\"\n    , size = 15\n    , grow = FALSE\n    ) +\n  geom_treemap_subgroup_text(\n    place = \"bottom\"\n    , grow = TRUE\n    , alpha = 0.4\n    , colour = c(rep(\"white\", 11), rep(\"black\",4))\n    , fontface = \"italic\"\n    ) +\n  geom_treemap_subgroup_border(\n    colour = \"white\"\n    , size = 5\n    ) +\n  scale_fill_viridis_d()  +\n  theme(legend.position = \"none\")\n\n\n\n\nImprovements: Title\n\nCodegsoep_tm %>%\n  arrange(cat, code) %>%\n  ggplot(aes(area = n, fill = cat, label = job, subgroup = cat)) +\n  geom_treemap(color = \"grey\", size = 3) +\n  geom_treemap_text(\n    colour = c(rep(\"white\", 11), rep(\"black\",4))\n    , place = \"centre\"\n    , size = 15\n    , grow = FALSE\n    ) +\n  geom_treemap_subgroup_text(\n    place = \"bottom\"\n    , grow = TRUE\n    , alpha = 0.4\n    , colour = c(rep(\"white\", 11), rep(\"black\",4))\n    , fontface = \"italic\"\n    ) +\n  geom_treemap_subgroup_border(\n    colour = \"white\"\n    , size = 5\n    ) +\n  scale_fill_viridis_d()  +\n  labs(title = \"White Collar Public Service, Sales, and\\nFinance Jobs Far Outnumber Blue Collar Jobs\") + \n  theme(legend.position = \"none\"\n        , plot.title = element_text(face = \"bold\", hjust = .5))"
  },
  {
    "objectID": "tables.html#packages",
    "href": "tables.html#packages",
    "title": "APA Tables Part 1",
    "section": "Packages",
    "text": "Packages\n\nCodelibrary(knitr)\nlibrary(lme4)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(broom)\nlibrary(plyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "tables.html#data",
    "href": "tables.html#data",
    "title": "APA Tables Part 1",
    "section": "Data",
    "text": "Data\nThe data we’re going to use are from the teaching sample from the German Socioeconomic Panel Study. These data have been pre-cleaned (see earlier workshop on workflow and creating guidelines for tips).\nThe data we’ll use fall into three categories:\n1. Personality trait composites: Negative Affect, Positive Affect, Self-Esteem, CESD Depression, and Optimism. These were cleaned, reversed coded, and composited prior to being included in this final data set.\n2. Outcomes: Moving in with a partner, marriage, divorce, and child birth. These were cleaned, coded as 1 (occurred) or 0 (did not occur) according to whether an outcome occurred for each individual or not after each possible measured personality year. Moreover, people who experienced these outcomes prior to the target personality year are excluded.\n3. Covariates: Age, gender (0 = male, 1 = female, education (0 = high school or below, 1 = college, 2 = higher than college), gross wages, self-rated health, smoking (0 = never smoked 1 = ever smoked), exercise, BMI, religion, parental education, and parental occupational prestige (ISEI). Each of these were composited for all available data up to the measured personality years.\nFirst, the code below will download a .zip file when you run it. Once, you do, navigate to your Desktop to unzip the folder. You should now be able to run the rest of the code.\n\nCodedata_source <- \"https://github.com/emoriebeck/R-tutorials/raw/master/09_tables-p1/09_tables-p1.zip\"\ndata_dest <- \"~/Desktop/09_tables-p1.zip\"\ndownload.file(data_source, data_dest)\nunzip(data_dest, exdir = \"~/Desktop\")\n\n\n\nCodewd <- \"~/Desktop/tables\"\n(gsoep <- sprintf(\"%s/data/gsoep.csv\", wd) %>% read_csv())"
  },
  {
    "objectID": "tables.html#important-tools",
    "href": "tables.html#important-tools",
    "title": "APA Tables Part 1",
    "section": "Important Tools",
    "text": "Important Tools\nAlthough it doesn’t cover all models, the broom and broom.mixed family of packages will provide easy to work with estimates of nearly all types of models and will also provide the model terms that are ideal for most APA tables, including estimates, standard errors, and confidence intervals.\nlavaan models are slightly more complicated, but it’s actually relatively easy to deal with them (and how to extract their terms), assuming that you understand the models you are running."
  },
  {
    "objectID": "tables.html#set-up-the-data-frame",
    "href": "tables.html#set-up-the-data-frame",
    "title": "APA Tables Part 1",
    "section": "Set up the data frame",
    "text": "Set up the data frame\nFirst, we’ll use some of what we learned in the purrr workshop to set ourselves up to be able to create these tables easily, using group_by() and nest() to create nested data frames for our target personality + outcome combinations. To do this, we’ll also use what you learned about filter() and mutate().\n\nCodegsoep_nested1 <- gsoep %>%\n  filter(Outcome == \"chldbrth\") %>%\n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup()\ngsoep_nested1\n\n\n\n  \n\n\n\nFirst, let’s pause and see what we have. We now have a data frame with 3 columns (Outcome, Trait, and data) and 4 rows. The data column is of class list, meaning it’s a “list column” that contains a tibble in each cell. This means that we can use purrr functions to run operations on each of these data frames individually but without having to copy and paste the same operation multiple times for each model we want to run."
  },
  {
    "objectID": "tables.html#run-models",
    "href": "tables.html#run-models",
    "title": "APA Tables Part 1",
    "section": "Run Models",
    "text": "Run Models\nTo run the models, I like to write short functions that are easier to read than including a local function within a call to purrr::map(). Here, we’re just going to write a simple function to predict child birth from personality.\n\nCodemod1_fun <- function(d){\n  d$o_value <- factor(d$o_value)\n  glm(o_value ~ p_value, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested1 <- gsoep_nested1 %>%\n  mutate(m = map(data, mod1_fun))\ngsoep_nested1\n\n\n\n  \n\n\n\nNow, when we look at the nested frame, we see an additional column, which is also a list, but this column contains <glm> objects rather than tibbles."
  },
  {
    "objectID": "tables.html#get-key-terms",
    "href": "tables.html#get-key-terms",
    "title": "APA Tables Part 1",
    "section": "Get Key Terms",
    "text": "Get Key Terms\nNow that we have the models, we want to get our key terms. I’m a big fan of using the function tidy from the broom package to do this. Bonus because it plays nicely with purrr. Double bonus because it will give us confidence intervals, which I generally prefer over p-values and standard erorrs because I find them more informative.\n\nCodegsoep_nested1 <- gsoep_nested1 %>%\n  mutate(tidy = map(m, ~tidy(., conf.int = T)))\ngsoep_nested1\n\n\n\n  \n\n\n\nNote that what I’ve used here is a local function, meaning that I’ve used the notation ~function(., arguments). The tilda tells R we want a local function, and the . tells R to use the mapped m column as the function input.\nNow we have a fifth column, which is a list column called tidy that contains a tibble, just like the data column."
  },
  {
    "objectID": "tables.html#creating-a-table",
    "href": "tables.html#creating-a-table",
    "title": "APA Tables Part 1",
    "section": "Creating a Table",
    "text": "Creating a Table\nNow we are finally ready to create a table! I’m going to use kable + kableExtra to do this in steps.\nFirst, we’ll unnest the tidy column from our data frame. Before doing so, we will drop the data and m columns because they’ve done their work for now.\n\nCodetidy1 <- gsoep_nested1 %>%\n  select(-data, -m) %>%\n  unnest(tidy)\ntidy1\n\n\n\n  \n\n\n\nAs you can see, we now have lots of information about our model terms, which are already nicely indexed by Outcome and Trait combinations.\nBut before we’re ready to create a table, we have to make a few considerations: - What is our target term? In this case “p_value” which is the change in log odds associated with a 1 unit increase/decrease in p_value.\n\nCodetidy1 <- tidy1 %>% filter(term == \"p_value\")\ntidy1\n\n\n\n  \n\n\n\n\nHow will we denote significance? In this case, we’ll use confidence intervals whose signs match. We’ll then bold these terms for our table.\n\n\nCodetidy1 <- tidy1 %>% mutate(sig = ifelse(sign(conf.low) == sign(conf.high), \"sig\", \"ns\"))\ntidy1\n\n\n\n  \n\n\n\n\nWhat is the desired final structure for the table? I’d like columns for Trait, estimate (b), and confidence intervals (CI) formatted to two decimal places and bolded if significant. I’d also like a span header denoting that the outcome measure is child birth.\n\nBefore we do this, though, we need to convert our log odds to odds ratios, using the exp() function.\n\nCodetidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) \ntidy1\n\n\n\n  \n\n\n\nNow, we can format them.\n\nCodetidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, conf.low, conf.high), ~sprintf(\"%.2f\", .)) \ntidy1\n\n\n\n  \n\n\n\nsprintf() is my favorite base R formatting function. “%.2f” means I’m asking it to take a floating point number and include 2 digits after the “.” and 0 before. We can now see that the estimate, conf.low, and conf.high columns are of class <chr> instead of <dbl>.\nBut now we need to create our confidence intervals.\n\nCodetidy1 <- tidy1 %>%\n  mutate(CI = sprintf(\"[%s, %s]\", conf.low, conf.high))\ntidy1\n\n\n\n  \n\n\n\nAnd bold the significant confidence intervals and estimates.\n\nCodetidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, CI), ~ifelse(sig == \"sig\", sprintf(\"<strong>%s</strong>\", .), .))\ntidy1\n\n\n\n  \n\n\n\nThis reads as “for both the estimate and the CI columns, if the sig column is equal to”sig”, then let’s format it as bold using html. Otherwise, let’s leave it alone.” And indeed, we can see that the final result formats 3/4 rows.\nThankfully, these can be achieved without considerable reshaping of the data, which is why we’ve started here, so we’re almost done. We just need to get rid of some unnecessary columnns.\n\nCodetidy1 <- tidy1 %>%\n  select(Trait, OR = estimate, CI)\n\n\nBecause we just have one target term and one outcome, we don’t need those columns, so we’re just keeping Trait, OR, which I renamed as such within in the select call, and CI.\nNow let’s kable. You’ve likely used the kable() function from the knitr before. It’s a very useful and simple function in most occasions.\n\nCodekable(tidy1)\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n OP \n    <strong>1.68</strong> \n    <strong>[1.50, 1.88]</strong> \n  \n\n DEP \n    <strong>1.15</strong> \n    <strong>[1.07, 1.24]</strong> \n  \n\n NegAff \n    <strong>1.11</strong> \n    <strong>[1.01, 1.22]</strong> \n  \n\n PA \n    <strong>1.97</strong> \n    <strong>[1.77, 2.20]</strong> \n  \n\n SE \n    1.09 \n    [0.97, 1.22] \n  \n\n\n\n\nIt will automatically generate the html code needed to create a table. But if we look closely at the code, it gives us some gobbledigook where we inputted html, so we need a way around that. I’m also going to throw in kable_styling(full_width = F) from the kableExtra package to help out here. It’s not doing much, but it will make the formatted table print in your Viewer.\n\nCodekable(tidy1, escape = F) %>%\n  kable_styling(full_width = F)\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n OP \n    1.68 \n    [1.50, 1.88] \n  \n\n DEP \n    1.15 \n    [1.07, 1.24] \n  \n\n NegAff \n    1.11 \n    [1.01, 1.22] \n  \n\n PA \n    1.97 \n    [1.77, 2.20] \n  \n\n SE \n    1.09 \n    [0.97, 1.22] \n  \n\n\n\n\nMuch better. But this still doesn’t look like an APA table, so let’s keep going.\n\nAPA tables usually write out long names for our predictors, so let’s change those first. I’m going to create a reference tibble and use mapvalues() from the plyr function for this.\n\n\nCodep_names <- tibble(\n  old = c(\"NegAff\", \"PA\", \"SE\", \"OP\", \"DEP\"),\n  new = c(\"Negative Affect\", \"Positive Affect\", \"Self-Esteem\", \"Optimism\", \"Depression\")\n)\n\ntidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F) %>%\n  kable_styling(full_width = F)\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n\nThe combinatin of factor plus arrange here is super helpful for ordering your table.\n\nThe alignment of the columns isn’t quite right. Let’s fix that. We’ll change the trait to right justified and b and CI to centered.\n\n\nCodetidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F,\n        align = c(\"r\", \"c\", \"c\")) %>%\n  kable_styling(full_width = F)\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n\n\nBut we’re still missing our span header. There’s a great function in the kableExtra package for this add_header_above. This function takes a named vector as argument, where the elements of the vector refer to the number of columns the named element should span.\n\n\nCodetidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F,\n        align = c(\"r\", \"c\", \"c\")) %>%\n  kable_styling(full_width = F) %>%\n  add_header_above(c(\" \" = 1, \"Birth of a Child\" = 2))\n\n\n\n\n\n\nBirth of a Child\n\n\n Trait \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n\nNote that what the \" \" = 1 does is skip the Trait column. This is very useful because it let’s us not have a span header over every column.\n\nAPA style requires we note how we denote significance and have a title, so let’s add a title and a note.\n\n\nCodetidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F,\n        align = c(\"r\", \"c\", \"c\"),\n        caption = \"<strong>Table 1</strong><br><em>Estimated Personality-Outcome Associations</em>\") %>%\n  kable_styling(full_width = F) %>%\n  add_header_above(c(\" \" = 1, \"Birth of a Child\" = 2)) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 1Estimated Personality-Outcome Associations\n\n \n\n\nBirth of a Child\n\n\n Trait \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0\n\n\n\n\nWe did it!"
  },
  {
    "objectID": "tables.html#set-up-data",
    "href": "tables.html#set-up-data",
    "title": "APA Tables Part 1",
    "section": "Set Up Data",
    "text": "Set Up Data\n\nCodegsoep_nested2 <- gsoep %>%\n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup()\ngsoep_nested2"
  },
  {
    "objectID": "tables.html#run-the-models",
    "href": "tables.html#run-the-models",
    "title": "APA Tables Part 1",
    "section": "Run the models",
    "text": "Run the models\n\nCodemod1_fun <- function(d){\n  d$o_value <- factor(d$o_value)\n  glm(o_value ~ p_value, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested2 <- gsoep_nested2 %>%\n  mutate(m = map(data, mod1_fun),\n         tidy = map(m, ~tidy(., conf.int = T)))\ngsoep_nested2"
  },
  {
    "objectID": "tables.html#create-the-table",
    "href": "tables.html#create-the-table",
    "title": "APA Tables Part 1",
    "section": "Create the Table",
    "text": "Create the Table\n\nCodetidy2 <- gsoep_nested2 %>%\n  select(Outcome, Trait, tidy) %>%\n  unnest(tidy)\ntidy2\n\n\n\n  \n\n\n\nThe basic steps from here are similar: filter target terms, index significance, exponentiate, format values, create CI’s, bold significance, select needed columns.\n\nCodetidy2 <- tidy2 %>%\n  filter(term == \"p_value\") %>%\n  mutate(sig = ifelse(sign(conf.low) == sign(conf.high), \"sig\", \"ns\")) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), ~sprintf(\"%.2f\", .)) %>%\n  mutate(CI = sprintf(\"[%s, %s]\", conf.low, conf.high)) %>%\n  mutate_at(vars(estimate, CI), ~ifelse(sig == \"sig\", sprintf(\"<strong>%s</strong>\", .), .)) %>%\n  select(Outcome, Trait, OR = estimate, CI)\ntidy2\n\n\n\n  \n\n\n\nGreat, we’re all set right? Not quite. If we want to do a span header, we need our data in shape for that. But right now, our outcomes are rows, not columns. To get them as columns, we will need to: (1) pivot_longer() the OR’s and CI’s, (2) unite() the outcomes and type of estimate, (3) pivot_wider() these united terms, (4) reorder these columns as we want.\nLet’s do each in turn:\n(1) Long format\n\nCodetidy2 <- tidy2 %>%\n  pivot_longer(cols = c(OR, CI), names_to = \"est\", values_to = \"value\")\ntidy2\n\n\n\n  \n\n\n\n\nUnite!\n\n\nCodetidy2 <- tidy2 %>%\n  unite(tmp, Outcome, est, sep = \"_\")\ntidy2\n\n\n\n  \n\n\n\n\nPivot wider\n\n\nCodetidy2 <- tidy2 %>%\n  pivot_wider(names_from = \"tmp\", values_from = \"value\")\n\n\n\nCreate the order of columns\n\n\nCodeO_names <- tibble(\n  old = c(\"mvInPrtnr\", \"married\", \"divorced\", \"chldbrth\"),\n  new = c(\"Move in with Partner\", \"Married\", \"Divorced\", \"Birth of a Child\")\n)\n\nlevs <- paste(rep(O_names$old, each = 2), rep(c(\"OR\",\"CI\"), times = 4), sep = \"_\")\ntidy2 <- tidy2 %>%\n  select(Trait, all_of(levs))\n\n\nNow we’re ready to kable()! This will proceed almost exactly as before. The only difference from the previous example is that we have multiple different columns we want to span. Thankfully, we know what these are because we carefully ordered them when we factored them.\nFor our named vector, we’ll take advantage of our O_names object to create the vector in advance:\n\nCodeheads <- c(1, rep(2, 4))\nheads\n\n[1] 1 2 2 2 2\n\nCodenames(heads) <- c(\" \", O_names$new)\nheads\n\n                     Move in with Partner              Married \n                   1                    2                    2 \n            Divorced     Birth of a Child \n                   2                    2 \n\n\n\nCodetidy2 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F,\n        align = c(\"r\", rep(\"c\", 8)),\n        caption = \"<strong>Table 2</strong><br><em>Estimated Personality-Outcome Associations</em>\") %>%\n  kable_styling(full_width = F) %>%\n  add_header_above(heads) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 2Estimated Personality-Outcome Associations\n\n \n\n\nMove in with Partner\nMarried\nDivorced\nBirth of a Child\n\n\n Trait \n    mvInPrtnr_OR \n    mvInPrtnr_CI \n    married_OR \n    married_CI \n    divorced_OR \n    divorced_CI \n    chldbrth_OR \n    chldbrth_CI \n  \n\n\n\n Negative Affect \n    1.33 \n    [1.21, 1.47] \n    1.29 \n    [1.19, 1.40] \n    1.58 \n    [1.38, 1.80] \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.36 \n    [1.21, 1.52] \n    1.63 \n    [1.49, 1.78] \n    0.85 \n    [0.74, 0.97] \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.00 \n    [0.88, 1.13] \n    0.86 \n    [0.79, 0.94] \n    0.95 \n    [0.81, 1.12] \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.58 \n    [1.40, 1.79] \n    1.32 \n    [1.19, 1.46] \n    1.10 \n    [0.93, 1.30] \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    0.91 \n    [0.85, 0.98] \n    0.99 \n    [0.93, 1.06] \n    0.74 \n    [0.67, 0.82] \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0\n\n\n\n\nEw, but those column names are terrible. Let’s fix them using the col.names argument in kable():\n\nCodetidy2 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F,\n        align = c(\"r\", rep(\"c\", 8)),\n        col.names = c(\"Trait\", rep(c(\"OR\", \"CI\"), times = 4)),\n        caption = \"<strong>Table 2</strong><br><em>Estimated Personality-Outcome Associations</em>\") %>%\n  kable_styling(full_width = F) %>%\n  add_header_above(heads) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 2Estimated Personality-Outcome Associations\n\n \n\n\nMove in with Partner\nMarried\nDivorced\nBirth of a Child\n\n\n Trait \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    1.33 \n    [1.21, 1.47] \n    1.29 \n    [1.19, 1.40] \n    1.58 \n    [1.38, 1.80] \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.36 \n    [1.21, 1.52] \n    1.63 \n    [1.49, 1.78] \n    0.85 \n    [0.74, 0.97] \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.00 \n    [0.88, 1.13] \n    0.86 \n    [0.79, 0.94] \n    0.95 \n    [0.81, 1.12] \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.58 \n    [1.40, 1.79] \n    1.32 \n    [1.19, 1.46] \n    1.10 \n    [0.93, 1.30] \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    0.91 \n    [0.85, 0.98] \n    0.99 \n    [0.93, 1.06] \n    0.74 \n    [0.67, 0.82] \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0"
  },
  {
    "objectID": "tables.html#covariates",
    "href": "tables.html#covariates",
    "title": "APA Tables Part 1",
    "section": "Covariates",
    "text": "Covariates\nSet up Data\nThis time, we need to add a new grouping variable – namely, the target covariates.\nOnly trick is we also need the combination with no covariate. There’s lots of ways to add this on. I’ll show you one way.\n\nCodegsoep_nested3 <- gsoep_long %>%\n  full_join(gsoep %>% select(SID, Outcome, o_value, Trait, p_value) %>%\n              mutate(Covariate = \"none\")) %>% \n  group_by(Trait, Outcome, Covariate) %>%\n  nest() %>%\n  ungroup() %>%\n  arrange(Outcome, Trait, Covariate)\ngsoep_nested3\n\n\n\n  \n\n\n\nRun the models\n\nCodefactor_fun <- function(x){if(is.numeric(x)){diff(range(x, na.rm = T)) %in% 1:2 & length(unique(x)) <= 4} else{F}}\n\nmod3_fun <- function(d, cov){\n  d$o_value <- factor(d$o_value)\n  d <- d %>% mutate_if(factor_fun, factor)\n  if(cov == \"none\"){f <- formula(o_value ~ p_value)} else{f <- formula(o_value ~ p_value + c_value)}\n  glm(f, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested3 <- gsoep_nested3 %>%\n  mutate(m = map2(data, Covariate, mod3_fun),\n         tidy = map(m, ~tidy(., conf.int = T)))\ngsoep_nested3\n\n\n\n  \n\n\n\nLooking specifically at the tidy column, notice that there are different numbers of rows. This is good! We should see 3 rows for age, gender, and SRhealth because we have one new term – a continuous covariate of two level binary covariate. We should see 4 rows for parEdu because we have two new terms – for a three level categorical covariate. Finally, when there is no covariate, we should just have 2 rows like in the previous example.\nCreate the Table\n\nCodetidy3 <- gsoep_nested3 %>%\n  select(Outcome, Trait, Covariate, tidy) %>%\n  unnest(tidy)\ntidy3\n\n\n\n  \n\n\n\nThe basic steps from here are similar: filter target terms, index significance, exponentiate, format values, create CI’s, bold significance, select needed columns.\n\nCodetidy3 <- tidy3 %>%\n  filter(term == \"p_value\") %>%\n  mutate(sig = ifelse(sign(conf.low) == sign(conf.high), \"sig\", \"ns\")) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), ~sprintf(\"%.2f\", .)) %>%\n  mutate(CI = sprintf(\"[%s, %s]\", conf.low, conf.high)) %>%\n  mutate_at(vars(estimate, CI), ~ifelse(sig == \"sig\", sprintf(\"<strong>%s</strong>\", .), .)) %>%\n  select(Outcome, Trait, Covariate, OR = estimate, CI)\n\n\nNow we’re ready for the pivot_longer(), unite(), pivot_wider(), order columns (using select()) combo from before. The only difference is that I’m going to use pivot_wider() to do the uniting for me this time!\n\nCodeO_names <- tibble(\n  old = c(\"mvInPrtnr\", \"married\", \"divorced\", \"chldbrth\"),\n  new = c(\"Move in with Partner\", \"Married\", \"Divorced\", \"Birth of a Child\")\n)\nlevs <- paste(rep(O_names$old, each = 2), rep(c(\"OR\",\"CI\"), times = 4), sep = \"_\")\n\ntidy3 <- tidy3 %>%\n  pivot_longer(cols = c(OR, CI), names_to = \"est\", values_to = \"value\") %>%\n  pivot_wider(names_from = c(\"Outcome\", \"est\"), values_from = \"value\", names_sep = \"_\") %>%\n  select(Trait, Covariate, all_of(levs))\ntidy3\n\n\n\n  \n\n\n\nAll right, time to use kable()! This will proceed almost exactly as before. The only difference from the previous example is that we have multiple different models with different combinations of p_value terms depending on what we controlled for. So we’ll introduce a new function collapse_rows() from kableExtra.\nGiven that we have multiple covariates, we’ll also want to order them. So we’ll make the Covariate column a factor as well. But we’ll do so after we’ve given our covariates nicer names. While we’re at it, we’ll go ahead and do the same things for our Trait column.\n\nCodec_names <- tibble(\n  old = c(\"none\", \"age\", \"SRhealth\", \"gender\", \"parEdu\"),\n  new = c(\"None\", \"Age\", \"Self-Rated Health\", \"Gender\", \"Parental Education\")\n)\n\ntidy3 <- tidy3 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new),\n         Covariate = mapvalues(Covariate, from = c_names$old, to = c_names$new),\n         Covariate = factor(Covariate, levels = c_names$new)) %>%\n  arrange(Trait, Covariate)\ntidy3\n\n\n\n  \n\n\n\nAnd again, for our spanned columns, we’ll take advantage of our O_names object to create the vector in advance:\n\nCodeheads <- rep(2, 5)\nheads\n\n[1] 2 2 2 2 2\n\nCodenames(heads) <- c(\" \", O_names$new)\nheads\n\n                     Move in with Partner              Married \n                   2                    2                    2 \n            Divorced     Birth of a Child \n                   2                    2 \n\n\nNow, this will proceed as before, with just a few small tweaks to the align and col.names arguments to account for the additional Covariate column.\n\nCodetidy3 %>%\n  kable(., escape = F,\n        align = c(\"r\", \"r\", rep(\"c\", 8)),\n        col.names = c(\"Trait\", \"Covariate\", rep(c(\"OR\", \"CI\"), times = 4)),\n        caption = \"<strong>Table 3</strong><br><em>Estimated Personality-Outcome Associations</em>\") %>%\n  kable_styling(full_width = F) %>%\n  add_header_above(heads) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 3Estimated Personality-Outcome Associations\n\n \n\n\nMove in with Partner\nMarried\nDivorced\nBirth of a Child\n\n\n Trait \n    Covariate \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    None \n    1.33 \n    [1.21, 1.47] \n    1.29 \n    [1.19, 1.40] \n    1.58 \n    [1.38, 1.80] \n    1.11 \n    [1.01, 1.22] \n  \n\n Negative Affect \n    Age \n    1.20 \n    [1.08, 1.33] \n    1.17 \n    [1.08, 1.27] \n    1.51 \n    [1.32, 1.73] \n    0.99 \n    [0.90, 1.09] \n  \n\n Negative Affect \n    Self-Rated Health \n    1.58 \n    [1.42, 1.75] \n    1.53 \n    [1.40, 1.66] \n    1.70 \n    [1.48, 1.95] \n    1.39 \n    [1.26, 1.53] \n  \n\n Negative Affect \n    Gender \n    1.34 \n    [1.21, 1.48] \n    1.30 \n    [1.20, 1.41] \n    1.59 \n    [1.39, 1.82] \n    1.11 \n    [1.01, 1.22] \n  \n\n Negative Affect \n    Parental Education \n    1.35 \n    [1.22, 1.49] \n    1.32 \n    [1.21, 1.43] \n    1.61 \n    [1.40, 1.84] \n    1.12 \n    [1.02, 1.22] \n  \n\n Positive Affect \n    None \n    1.36 \n    [1.21, 1.52] \n    1.63 \n    [1.49, 1.78] \n    0.85 \n    [0.74, 0.97] \n    1.97 \n    [1.77, 2.20] \n  \n\n Positive Affect \n    Age \n    1.11 \n    [0.99, 1.24] \n    1.35 \n    [1.23, 1.48] \n    0.77 \n    [0.68, 0.89] \n    1.61 \n    [1.44, 1.80] \n  \n\n Positive Affect \n    Self-Rated Health \n    1.16 \n    [1.03, 1.31] \n    1.42 \n    [1.29, 1.56] \n    0.79 \n    [0.68, 0.91] \n    1.61 \n    [1.44, 1.81] \n  \n\n Positive Affect \n    Gender \n    1.35 \n    [1.21, 1.52] \n    1.63 \n    [1.49, 1.78] \n    0.85 \n    [0.74, 0.97] \n    1.96 \n    [1.76, 2.19] \n  \n\n Positive Affect \n    Parental Education \n    1.31 \n    [1.17, 1.47] \n    1.55 \n    [1.42, 1.70] \n    0.87 \n    [0.75, 1.00] \n    1.87 \n    [1.68, 2.09] \n  \n\n Self-Esteem \n    None \n    1.00 \n    [0.88, 1.13] \n    0.86 \n    [0.79, 0.94] \n    0.95 \n    [0.81, 1.12] \n    1.09 \n    [0.97, 1.22] \n  \n\n Self-Esteem \n    Age \n    1.07 \n    [0.94, 1.22] \n    0.90 \n    [0.83, 0.99] \n    0.97 \n    [0.82, 1.15] \n    1.18 \n    [1.05, 1.33] \n  \n\n Self-Esteem \n    Self-Rated Health \n    0.94 \n    [0.83, 1.07] \n    0.79 \n    [0.73, 0.87] \n    0.92 \n    [0.78, 1.09] \n    0.98 \n    [0.87, 1.10] \n  \n\n Self-Esteem \n    Gender \n    1.00 \n    [0.88, 1.13] \n    0.86 \n    [0.79, 0.94] \n    0.94 \n    [0.81, 1.12] \n    1.09 \n    [0.98, 1.23] \n  \n\n Self-Esteem \n    Parental Education \n    0.99 \n    [0.87, 1.12] \n    0.86 \n    [0.79, 0.93] \n    0.96 \n    [0.82, 1.15] \n    1.08 \n    [0.97, 1.22] \n  \n\n Optimism \n    None \n    1.58 \n    [1.40, 1.79] \n    1.32 \n    [1.19, 1.46] \n    1.10 \n    [0.93, 1.30] \n    1.68 \n    [1.50, 1.88] \n  \n\n Optimism \n    Age \n    1.23 \n    [1.08, 1.41] \n    1.08 \n    [0.97, 1.20] \n    1.00 \n    [0.85, 1.19] \n    1.33 \n    [1.18, 1.50] \n  \n\n Optimism \n    Self-Rated Health \n    1.33 \n    [1.17, 1.51] \n    1.11 \n    [1.00, 1.24] \n    1.04 \n    [0.87, 1.24] \n    1.31 \n    [1.16, 1.48] \n  \n\n Optimism \n    Gender \n    1.58 \n    [1.40, 1.79] \n    1.32 \n    [1.19, 1.46] \n    1.10 \n    [0.93, 1.30] \n    1.68 \n    [1.50, 1.89] \n  \n\n Optimism \n    Parental Education \n    1.50 \n    [1.32, 1.70] \n    1.32 \n    [1.19, 1.47] \n    1.07 \n    [0.90, 1.27] \n    1.63 \n    [1.45, 1.83] \n  \n\n Depression \n    None \n    0.91 \n    [0.85, 0.98] \n    0.99 \n    [0.93, 1.06] \n    0.74 \n    [0.67, 0.82] \n    1.15 \n    [1.07, 1.24] \n  \n\n Depression \n    Age \n    0.88 \n    [0.81, 0.95] \n    0.99 \n    [0.92, 1.06] \n    0.73 \n    [0.66, 0.80] \n    1.16 \n    [1.07, 1.25] \n  \n\n Depression \n    Self-Rated Health \n    0.63 \n    [0.59, 0.69] \n    0.74 \n    [0.69, 0.80] \n    0.63 \n    [0.56, 0.70] \n    0.77 \n    [0.71, 0.83] \n  \n\n Depression \n    Gender \n    0.91 \n    [0.85, 0.98] \n    0.99 \n    [0.93, 1.06] \n    0.74 \n    [0.67, 0.82] \n    1.16 \n    [1.08, 1.25] \n  \n\n Depression \n    Parental Education \n    0.89 \n    [0.83, 0.96] \n    0.97 \n    [0.91, 1.04] \n    0.76 \n    [0.68, 0.84] \n    1.12 \n    [1.05, 1.21] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0\n\n\n\n\nSo this looks pretty good, except that it’s annoying how the Trait name is repeated five times. This is where we’ll use collapse_rows().\n\nCodetidy3 %>%\n  kable(., escape = F,\n        align = c(\"r\", \"r\", rep(\"c\", 8)),\n        col.names = c(\"Trait\", \"Covariate\", rep(c(\"OR\", \"CI\"), times = 4)),\n        caption = \"<strong>Table 3</strong><br><em>Estimated Personality-Outcome Associations</em>\") %>%\n  kable_styling(full_width = F) %>%\n  collapse_rows(1, valign = \"top\") %>%\n  add_header_above(heads) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 3Estimated Personality-Outcome Associations\n\n \n\n\nMove in with Partner\nMarried\nDivorced\nBirth of a Child\n\n\n Trait \n    Covariate \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    None \n    1.33 \n    [1.21, 1.47] \n    1.29 \n    [1.19, 1.40] \n    1.58 \n    [1.38, 1.80] \n    1.11 \n    [1.01, 1.22] \n  \n\n Age \n    1.20 \n    [1.08, 1.33] \n    1.17 \n    [1.08, 1.27] \n    1.51 \n    [1.32, 1.73] \n    0.99 \n    [0.90, 1.09] \n  \n\n Self-Rated Health \n    1.58 \n    [1.42, 1.75] \n    1.53 \n    [1.40, 1.66] \n    1.70 \n    [1.48, 1.95] \n    1.39 \n    [1.26, 1.53] \n  \n\n Gender \n    1.34 \n    [1.21, 1.48] \n    1.30 \n    [1.20, 1.41] \n    1.59 \n    [1.39, 1.82] \n    1.11 \n    [1.01, 1.22] \n  \n\n Parental Education \n    1.35 \n    [1.22, 1.49] \n    1.32 \n    [1.21, 1.43] \n    1.61 \n    [1.40, 1.84] \n    1.12 \n    [1.02, 1.22] \n  \n\n Positive Affect \n    None \n    1.36 \n    [1.21, 1.52] \n    1.63 \n    [1.49, 1.78] \n    0.85 \n    [0.74, 0.97] \n    1.97 \n    [1.77, 2.20] \n  \n\n Age \n    1.11 \n    [0.99, 1.24] \n    1.35 \n    [1.23, 1.48] \n    0.77 \n    [0.68, 0.89] \n    1.61 \n    [1.44, 1.80] \n  \n\n Self-Rated Health \n    1.16 \n    [1.03, 1.31] \n    1.42 \n    [1.29, 1.56] \n    0.79 \n    [0.68, 0.91] \n    1.61 \n    [1.44, 1.81] \n  \n\n Gender \n    1.35 \n    [1.21, 1.52] \n    1.63 \n    [1.49, 1.78] \n    0.85 \n    [0.74, 0.97] \n    1.96 \n    [1.76, 2.19] \n  \n\n Parental Education \n    1.31 \n    [1.17, 1.47] \n    1.55 \n    [1.42, 1.70] \n    0.87 \n    [0.75, 1.00] \n    1.87 \n    [1.68, 2.09] \n  \n\n Self-Esteem \n    None \n    1.00 \n    [0.88, 1.13] \n    0.86 \n    [0.79, 0.94] \n    0.95 \n    [0.81, 1.12] \n    1.09 \n    [0.97, 1.22] \n  \n\n Age \n    1.07 \n    [0.94, 1.22] \n    0.90 \n    [0.83, 0.99] \n    0.97 \n    [0.82, 1.15] \n    1.18 \n    [1.05, 1.33] \n  \n\n Self-Rated Health \n    0.94 \n    [0.83, 1.07] \n    0.79 \n    [0.73, 0.87] \n    0.92 \n    [0.78, 1.09] \n    0.98 \n    [0.87, 1.10] \n  \n\n Gender \n    1.00 \n    [0.88, 1.13] \n    0.86 \n    [0.79, 0.94] \n    0.94 \n    [0.81, 1.12] \n    1.09 \n    [0.98, 1.23] \n  \n\n Parental Education \n    0.99 \n    [0.87, 1.12] \n    0.86 \n    [0.79, 0.93] \n    0.96 \n    [0.82, 1.15] \n    1.08 \n    [0.97, 1.22] \n  \n\n Optimism \n    None \n    1.58 \n    [1.40, 1.79] \n    1.32 \n    [1.19, 1.46] \n    1.10 \n    [0.93, 1.30] \n    1.68 \n    [1.50, 1.88] \n  \n\n Age \n    1.23 \n    [1.08, 1.41] \n    1.08 \n    [0.97, 1.20] \n    1.00 \n    [0.85, 1.19] \n    1.33 \n    [1.18, 1.50] \n  \n\n Self-Rated Health \n    1.33 \n    [1.17, 1.51] \n    1.11 \n    [1.00, 1.24] \n    1.04 \n    [0.87, 1.24] \n    1.31 \n    [1.16, 1.48] \n  \n\n Gender \n    1.58 \n    [1.40, 1.79] \n    1.32 \n    [1.19, 1.46] \n    1.10 \n    [0.93, 1.30] \n    1.68 \n    [1.50, 1.89] \n  \n\n Parental Education \n    1.50 \n    [1.32, 1.70] \n    1.32 \n    [1.19, 1.47] \n    1.07 \n    [0.90, 1.27] \n    1.63 \n    [1.45, 1.83] \n  \n\n Depression \n    None \n    0.91 \n    [0.85, 0.98] \n    0.99 \n    [0.93, 1.06] \n    0.74 \n    [0.67, 0.82] \n    1.15 \n    [1.07, 1.24] \n  \n\n Age \n    0.88 \n    [0.81, 0.95] \n    0.99 \n    [0.92, 1.06] \n    0.73 \n    [0.66, 0.80] \n    1.16 \n    [1.07, 1.25] \n  \n\n Self-Rated Health \n    0.63 \n    [0.59, 0.69] \n    0.74 \n    [0.69, 0.80] \n    0.63 \n    [0.56, 0.70] \n    0.77 \n    [0.71, 0.83] \n  \n\n Gender \n    0.91 \n    [0.85, 0.98] \n    0.99 \n    [0.93, 1.06] \n    0.74 \n    [0.67, 0.82] \n    1.16 \n    [1.08, 1.25] \n  \n\n Parental Education \n    0.89 \n    [0.83, 0.96] \n    0.97 \n    [0.91, 1.04] \n    0.76 \n    [0.68, 0.84] \n    1.12 \n    [1.05, 1.21] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0\n\n\n\n\nThat’s a pretty good-looking table!"
  },
  {
    "objectID": "tables.html#moderators",
    "href": "tables.html#moderators",
    "title": "APA Tables Part 1",
    "section": "Moderators",
    "text": "Moderators\nAll right, time to introduce a moderator!\nThe procedure for this is going to be very close to above, with the main change being that the key term will no longer be p_value but p_value:moderator.\nBut first, the set up\n### Set up Data\nThis time, we need to add a new grouping variable – namely, the target moderators. We aren’t going to include the “none” condition here, as we assume we’ve already tested and presented those results separately.\n\nCodegsoep_nested4 <- gsoep_long %>%\n  group_by(Trait, Outcome, Covariate) %>%\n  nest() %>%\n  ungroup() %>%\n  arrange(Outcome, Trait, Covariate)\ngsoep_nested4\n\n\n\n  \n\n\n\nRun the models\n\nCodefactor_fun <- function(x){if(is.numeric(x)){diff(range(x, na.rm = T)) %in% 1:2 & length(unique(x)) <= 4} else{F}}\n\nmod4_fun <- function(d, cov){\n  d$o_value <- factor(d$o_value)\n  d <- d %>% mutate_if(factor_fun, factor)\n  if(cov == \"none\"){f <- formula(o_value ~ p_value)} else{f <- formula(o_value ~ p_value* c_value)}\n  glm(f, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested4 <- gsoep_nested4 %>%\n  mutate(m = map2(data, Covariate, mod4_fun),\n         tidy = map(m, ~tidy(., conf.int = T)))\ngsoep_nested4\n\n\n\n  \n\n\n\nLike before, we are going to have different numbers of rows. Again this is good becuase it means that the appropriate moderators and main effects were added.\nCreate the Table\n\nCodetidy4 <- gsoep_nested4 %>%\n  select(Outcome, Trait, Moderator = Covariate, tidy) %>%\n  unnest(tidy)\ntidy4\n\n\n\n  \n\n\n\nThe basic steps from here are similar: filter target terms, index significance, exponentiate, format values, create CI’s, bold significance, select needed columns.\nWe do need to a bit of work on how we index our moderators. Specifically, for the factor variables, we want to make sure we indicate what the levels are.\n\nCodetidy4 <- tidy4 %>%\n  filter(grepl(\"p_value:\", term)) %>%\n  mutate(term = str_replace(term, \"c_value\", Moderator),\n         Moderator = str_remove(term, \"p_value:\"),\n         sig = ifelse(sign(conf.low) == sign(conf.high), \"sig\", \"ns\")) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), ~sprintf(\"%.2f\", .)) %>%\n  mutate(CI = sprintf(\"[%s, %s]\", conf.low, conf.high)) %>%\n  mutate_at(vars(estimate, CI), ~ifelse(sig == \"sig\", sprintf(\"<strong>%s</strong>\", .), .)) %>%\n  select(Outcome, Trait, Moderator, OR = estimate, CI)\n\n\nNow we’re ready for the pivot_longer(), unite(), pivot_wider(), order columns (using select()) combo from before. The only difference is that I’m going to use pivot_wider() to do the uniting for me this time!\n\nCodeO_names <- tibble(\n  old = c(\"mvInPrtnr\", \"married\", \"divorced\", \"chldbrth\"),\n  new = c(\"Move in with Partner\", \"Married\", \"Divorced\", \"Birth of a Child\")\n)\nlevs <- paste(rep(O_names$old, each = 2), rep(c(\"OR\",\"CI\"), times = 4), sep = \"_\")\n\ntidy4 <- tidy4 %>%\n  pivot_longer(cols = c(OR, CI), names_to = \"est\", values_to = \"value\") %>%\n  pivot_wider(names_from = c(\"Outcome\", \"est\"), values_from = \"value\", names_sep = \"_\") %>%\n  select(Trait, Moderator, all_of(levs))\n\n\nAll right, time to use kable()! This will proceed almost exactly as the example with covariates, with the main difference being that our target term is now the interaction.\nGiven that we have multiple moderators, some of which have multiple levels, we’ll also want to order them. So we’ll make the Moderator column a factor as well. But we’ll do so after we’ve given our covariates nicer names. While we’re at it, we’ll go ahead and do the same things for our Trait column.\n\nCodem_names <- tibble(\n  old = c(\"age\", \"SRhealth\", \"gender1\", \"parEdu1\", \"parEdu2\"),\n  new = c(\"Age\", \"Self-Rated Health\", \"Gender (Female)\", \n          \"Parental Education (College)\", \"Parental Education (Beyond College)\")\n)\n\ntidy4 <- tidy4 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new),\n         Moderator = mapvalues(Moderator, from = m_names$old, to = m_names$new),\n         Moderator = factor(Moderator, levels = m_names$new)) %>%\n  arrange(Trait, Moderator)\n\n\nAnd again, for our spanned columns, we’ll take advantage of our O_names object to create the vector in advance:\n\nCodeheads <- rep(2, 5)\nheads\n\n[1] 2 2 2 2 2\n\nCodenames(heads) <- c(\" \", O_names$new)\nheads\n\n                     Move in with Partner              Married \n                   2                    2                    2 \n            Divorced     Birth of a Child \n                   2                    2 \n\n\nNow, this will proceed as before, including using collapse_rows(). The change this time will simply be to our table caption.\n\nCodetidy4 %>%\n  kable(., escape = F,\n        align = c(\"r\", \"r\", rep(\"c\", 8)),\n        col.names = c(\"Trait\", \"Moderator\", rep(c(\"OR\", \"CI\"), times = 4)),\n        caption = \"<strong>Table 4</strong><br><em>Estimated Moderators of Personality-Outcome Associations</em>\") %>%\n  kable_styling(full_width = F) %>%\n  collapse_rows(1, valign = \"top\") %>%\n  add_header_above(heads) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 4Estimated Moderators of Personality-Outcome Associations\n\n \n\n\nMove in with Partner\nMarried\nDivorced\nBirth of a Child\n\n\n Trait \n    Moderator \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    Age \n    1.00 \n    [1.00, 1.01] \n    1.00 \n    [1.00, 1.01] \n    1.00 \n    [0.99, 1.01] \n    1.00 \n    [1.00, 1.01] \n  \n\n Self-Rated Health \n    0.95 \n    [0.83, 1.09] \n    0.97 \n    [0.87, 1.09] \n    0.94 \n    [0.80, 1.11] \n    0.91 \n    [0.80, 1.05] \n  \n\n Gender (Female) \n    1.07 \n    [0.87, 1.31] \n    1.13 \n    [0.96, 1.32] \n    0.77 \n    [0.59, 1.01] \n    0.99 \n    [0.82, 1.20] \n  \n\n Parental Education (College) \n    1.07 \n    [0.85, 1.35] \n    0.83 \n    [0.69, 1.00] \n    1.13 \n    [0.76, 1.65] \n    1.11 \n    [0.89, 1.39] \n  \n\n Parental Education (Beyond College) \n    0.85 \n    [0.58, 1.22] \n    0.80 \n    [0.55, 1.14] \n    1.39 \n    [0.82, 2.33] \n    1.09 \n    [0.76, 1.56] \n  \n\n Positive Affect \n    Age \n    0.99 \n    [0.99, 1.00] \n    1.00 \n    [1.00, 1.01] \n    1.01 \n    [1.00, 1.02] \n    1.01 \n    [1.00, 1.02] \n  \n\n Self-Rated Health \n    1.22 \n    [1.06, 1.40] \n    0.95 \n    [0.83, 1.07] \n    1.01 \n    [0.85, 1.19] \n    0.92 \n    [0.78, 1.08] \n  \n\n Gender (Female) \n    1.09 \n    [0.87, 1.37] \n    1.16 \n    [0.97, 1.39] \n    1.13 \n    [0.86, 1.48] \n    1.26 \n    [1.01, 1.57] \n  \n\n Parental Education (College) \n    0.92 \n    [0.71, 1.20] \n    0.88 \n    [0.71, 1.10] \n    0.98 \n    [0.65, 1.49] \n    0.93 \n    [0.71, 1.22] \n  \n\n Parental Education (Beyond College) \n    1.06 \n    [0.70, 1.63] \n    1.14 \n    [0.75, 1.78] \n    0.47 \n    [0.28, 0.79] \n    0.49 \n    [0.34, 0.73] \n  \n\n Self-Esteem \n    Age \n    1.00 \n    [0.99, 1.01] \n    1.00 \n    [1.00, 1.01] \n    1.00 \n    [0.99, 1.01] \n    1.00 \n    [0.99, 1.01] \n  \n\n Self-Rated Health \n    1.03 \n    [0.86, 1.21] \n    0.99 \n    [0.88, 1.11] \n    1.13 \n    [0.92, 1.35] \n    0.99 \n    [0.83, 1.17] \n  \n\n Gender (Female) \n    0.77 \n    [0.59, 0.99] \n    0.90 \n    [0.76, 1.07] \n    1.15 \n    [0.83, 1.60] \n    0.73 \n    [0.57, 0.94] \n  \n\n Parental Education (College) \n    0.97 \n    [0.73, 1.32] \n    1.00 \n    [0.83, 1.21] \n    0.88 \n    [0.59, 1.36] \n    0.91 \n    [0.71, 1.18] \n  \n\n Parental Education (Beyond College) \n    0.95 \n    [0.62, 1.53] \n    0.97 \n    [0.65, 1.51] \n    NA \n    NA \n    0.82 \n    [0.51, 1.43] \n  \n\n Optimism \n    Age \n    1.00 \n    [0.99, 1.01] \n    1.01 \n    [1.00, 1.01] \n    1.01 \n    [1.00, 1.02] \n    1.02 \n    [1.01, 1.02] \n  \n\n Self-Rated Health \n    0.89 \n    [0.74, 1.05] \n    0.93 \n    [0.81, 1.07] \n    1.01 \n    [0.82, 1.24] \n    0.77 \n    [0.65, 0.92] \n  \n\n Gender (Female) \n    1.03 \n    [0.81, 1.32] \n    0.97 \n    [0.79, 1.18] \n    1.06 \n    [0.76, 1.48] \n    1.06 \n    [0.84, 1.33] \n  \n\n Parental Education (College) \n    0.91 \n    [0.68, 1.24] \n    0.85 \n    [0.65, 1.13] \n    0.74 \n    [0.44, 1.25] \n    1.04 \n    [0.76, 1.42] \n  \n\n Parental Education (Beyond College) \n    1.03 \n    [0.62, 1.75] \n    1.30 \n    [0.82, 2.15] \n    1.73 \n    [0.78, 4.28] \n    0.77 \n    [0.51, 1.19] \n  \n\n Depression \n    Age \n    1.00 \n    [1.00, 1.01] \n    1.01 \n    [1.00, 1.01] \n    1.00 \n    [1.00, 1.01] \n    1.00 \n    [1.00, 1.01] \n  \n\n Self-Rated Health \n    0.97 \n    [0.89, 1.06] \n    0.87 \n    [0.80, 0.95] \n    0.91 \n    [0.80, 1.02] \n    0.88 \n    [0.79, 0.97] \n  \n\n Gender (Female) \n    1.05 \n    [0.91, 1.21] \n    1.03 \n    [0.91, 1.17] \n    1.21 \n    [1.00, 1.48] \n    1.04 \n    [0.90, 1.20] \n  \n\n Parental Education (College) \n    0.87 \n    [0.73, 1.04] \n    0.94 \n    [0.79, 1.10] \n    0.99 \n    [0.73, 1.34] \n    0.80 \n    [0.67, 0.96] \n  \n\n Parental Education (Beyond College) \n    1.05 \n    [0.80, 1.40] \n    1.06 \n    [0.80, 1.43] \n    0.85 \n    [0.56, 1.33] \n    1.12 \n    [0.84, 1.52] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0"
  },
  {
    "objectID": "tables.html#simple-effects",
    "href": "tables.html#simple-effects",
    "title": "APA Tables Part 1",
    "section": "Simple Effects",
    "text": "Simple Effects\nWith moderator analyses, we typically need to unpack the results and present them as simple effects. In many cases, plots are super useful. I demonstrated how to make simple effects plots in the purrr tutorial, so check there for more!\nWe’re going to start here with our same nested data frame from the previous example. We’ll basically want to get estimates of the personality-outcome associations for different levels of our factor or continuous moderators.\nTo do this, we’ll use the predict() functions.\n** In progress **"
  },
  {
    "objectID": "tables.html#descriptives",
    "href": "tables.html#descriptives",
    "title": "APA Tables Part 1",
    "section": "Descriptives",
    "text": "Descriptives\nDescriptives tables are slightly tricky to add generalizable code for becuase the needs of those tables vary. Below, I’m going to give some code for a couple of basic descriptive tables I’d be likely to include for a paper looking at predictive personality-outcome associations.\nFirst, one indexing demographic characteristics and samples sizes. Second, one indexing means and standard deviations. Then finally wrapping up with a correlation table of study measures.\nDemographic Charateristics\nFirst, let’s look at demographic characteristics, including sample sizes, age, and gender.\nBecause I’m interested in events, I’m going to specifically look at sample sizes of those who did and did experience events as well as any gender or age differences.\n\nCodegsoep %>%\n  select(p_year, SID, Outcome, o_value, gender, age) %>%\n  group_by(p_year, Outcome, o_value) %>%\n  summarize(N = n(), \n            Female = sum(gender, na.rm = T), \n            Age = mean(age, na.rm = T), \n            SD_Age = sd(age, na.rm = T)) %>%\n  ungroup() %>%\n  pivot_longer(cols = N:SD_Age, names_to = \"measure\", values_to = \"value\") %>%\n  pivot_wider(names_from = c(\"measure\", \"o_value\"), values_from = \"value\",\n              names_sep = \"_\") %>%\n  mutate(N = sprintf(\"%s (%s)\", N_1, N_0),\n         `% Female` = sprintf(\"%.2f (%.2f)\", Female_1/N_1*100, Female_0/N_0*100),\n         `Mean` = sprintf(\"%.1f (%.1f)\", Age_1, Age_0),\n         `SD` = sprintf(\"%.1f (%.1f)\", SD_Age_1, SD_Age_0),\n         Outcome = mapvalues(Outcome, O_names$old, O_names$new),\n         Outcome = factor(Outcome, levels = O_names$new)) %>%\n  select(Outcome, Year = p_year, N:`SD`) %>%\n  arrange(Outcome, Year) %>%\n  kable(., escape = F,\n        caption = \"<strong>Table 7</strong><br><em>Descriptive Statistics of Those Who Experienced Outcomes versus Those Who Did Not\",\n        align = c(\"r\", \"r\", rep(\"c\", 4))) %>%\n  kable_styling(full_width = F) %>%\n  add_header_above(c(\" \" = 4, \"Age\" = 2)) %>%\n  collapse_rows(1, \"top\") %>%\n  add_footnote(label = \"Values in parentheses indicate demographics of those who did not experience outcomes after the noted year\", notation = \"none\")\n\n\n\n\nTable 7Descriptive Statistics of Those Who Experienced Outcomes versus Those Who Did Not\n\n \n\n\nAge\n\n\n Outcome \n    Year \n    N \n    % Female \n    Mean \n    SD \n  \n\n\n\n Move in with Partner \n    2002 \n    734 (10381) \n    52.72 (51.61) \n    21.5 (41.4) \n    12.6 (17.8) \n  \n\n 2005 \n    460 (8802) \n    54.35 (52.08) \n    18.8 (39.6) \n    12.4 (18.2) \n  \n\n 2006 \n    433 (9387) \n    53.58 (52.43) \n    20.8 (41.2) \n    13.8 (18.2) \n  \n\n 2007 \n    671 (17433) \n    55.14 (52.41) \n    20.1 (40.4) \n    13.9 (18.3) \n  \n\n 2010 \n    626 (31164) \n    50.32 (52.14) \n    21.3 (40.7) \n    15.1 (18.9) \n  \n\n Married \n    2002 \n    837 (10309) \n    52.21 (51.14) \n    24.3 (41.0) \n    12.3 (18.1) \n  \n\n 2005 \n    632 (8766) \n    51.58 (51.87) \n    23.2 (39.1) \n    12.8 (18.6) \n  \n\n 2006 \n    602 (9370) \n    51.00 (52.27) \n    24.1 (40.8) \n    13.2 (18.6) \n  \n\n 2007 \n    1042 (17410) \n    51.06 (52.19) \n    22.9 (40.0) \n    13.5 (18.6) \n  \n\n 2010 \n    1210 (31095) \n    52.73 (52.00) \n    19.9 (40.6) \n    14.7 (19.0) \n  \n\n Divorced \n    2002 \n    320 (11374) \n    53.12 (51.70) \n    31.4 (39.6) \n    11.5 (18.3) \n  \n\n 2005 \n    216 (9924) \n    52.78 (52.19) \n    30.1 (37.5) \n    10.7 (18.6) \n  \n\n 2006 \n    204 (10564) \n    53.43 (52.52) \n    31.3 (39.0) \n    11.7 (18.7) \n  \n\n 2007 \n    360 (19822) \n    54.44 (52.52) \n    30.7 (38.2) \n    11.8 (18.7) \n  \n\n 2010 \n    336 (35446) \n    52.38 (52.34) \n    31.1 (38.6) \n    12.4 (19.1) \n  \n\n Birth of a Child \n    2002 \n    779 (10153) \n    53.02 (51.62) \n    21.9 (41.9) \n    10.2 (17.9) \n  \n\n 2005 \n    543 (8649) \n    54.51 (51.95) \n    20.4 (40.0) \n    10.7 (18.4) \n  \n\n 2006 \n    483 (9264) \n    54.66 (52.29) \n    21.1 (41.6) \n    11.0 (18.4) \n  \n\n 2007 \n    795 (17267) \n    52.83 (52.30) \n    20.8 (40.8) \n    11.3 (18.5) \n  \n\n 2010 \n    821 (30967) \n    57.25 (52.02) \n    19.5 (41.2) \n    12.3 (18.9) \n  \n\n\n\n Values in parentheses indicate demographics of those who did not experience outcomes after the noted year\n\n\n\n\nMeans and Standard Deviations\nZero-Order Correlations"
  },
  {
    "objectID": "06-week6-workbook.html#visualizing-uncertainty-1",
    "href": "06-week6-workbook.html#visualizing-uncertainty-1",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Visualizing Uncertainty",
    "text": "Visualizing Uncertainty\n\nWhy is visualizing uncertainty important?\n\nPoint estimates are over-emphasized and interval estimates are unemphasized (or ignored)\nMost people misperceive both (1) common uncertainty visualizations and (2) most common uncertainty metrics\nIn other words, people make errors about error\nProbability is hard, and most aren’t taught about probability distributions (and more)"
  },
  {
    "objectID": "06-week6-workbook.html#theories-of-visualizing-uncertainty",
    "href": "06-week6-workbook.html#theories-of-visualizing-uncertainty",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Theories of Visualizing Uncertainty",
    "text": "Theories of Visualizing Uncertainty\nWhy do people misperceive uncertainty, and how can we mitigate it?"
  },
  {
    "objectID": "06-week6-workbook.html#quick-side-note-custom-themes",
    "href": "06-week6-workbook.html#quick-side-note-custom-themes",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Quick Side Note: Custom Themes",
    "text": "Quick Side Note: Custom Themes\n\nWe have a lot to cover today, so I’m going to skip over some of the usual “how to start with the basics and make it aesthetically pleasing”\nInstead, we’ll create a custom these that captures some of our usual additions\n\nThis will save us both time and text!\n\nI highly recommend doing this in your own work\n\n\nCodemy_theme <- function(){\n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , legend.title = element_text(face = \"bold\", size = rel(1))\n    , legend.text = element_text(face = \"italic\", size = rel(1))\n    , axis.text = element_text(face = \"bold\", size = rel(1.1), color = \"black\")\n    , axis.title = element_text(face = \"bold\", size = rel(1.2))\n    , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", size = rel(1.2), hjust = .5)\n    , strip.text = element_text(face = \"bold\", size = rel(1.1), color = \"white\")\n    , strip.background = element_rect(fill = \"black\")\n    )\n}"
  },
  {
    "objectID": "06-week6-workbook.html#error-bars",
    "href": "06-week6-workbook.html#error-bars",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Error Bars",
    "text": "Error Bars\n\nFirst, let’s examine the usual ways that we show uncertainty around point estimates (e.g., means, model parameter estimates, etc.) using interval estimates (+/- 1 SE/D, confidence interval)\n\n\n\n\n\n  \n\n\n\n\nCodepomp <- function(x) (x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))*10\ngsoep %>%\n  filter(year == 2005) %>%\n  filter(SID %in% sample(unique(.$SID, 500))) %>%\n  mutate(SRhealth = pomp(SRhealth)) %>%\n  group_by(SID, age) %>%\n  mutate(health = rowMeans(cbind(SRhealth, satHealth))) %>%\n  ungroup() %>%\n  select(SID, age, health) %>%\n  drop_na()\n\n\n\n  \n\n\n\n\nCodegsoep_desc <- gsoep %>%\n  filter(year == 2005 & age < 30) %>%\n  filter(SID %in% sample(unique(.$SID), 500)) %>%\n  mutate(SRhealth = pomp(SRhealth)) %>%\n  group_by(SID, age) %>%\n  mutate(health = rowMeans(cbind(SRhealth, satHealth), na.rm = T)) %>%\n  ungroup() %>%\n  select(SID, age, health) %>%\n  drop_na() %>%\n  mutate(\n    mean = mean(health)\n    , sd = sd(health)\n    , se = sd/sqrt(n())\n    , ci99 = se*2.576\n    , ci95 = se*1.96\n    , ci80 = se*1.282\n    )\ngsoep_desc\n\n\n\n  \n\n\n\n\nCodegsoep_desc <- gsoep_desc %>% \n  select(mean:ci80, health, SID) %>%\n  pivot_longer(\n    cols = c(-mean, -SID)\n    , names_to = \"measure\"\n    , values_to = \"value\"\n    ) %>%\n  mutate(SID = ifelse(measure == \"health\" | row_number() %in% 1:5, SID, NA)) %>%\n  drop_na() %>%\n  mutate(measure = factor(measure, rev(c(\"health\", \"sd\", \"se\", \"ci99\", \"ci95\", \"ci80\"))))\ngsoep_desc\n\n\n\n  \n\n\n\n\nCodegsoep_desc %>%\n  ggplot(aes(y = measure, x = mean)) +\n    geom_point(size = 3, color = \"darkorange3\") + \n    geom_jitter(\n      data = gsoep_desc %>% filter(measure == \"health\")\n      , aes(x = value), alpha = .5, height = .3, width = 0\n    ) + \n    geom_errorbar(\n      data =  gsoep_desc %>% filter(measure != \"health\")\n      , aes(xmin = mean - value, xmax = mean + value)\n      , width = .1\n      ) + \n    geom_point(size = 3, color = \"darkorange3\") + \n    my_theme()"
  },
  {
    "objectID": "06-week6-workbook.html#so-what-do-we-do",
    "href": "06-week6-workbook.html#so-what-do-we-do",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "So What Do We Do?",
    "text": "So What Do We Do?\n\nLots of things, not all of which we have time for today. See also:\n\nhttps://janhove.github.io/visualise_uncertainty/\nhttps://wilkelab.org/SDS375/slides/visualizing-uncertainty.html#1\nhttps://wilkelab.org/ungeviz/\n#TeamBayes: http://mjskay.github.io/tidybayes/\n\n#TeamFrequentist: https://mjskay.github.io/ggdist/"
  },
  {
    "objectID": "06-week6-workbook.html#outline-for-today",
    "href": "06-week6-workbook.html#outline-for-today",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Outline for Today:",
    "text": "Outline for Today:\n\nProportions / Probability\n\nicon array\n\n\nPoint Estimates\n\nhalf-eye\ngradient interval\nquantile dotplot\nraincloud\n\n\nAnimated (sometimes)\n\nhypothetical outcome plots\nensemble display"
  },
  {
    "objectID": "06-week6-workbook.html#point-estimates-from-model-predictions",
    "href": "06-week6-workbook.html#point-estimates-from-model-predictions",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Point Estimates From Model Predictions",
    "text": "Point Estimates From Model Predictions"
  },
  {
    "objectID": "06-week6-workbook.html#marginal-means-from-model-predictions",
    "href": "06-week6-workbook.html#marginal-means-from-model-predictions",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Marginal Means from Model Predictions",
    "text": "Marginal Means from Model Predictions"
  },
  {
    "objectID": "06-week6-workbook.html#point-estimates-and-marginal-means",
    "href": "06-week6-workbook.html#point-estimates-and-marginal-means",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Point Estimates and Marginal Means",
    "text": "Point Estimates and Marginal Means\n\n\nstat_halfeye(): Visual Boundaries\n\nstat_eye(): Visual Boundaries\n\nstat_gradientinterval(): Visual Semiotics\n\nstat_dots(): Frequency Framing\n\nstat_dotsinterval(): Frequency Framing\n\nstat_halfeye()+ stat_dots(): Visual Boundaries + Frequency Framing\n\nCore syntax\n\nThe benefit of ggdist is that it allows you to use essentially identical syntax to produce lots of different kinds of plots\n\nAll we have to do is swap out the geom\n\n\n\nCodem1 <- lm(SRhealth ~ marital + age, data = gsoep_ex)\ntidy1 <- tidy(m1)\n\ntidy1 %>%\n  filter(term != \"(Intercept)\") %>%\n  ggplot(aes(y = term)) + \n    stat_halfeye(\n        aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n    ) +\n  my_theme()\n\n\n\n\nstat_halfeye()\nWe can pull the predictions from model terms or marginal means\nModel Terms:\n\nCodetidy1 %>%\n  filter(term != \"(Intercept)\") %>%\n  ggplot(aes(y = term)) + \n    stat_halfeye(\n        aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n    ) +\n  my_theme()\n\n\n\n\nMarginal Means:\n\nCodegsoep_ex %>%\n  data_grid(marital) %>%\n  mutate(age = mean(gsoep_ex$age)) %>%\n  augment(m1, newdata = ., se_fit = T) %>%\n  ggplot(aes(y = marital)) + \n    stat_halfeye(\n        aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)), \n    ) +\n  my_theme()\n\n\n\n\nLet’s do a little hack and create our whole plots except the geom, so that we can build them with less syntax:\n\nCodep1 <- tidy1 %>%\n  filter(term != \"(Intercept)\") %>%\n  ggplot(aes(y = term)) + \n  labs(\n    x = \"Parameter Estimate\"\n    , y = NULL\n    , title = \"Model Estimates\"\n    , caption = \"Outcome: Self-Rated Health\"\n    ) +\n  my_theme()\n\n\n\nCodep2 <- gsoep_ex %>%\n  data_grid(marital) %>%\n  mutate(age = mean(gsoep_ex$age)) %>%\n  augment(m1, newdata = ., se_fit = T) %>%\n  ggplot(aes(y = marital)) + \n  labs(\n    x = \"Model Predicted Self-Rated Health\"\n    , title = \"Marginal Means\"\n    , y = NULL\n    ) +\n  my_theme()\n\n\nWe can pull the predictions from model terms or marginal means\nModel Terms:\n\nCodep1 +\n  stat_halfeye(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n  ) + \n  labs(subtitle = \"stat_halfeye()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_halfeye(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)), \n  ) + \n  labs(subtitle = \"stat_halfeye()\")\n\n\n\n\nstat_eye()\nModel Terms:\n\nCodep1 +\n  stat_eye(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n  ) + \n  labs(subtitle = \"stat_eye()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_eye(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)), \n  ) + \n  labs(subtitle = \"stat_eye()\")\n\n\n\n\nstat_gradientinterval()\nModel Terms:\n\nCodep1 +\n  stat_gradientinterval(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n  ) + \n  labs(subtitle = \"stat_gradientinterval()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_gradientinterval(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)), \n  ) + \n  labs(subtitle = \"stat_gradientinterval()\")\n\n\n\n\nstat_dots()\nModel Terms:\n\nCodep1 +\n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)), \n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nYou can also change the number of dots:\nModel Terms:\n\nCodep1 +\n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n      , quantiles = 50\n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nThere are also three different layouts\nlayout = \"bin\":\n\nCodep2 + \n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n      , layout = \"bin\"\n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nlayout = \"weave\":\n\nCodep2 + \n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n      , layout = \"weave\"\n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nlayout = \"swarm\":\n\nCodep2 + \n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n      , layout = \"swarm\"\n  ) + \n  labs(subtitle = \"stat_dots()\")\n\n\n\n\nstat_dotsinterval()\nModel Terms:\n\nCodep1 +\n  stat_dotsinterval(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n      , quantiles = 50\n  ) + \n  labs(subtitle = \"stat_dotsinterval()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_dotsinterval(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n  ) + \n  labs(subtitle = \"stat_dotsinterval()\")\n\n\n\n\nYou can apply many of the same arguments as “regular” stat_dots()\nModel Terms:\n\nCodep1 +\n  stat_dotsinterval(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n      , quantiles = 50\n      , layout = \"swarm\"\n  ) + \n  labs(subtitle = \"stat_dotsinterval()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_dotsinterval(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n      , layout = \"swarm\"\n  ) + \n  labs(subtitle = \"stat_dotsinterval()\")\n\n\n\n\n\nstat_halfeye()+ stat_dots()\n\nModel Terms:\n\nCodep1 +\n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n      , quantiles = 50\n      , side = \"bottomleft\"\n      , layout = \"swarm\"\n  ) + \n  stat_halfeye(\n    aes(xdist = dist_student_t(df = df.residual(m1), mu = estimate, sigma = std.error))\n  ) + \n  labs(subtitle = \"`stat_halfeye()`+ `stat_dots()\")\n\n\n\n\nMarginal Means:\n\nCodep2 + \n  stat_dots(\n      aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n      , quantiles = 50\n      , side = \"bottomleft\"\n      , layout = \"swarm\"\n  ) + \n  stat_halfeye(\n    aes(xdist = dist_student_t(df = df.residual(m1), mu = .fitted, sigma = .se.fit)) \n  ) + \n  labs(subtitle = \"`stat_halfeye()`+ `stat_dots()\")"
  },
  {
    "objectID": "06-week6-workbook.html#geom_line",
    "href": "06-week6-workbook.html#geom_line",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "geom_line()",
    "text": "geom_line()\n\nCodeb_df3 %>%\n  ggplot(aes(x = age, y = pred)) + \n    geom_line(\n      aes(color = marital, group = interaction(marital, boot))\n      , alpha = .2, size = .25\n      ) + \n    my_theme()"
  },
  {
    "objectID": "06-week6-workbook.html#geom_lineribbon-summarized",
    "href": "06-week6-workbook.html#geom_lineribbon-summarized",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "\ngeom_lineribbon(): summarized",
    "text": "geom_lineribbon(): summarized\n\nCodeb_df3 %>%\n  group_by(marital, age) %>%\n  median_qi(pred) %>%\n  ggplot(aes(x = age, y = pred, ymin = .lower, ymax = .upper)) +\n  geom_lineribbon(aes(fill = marital), size = .9) + \n  scale_fill_brewer(palette = \"Set2\") +\n  my_theme()"
  },
  {
    "objectID": "06-week6-workbook.html#geom_lineribbon-bands-summarized",
    "href": "06-week6-workbook.html#geom_lineribbon-bands-summarized",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "\ngeom_lineribbon() bands: summarized",
    "text": "geom_lineribbon() bands: summarized\n\nCodeb_df3 %>%\n  group_by(marital, age) %>%\n  median_qi(pred, .width = c(.50, .80, .95)) %>%\n  ggplot(aes(x = age, y = pred, ymin = .lower, ymax = .upper)) +\n  geom_lineribbon(size = .9) + \n  scale_fill_brewer() +\n  facet_grid(~marital) + \n  my_theme()"
  },
  {
    "objectID": "06-week6-workbook.html#stat_lineribbon-bands-samples",
    "href": "06-week6-workbook.html#stat_lineribbon-bands-samples",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "\nstat_lineribbon() bands: samples",
    "text": "stat_lineribbon() bands: samples\n\nCodeb_df3 %>%\n  ggplot(aes(x = age, y = pred, fill = marital)) + \n  stat_lineribbon(alpha = .25) + \n  my_theme()\n\n\n\n\nWe can also use a new aesthetic: fill_ramp:\n\nCodeb_df3 %>%\n  ggplot(aes(x = age, y = pred, fill = marital)) + \n  stat_lineribbon(aes(fill_ramp = stat(level))) +\n  my_theme()"
  },
  {
    "objectID": "06-week6-workbook.html#geom_lineribbon-gradient-samples",
    "href": "06-week6-workbook.html#geom_lineribbon-gradient-samples",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "\ngeom_lineribbon() gradient: samples",
    "text": "geom_lineribbon() gradient: samples\n\nCodeb_df3 %>%\n  ggplot(aes(x = age, y = pred, fill = marital)) + \n  stat_lineribbon(alpha = .25, .width = ppoints(25)) + \n  scale_fill_brewer(palette = \"Set2\") +\n  my_theme()\n\n\n\n\nLet’s clean it up:\n\nCodems <- b_df3 %>% filter(age == max(age)) %>% group_by(marital) %>% summarize(m = mean(pred))\n\nb_df3 %>%\n  ggplot(aes(x = age*10, y = pred, fill = marital, fill_ramp = stat(.width))) + \n  stat_lineribbon(alpha = .25, .width = ppoints(25)) +\n  scale_x_continuous(limits = c(15,100), breaks = seq(15,90,15)) + \n  scale_fill_manual(values = c(\"grey\", \"darkorange\")) + \n  annotate(\"text\", label = \"Married\", x = max(b_df3$age)*10+1, y = ms$m[1], hjust = 0) + \n  annotate(\"text\", label = \"Never\\nMarried\", x = max(b_df3$age)*10+1, y = ms$m[2], hjust = 0) + \n  labs(\n    x = \"Age (Years)\"\n    , y = \"Predicted Self Rated Health\\n(Bootstrapped Interval Estimates)\"\n    , fill = NULL\n    , title = \"Self-Rated Health Declines More Rapidly for Unmarried People\"\n    ) + \n  guides(fill = \"none\") + \n  my_theme()"
  },
  {
    "objectID": "06-week6-workbook.html#ensemble-displays",
    "href": "06-week6-workbook.html#ensemble-displays",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Ensemble Displays",
    "text": "Ensemble Displays\n\nEnsemble displays are an alternative to putting hard boundaries around an interval estimate\nRemember that hard boundaries make people interpret categorical differences even when the underlying distribution is continuous\nWe’ve already seen this:\n\n\n\n\n\n\n\nBut the challenge with visualizing uncertainty is between inference and understanding\nWe need to leverage a knowledge of perception and cognitive processes to help us leverage strengths and overcome weaknesses\nAnimating visualizations can help us nudge people to process was they see and update their uncertainty estimates over time\n\nHere’s a quick example:\n\n\n\n\n\nLet’s break this down:\n\nCodeb_df3 %>%\n  group_by(marital, age) %>%\n  median_qi(pred) \n\n\n\n  \n\n\n\nNow, let’s plot the ribbon:\n\nCodeb_df3 %>%\n  group_by(marital, age) %>%\n  median_qi(pred) %>%\n  ggplot(aes(x = age, y = pred)) +\n    geom_ribbon(\n      aes(fill = marital, ymin = .lower, ymax = .upper)\n      , size = .9, alpha = .5\n      ) + \n    scale_fill_brewer(palette = \"Set2\") +\n    facet_grid(~marital) + \n    my_theme() + \n    theme(legend.position = \"none\")\n\n\n\n\nAnd add the lines (all of them, it will be ugly):\n\nCodeb_df3 %>%\n  group_by(marital, age) %>%\n  median_qi(pred) %>%\n  ggplot(aes(x = age, y = pred)) +\n    geom_ribbon(\n      aes(fill = marital, ymin = .lower, ymax = .upper)\n      , size = .9, alpha = .5\n      ) + \n    scale_fill_brewer(palette = \"Set2\") +\n    geom_line(\n      data = b_df3\n      , aes(group = interaction(marital, boot))\n      , size = 1\n      ) +\n    facet_grid(~marital) + \n    my_theme() + \n    theme(legend.position = \"none\")\n\n\n\n\nAnd finally, use transition_states() to animate it\n\nCodeb_df3 %>%\n  group_by(marital, age) %>%\n  median_qi(pred) %>%\n  ggplot(aes(x = age, y = pred)) +\n    geom_ribbon(\n      aes(fill = marital, ymin = .lower, ymax = .upper)\n      , size = .9, alpha = .5\n      ) + \n    scale_fill_brewer(palette = \"Set2\") +\n    geom_line(\n      data = b_df3\n      , aes(group = interaction(marital, boot))\n      , size = 1\n      ) +\n    facet_grid(~marital) + \n    my_theme() + \n    theme(legend.position = \"none\") + \n    transition_states(boot, 1, 1)"
  },
  {
    "objectID": "06-week6-workbook.html#hypothetical-outcome-plots-hops",
    "href": "06-week6-workbook.html#hypothetical-outcome-plots-hops",
    "title": "Week 6: Visualizing Uncertainty",
    "section": "Hypothetical Outcome Plots (HOPs)",
    "text": "Hypothetical Outcome Plots (HOPs)\n\nSimilarly, hypothetical outcome plots let us see plausible mean estimates among raw data\nHere’s self-rated health (1-5) across married and unmarried individuals:\n\n\nCodegsoep_ex3 %>%\n  ggplot(aes(y = marital, x = SRhealth)) + \n    geom_jitter(aes(color = marital), alpha = .5) + \n    my_theme()\n\n\n\n\n\nUsing the ungeviz package, we can then use the geom_vpline() function to sample from the data across groups and plot the mean from different samples:\n\n\nCodegsoep_ex3 %>%\n  ggplot(aes(y = marital, x = SRhealth)) + \n    geom_jitter(aes(color = marital), alpha = .25) + \n    geom_vpline(\n      data = sampler(25, group = marital)\n      , height = 0.6\n      , color = \"#D55E00\"\n      ) +\n    scale_color_manual(values = c(\"seagreen2\", \"darkorange\")) + \n    my_theme()\n\n\n\n\nAnd finally, we can animate those samples the transition_states() function from the gganimate package again:\n\nCodegsoep_ex3 %>%\n  ggplot(aes(y = marital, x = SRhealth)) + \n    geom_jitter(aes(color = marital), alpha = .5) + \n    geom_vpline(\n      data = sampler(25, group = marital)\n      , height = 0.6\n      , color = \"#D55E00\"\n      ) +\n    scale_color_manual(values = c(\"seagreen2\", \"darkorange\")) + \n    my_theme() + \n    transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "associations.html#the-data",
    "href": "associations.html#the-data",
    "title": "Visualizing Associations and Models",
    "section": "The Data",
    "text": "The Data\n\nCodeload(url(\"https://github.com/emoriebeck/psc290-data-viz-2022/blob/main/04-week4-associations/04-data/week4-data.RData?raw=true\"))\npred_data"
  },
  {
    "objectID": "associations.html#scatterplots",
    "href": "associations.html#scatterplots",
    "title": "Visualizing Associations and Models",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nScatterplots are pretty ubiquitous\nFrom a data visualization standpoint, this makes sense\nScatterplots\n\nshow raw data\n\nare common enough that little visualization literacy is needed\nallow for lots of summaries to be placed atop them\nthis is why they are our entry point for today"
  },
  {
    "objectID": "associations.html#scatterplots---basics",
    "href": "associations.html#scatterplots---basics",
    "title": "Visualizing Associations and Models",
    "section": "Scatterplots - Basics",
    "text": "Scatterplots - Basics\n\nCodepred_data %>% \n  select(study, SID, p_value, SRhealth)\n\n\n\n  \n\n\n\nLet’s look at a basic scatterplot:\n\nCodepred_data %>% \n  select(study, SID, p_value, SRhealth) %>%\n  ggplot(aes(x = p_value, y = SRhealth)) + \n    geom_point(shape = 21, fill = \"grey80\", color = \"black\", size = 2) + \n    labs(\n      x = \"Agreeableness (POMP; 0-10)\"\n      , y = \"Self-Rated Health (POMP; 0-10)\"\n    ) + \n    theme_classic()\n\n\n\n\nNow let’s add a trend line:\n\nCodepred_data %>% \n  select(study, SID, p_value, SRhealth) %>%\n  ggplot(aes(x = p_value, y = SRhealth)) + \n    geom_point(shape = 21, fill = \"grey80\", color = \"black\", size = 2) + \n    geom_smooth(method = \"lm\", size = 3, se = F) + \n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Self-Rated Health (POMP; 0-10)\"\n    ) + \n    theme_classic()\n\n\n\n\nBut we have multiple studies, so we need to separate them out using facet_wrap()\n\nCodepred_data %>% \n  select(study, SID, p_value, SRhealth) %>%\n  filter(!is.na(SRhealth)) %>%\n  ggplot(aes(x = p_value, y = SRhealth)) + \n    geom_point(shape = 21, fill = \"grey80\", color = \"black\", size = 2) + \n    scale_fill_manual(values = c(\"grey80\", \"seagreen4\")) + \n    facet_wrap(~study) +\n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Self-Rated Health (POMP; 0-10)\"\n    ) + \n    theme_classic()\n\n\n\n\n\nCodepred_data %>% \n  select(study, SID, p_value, SRhealth) %>%\n  filter(!is.na(SRhealth)) %>%\n  ggplot(aes(x = p_value, y = SRhealth)) + \n    geom_point(shape = 21, fill = \"grey80\", color = \"black\", size = 2, alpha = .25) + \n    geom_smooth(method = \"lm\", size = 3, se = F) + \n    scale_fill_manual(values = c(\"grey80\", \"seagreen4\")) + \n    facet_wrap(~study) +\n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Self-Rated Health (POMP; 0-10)\"\n    ) + \n    theme_classic()\n\n\n\n\nBut if you remember from your readings, we don’t typically want to show associations without some sort of estimate of error, confidence, etc.\n\nCodepred_data %>% \n  select(study, SID, p_value, SRhealth) %>%\n  filter(!is.na(SRhealth)) %>%\n  ggplot(aes(x = p_value, y = SRhealth)) + \n    geom_point(shape = 21, fill = \"grey80\", color = \"black\", size = 2, alpha = .25) + \n    geom_smooth(method = \"lm\", size = 1.5, se = T, color = \"black\") + \n    scale_fill_manual(values = c(\"grey80\", \"seagreen4\")) + \n    facet_wrap(~study) +\n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Self-Rated Health (POMP; 0-10)\"\n      , title = \"Conscientiousness -- Self-Rated Health Associations Across Samples\"\n    ) + \n    theme_classic()"
  },
  {
    "objectID": "associations.html#correlations-and-correlelograms",
    "href": "associations.html#correlations-and-correlelograms",
    "title": "Visualizing Associations and Models",
    "section": "Correlations and Correlelograms",
    "text": "Correlations and Correlelograms\n\nUnderstanding associations is always important, but perhaps never more so than when we do descriptives\n\nMy hot take is that zero-order correlation maatrices should always be included in papers\n\nSomeone’s meta-analysis will thank you\n\n\nIf you’re dumping correlations in supplementary materials, then tables are fine\n\nBut you (and your brain) will thank yourself if you use heat maps or correlelograms to visualize the correlations\n\n(Remember how quickly and preattentively we perceive color and size?)\n\n\nThere are R packages for this, but where’s the fun in that?\nAll right, let’s estimate some correlation matrices for each sample.\n\n\nCoder_data <- pred_data %>%\n  select(study, p_value, age, gender, SRhealth, smokes, exercise, BMI, education, parEdu, mortality = o_value) %>%\n  mutate_if(is.factor, ~as.numeric(as.character(.))) %>%\n  group_by(study) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(r = map(data, ~cor(., use = \"pairwise\")))\nr_data\n\n\n\n  \n\n\n\n\nThe thing is that we know ggplot doesn’t like wide form data, which is what cor() produces\n\n\nCoder_data$r[[1]]\n\n               p_value          age       gender    SRhealth       smokes\np_value    1.000000000 -0.005224085  0.053627861  0.15917525 -0.069013463\nage       -0.005224085  1.000000000 -0.057243245 -0.22438335 -0.078788619\ngender     0.053627861 -0.057243245  1.000000000 -0.03182278  0.022275557\nSRhealth   0.159175251 -0.224383351 -0.031822781  1.00000000 -0.129241536\nsmokes    -0.069013463 -0.078788619  0.022275557 -0.12924154  1.000000000\nexercise   0.048576025 -0.361768736  0.061659017  0.34546038 -0.155018841\nBMI       -0.019741798  0.036151816  0.012217132 -0.09340105 -0.037713371\neducation  0.001465775 -0.173399716 -0.001603648  0.11008540 -0.096936630\nparEdu     0.019871078 -0.374733606  0.055468171  0.08273023  0.005215303\nmortality -0.089637524  0.627069166 -0.092109448 -0.31142292  0.035759332\n             exercise         BMI    education       parEdu   mortality\np_value    0.04857602 -0.01974180  0.001465775  0.019871078 -0.08963752\nage       -0.36176874  0.03615182 -0.173399716 -0.374733606  0.62706917\ngender     0.06165902  0.01221713 -0.001603648  0.055468171 -0.09210945\nSRhealth   0.34546038 -0.09340105  0.110085399  0.082730234 -0.31142292\nsmokes    -0.15501884 -0.03771337 -0.096936630  0.005215303  0.03575933\nexercise   1.00000000 -0.06217297  0.210204022  0.176766791 -0.32138385\nBMI       -0.06217297  1.00000000 -0.048914825 -0.075000576  0.01643219\neducation  0.21020402 -0.04891483  1.000000000  0.232321970 -0.17215791\nparEdu     0.17676679 -0.07500058  0.232321970  1.000000000 -0.18796244\nmortality -0.32138385  0.01643219 -0.172157913 -0.187962436  1.00000000\n\n\nReshaping\n\nSo we need to reshape it in long form\nWe’re going to use a function so we only have to write the code once and can apply it to all the samples\nHere’s what we’ll do:\n\nremove the lower triangle and the diagonal of the correlation matrix\nmake matrix a data frame\nmake the row names of the matrix a column\nmake the columns long\nfactor them to retain order\n\n\n\n\nCoder_reshape_fun <- function(r){\n  coln <- colnames(r)\n  # remove lower tri and diagonal\n  r[lower.tri(r, diag = T)] <- NA\n  r %>% data.frame() %>%\n    rownames_to_column(\"V1\") %>%\n    pivot_longer(\n      cols = -V1\n      , values_to = \"r\"\n      , names_to = \"V2\"\n    ) %>%\n    mutate_at(vars(V1, V2), ~factor(., coln))\n}\n\nr_data <- r_data %>%\n  mutate(r = map(r, r_reshape_fun))\nr_data$r[[1]]\n\n\n\n  \n\n\n\nHeat Map Time!\nThis is, technically, a heat map, but I think we can do better!\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(\n    x = V1\n    , y = V2\n    , fill = r\n  )) + \n  geom_raster() + \n  theme_minimal()\n\n\n\n\nColors\nLet’s add some intuitive colors using scale_fill_gradient2()\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n  geom_raster() + \n  scale_fill_gradient2(\n    limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\"\n    , high = \"red\"\n    , mid = \"white\"\n    , na.value = \"white\"\n    ) + \n  theme_minimal()\n\n\n\n\nLabels\nDo we need axis labels?\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n  geom_raster() + \n  scale_fill_gradient2(limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables in Sample 1\"\n    ) + \n  theme_minimal()\n\n\n\n\nTheme Elements\nLet’s fix the theme elements. So close!\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n  geom_raster() + \n  scale_fill_gradient2(limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    , subtitle = \"Sample 1\"\n    ) + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )\n\n\n\n\nFinishing Touches!\nLet’s fix the theme elements. So close!\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n  geom_raster() + \n  geom_text(aes(label = round(r, 2))) + \n  scale_fill_gradient2(limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    , subtitle = \"Sample 1\"\n    ) + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )\n\n\n\n\nCorrelelogram\nA correlelogram is basically a heat map that uses size in addition to color.\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, color = r, size = abs(r))) + \n  geom_point() + \n  theme_classic()\n\n\n\n\nImprovements\nWe’re going to skip the steps we took with a heat map. So close! Just need to get rid of that size legend.\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r, size = abs(r))) + \n  geom_point(shape = 21) + \n  scale_fill_gradient2(limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  scale_size_continuous(range = c(3,14)) + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    , subtitle = \"Sample 1\"\n    ) + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )\n\n\n\n\nLegend\n\nTo do this, we’ll use the guides() function!\n\n\nCoder_data$r[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r, size = abs(r))) + \n  geom_point(shape = 21) + \n  scale_fill_gradient2(limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  scale_size_continuous(range = c(3,14)) + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    , subtitle = \"Sample 1\"\n    ) + \n  guides(size = \"none\") + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )"
  },
  {
    "objectID": "associations.html#models-broom-tidy",
    "href": "associations.html#models-broom-tidy",
    "title": "Visualizing Associations and Models",
    "section": "Models + broom: tidy()\n",
    "text": "Models + broom: tidy()\n\n\nOutside of dplyr/tidyr, tidy() is a close contender with purrr::map() functions as my most used function\nWhy?\n\nWhen you run a model, base R provides the summary(), coef(), etc. to extract various components of the model\nBut these aren’t data.frames, which are core input to a lot of other R functions across packages\n\ntidy() provides a data frame with core model coefficients, inferential tests, etc. that be easily matched and merged across models, etc.\n\n\nBut with logistic regression with a logit link, we are left with coefficents that have to be interpreted in log odds, which realistically, almost no one can do\nSo we have to “undo” the log, which you may remember can done by exponentiating the natural log (ln)\nBut we can directly exponentiate from the summary function because it’s the wrong class of object\nWe could just exponentiate the coefficients from the coef() function, but this still leaves us with the need to extract estimates of precision, like standard errors, confidence intervals, and more.\n\n\nCodecoef(m1)\n\n(Intercept)     p_value \n  0.2321341  -0.0934916 \n\n\n\nEnter broom::tidy()!\n\n\nCodetidy(m1)\n\n\n\n  \n\n\n\n\nEven better, we can easily get confidence intervals\n\n\nCodetidy(m1, conf.int = T)\n\n\n\n  \n\n\n\nMultiple Parameter Plots\n\nBut when would you ever want to create a plot of just two parameters? Maybe never, but what if we wanted to do it for all 6 samples?\nWatch! Let’s make a nested data frame that will hold\n\nAll the data for each sample\nA model for each sample\nThe tidy() data frame of the parmeter estimates for each sample\n\n\n\n\nCodetidy_ci <- function(m) tidy(m, conf.int = T)\n\nnested_m <- pred_data %>%\n  group_by(study) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(\n    m = map(data, ~glm(o_value ~p_value, data = ., family = binomial(link = \"logit\")))\n    , tidy = map(m, tidy_ci)\n  )\nnested_m\n\n\n\n  \n\n\n\nNow, we’ll drop the data and m columns that we don’t need and unnest() our tidy() data frames\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy)\n\n\n\n  \n\n\n\nBasic Plot\nNow these parameters from multiple models, we may want to plot!\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  ggplot(\n    aes(y = study, x = estimate)\n  ) + \n    geom_errorbar(\n      aes(xmin = conf.low, xmax = conf.high)\n      , position = position_dodge(width = .9)\n      , width = .1\n      ) + \n    geom_point() + \n    theme_classic()\n\n\n\n\nFaceting\nAlmost, but we have two parameters for each model (Intercept and p_value), so let’s split those in a facet:\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  ggplot(\n    aes(y = study, x = estimate)\n  ) + \n    geom_errorbar(\n      aes(xmin = conf.low, xmax = conf.high)\n      , position = position_dodge(width = .9)\n      , width = .1\n      ) + \n    geom_point() + \n    facet_grid(~term) + \n    theme_classic()\n\n\n\n\nWe’ve got some work to do to make this an intuitive figure. Let’s: + Add a dashed line at 1 (odd ratio of 1 is a null effect) + Make the points bigger + Fix the titles on the plot and axis titles + Add some color + Fiddle with themes to make it prettier\nNull Comparison\nAdd a dashed line at 1 (odd ratio of 1 is a null effect)\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  ggplot(\n    aes(y = study, x = estimate)\n  ) + \n    geom_vline(aes(xintercept = 1), linetype = \"dashed\") + \n    geom_errorbar(\n      aes(xmin = conf.low, xmax = conf.high)\n      , position = position_dodge(width = .9)\n      , width = .1\n      ) + \n    geom_point() + \n    facet_grid(~term, scales = \"free\") + \n    theme_classic()\n\n\n\n\nPoint Size\n\nMake the points bigger\n\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  ggplot(\n    aes(y = study, x = estimate)\n  ) + \n    geom_vline(aes(xintercept = 1), linetype = \"dashed\") + \n    geom_errorbar(\n      aes(xmin = conf.low, xmax = conf.high)\n      , position = position_dodge(width = .9)\n      , width = .1\n      ) + \n    geom_point(size = 3, shape = 15) + \n    facet_grid(~term, scales = \"free\") + \n    theme_classic()\n\n\n\n\nTitles\n\nFix the titles on the plot and axis titles\n\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  ggplot(\n    aes(y = study, x = estimate)\n  ) + \n    geom_vline(aes(xintercept = 1), linetype = \"dashed\") + \n    geom_errorbar(\n      aes(xmin = conf.low, xmax = conf.high)\n      , position = position_dodge(width = .9)\n      , width = .1\n      ) + \n    geom_point(size = 3, shape = 15) + \n    labs(\n      x = \"Estimate (CI) in OR\"\n      , y = NULL\n      , title = \"Conscientiousness was associated with mortality 50% of samples\"\n      , subtitle = \"Samples with lower mortality risk overall had fewer significant associations\"\n      ) + \n    facet_grid(~term, scales = \"free\") + \n    theme_classic()\n\n\n\n\nColor and Themes\nAdd some color Fiddle with themes to make it prettier\n\nCodenested_m %>%\n  select(study, tidy) %>%\n  unnest(tidy) %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) %>%\n  ggplot(\n    aes(y = study, x = estimate, fill = study)\n  ) + \n    geom_vline(aes(xintercept = 1), linetype = \"dashed\") + \n    geom_errorbar(\n      aes(xmin = conf.low, xmax = conf.high)\n      , position = position_dodge(width = .9)\n      , width = .1\n      ) + \n    geom_point(size = 3, shape = 22) + \n    labs(\n      x = \"Estimate (CI) in OR\"\n      , y = NULL\n      , title = \"Conscientiousness was associated with mortality 50% of samples\"\n      , subtitle = \"Samples with lower mortality risk overall had fewer significant associations\"\n      ) + \n    facet_grid(~term, scales = \"free\") + \n    theme_classic() + \n    theme(\n      legend.position = \"none\"\n      , axis.text = element_text(face = \"bold\", size = rel(1.1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.2))\n      , axis.line = element_blank()\n      , strip.text = element_text(face = \"bold\", size = rel(1.1), color = \"white\")\n      , strip.background = element_rect(fill = \"black\")\n      , plot.title = element_text(face = \"bold\", size = rel(1.1), hjust = .5)\n      , plot.subtitle = element_text(face = \"italic\", size = rel(1.1))\n      , panel.border = element_rect(color = \"black\", fill = NA, size = 1)\n    )\n\n\n\n\n\nThis isn’t perfect. But we’re going to come back to this kind of plot when we talk about “piecing plots together.”\nPersonally, I would:\n\nAdd text with Est. (CI) and N for each sample in the figure\nBuild both of these separately in order to order by effect size\nThen put them back together and re-add the title"
  },
  {
    "objectID": "associations.html#models-broom-glance",
    "href": "associations.html#models-broom-glance",
    "title": "Visualizing Associations and Models",
    "section": "Models + broom: glance()\n",
    "text": "Models + broom: glance()\n\n\nWhen we run models, we need to care about more that just point and interval estimates\nOften we are interested in comparing models, checking diagnostics, etc.\nAgain, all of these are embedded (mostly), in the model objects\nThe glance() function brings some of these important ones into a single object\nHere’s what it gives us for our logistic regression model\n\n\nCodeglance(m1)\n\n\n\n  \n\n\n\n\nLet’s also look for al linear model, which may be more familiar for many of you:\n\n\nCodem2 <- lm(SRhealth ~ age, data = ds1)\nglance(m2)\n\n\n\n  \n\n\n\nAs before, we can do this with lots of models to compare across samples:\n\nCodenested_m <- nested_m %>%\n  mutate(glance = map(m, glance))\nnested_m\n\n\n\n  \n\n\n\n\nCodenested_m %>%\n  select(study, glance) %>%\n  unnest(glance)\n\n\n\n  \n\n\n\nRealistically, this is the kind of info we table, but we can also merge it with info from tidy:\n\nCodenested_m %>%\n  select(-data, -m) %>%\n  unnest(tidy) %>% \n  unnest(glance) %>%\n  mutate_if(is.numeric, ~round(., 2))\n\n\n\n  \n\n\n\n\nDiagnostics are not just summary statistics!\nWe care a lot about prediction, too\n\nResiduals both tell us unexplained variance (i.e. how observed data deviate from model predictions)\nModel predictions and prediction intervals tell us about how our model is doing across levels our variables\n\n\n\nLet’s keep working with our nested data frame. Remember, it looks like this:\n\nCodenested_m"
  },
  {
    "objectID": "associations.html#models-broom-augment",
    "href": "associations.html#models-broom-augment",
    "title": "Visualizing Associations and Models",
    "section": "Models + broom: augment()\n",
    "text": "Models + broom: augment()\n\n\n\naugment() let’s us add (augment) the raw data we feed the model based on the fitted model\nNotice we now have more columns\n\n\nCodenested_m <- nested_m %>%\n  mutate(data = map2(m, data, augment, se_fit = T))\nnested_m\n\n\n\n  \n\n\n\n\nglm() + augment()\n\n\nHere’s the columns we used along with the additional columns with a glm:\n\n\n.fitted: fitted / predicted value\n\n.se.fit: standard error\n\n.resid: observed - fitted\n\n.std.resd: standardized residuals\n\n.sigma: estimated residual SD when this obs is dropped from model\n\ncooksd: Cooks distance (is this an outlier?)\n\n\n\n\nCodenested_m$data[[1]] %>%\n  select(o_value, SID, p_value, .fitted:.cooksd)\n\n\n\n  \n\n\n\n\nlm() + augment()\n\nFor the most part, many of the checks with glm’s and lm’s are the same. But it’s a bit easier to wrap your head around lm(), so let’s switch to that:\n\nCodenested_lm <- pred_data %>%\n  select(study, SID, p_value, age, SRhealth) %>%\n  drop_na() %>%\n  group_by(study) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(m = map(data, ~lm(SRhealth ~ p_value + age, data = .))\n         , tidy = map(m, tidy_ci)\n         , glance = map(m, glance)\n         , data = map2(m, data, augment, se_fit = T, interval = \"confidence\"))\nnested_lm\n\n\n\n  \n\n\n\n\nHere’s the columns we used along with the additional columns with an lm:\n\n\n.fitted: fitted / predicted value\n\n.se.fit: standard error\n\n.lower: lower bound of the confidence/prediction interval\n\n.upper: upper bound of the confidence/prediction interval\n\n.resid: observed - fitted\n\n.std.resd: standardized residuals\n\n.sigma: estimated residual SD when this obs is dropped from model\n\ncooksd: Cooks distance (is this an outlier?)\n\n\nOne standard diagnostic plot is to plot fitted values v residuals\nLooks a little wonky (remember, these are results from multiple harmonized studies)\n\n\nCodenested_lm %>%\n  select(study, data) %>%\n  unnest(data) %>%\n  ggplot(aes(\n    x = .fitted\n    , y = .resid\n  )) + \n  geom_point() + \n  theme_classic()\n\n\n\n\nPlotting augment() Diagnostics\n\nOne standard diagnostic plot is to plot fitted values v residuals\nLooks a little wonky (remember, these are results from multiple harmonized studies)\n\n\nCodenested_lm %>%\n  select(study, data) %>%\n  unnest(data) %>%\n  ggplot(aes(\n    x = .fitted\n    , y = .resid\n  )) + \n  geom_point() +\n  labs(\n    x = \"Model Fitted Values\"\n    , y = \"Residual\") +\n  facet_wrap(~study) + \n  theme_classic()"
  },
  {
    "objectID": "associations.html#models-broom-augment-1",
    "href": "associations.html#models-broom-augment-1",
    "title": "Visualizing Associations and Models",
    "section": "Models + broom: augment()\n",
    "text": "Models + broom: augment()\n\nAnother is raw v. fitted\n\nCodenested_lm %>%\n  select(study, data) %>%\n  unnest(data) %>%\n  ggplot(aes(\n    x = p_value\n    , y = .resid\n  )) + \n  geom_point() +\n  facet_wrap(~study) + \n  theme_classic()\n\n\n\n\nModel Predictions\n\nAlthough we can get the standard error of the prediction for each person, we often want to look at theoretical predictions, adjusting for covariates. We can typically use built-in predict() or fitted() functions\nTo do this, we need to see theoretical ranges of key variables and grab averages of covariates\nI use functions for this. We’ll do one and build\n\n\nCodem1 <- nested_lm$m[[1]]\nd1 <- m1$model\n\ncrossing(\n  p_value = seq(0, 10, length.out = 100)\n  , age = mean(d1$age)\n) %>%\n  bind_cols(\n    .\n    , predict(m1, newdata = ., interval = \"prediction\")\n  )\n\n\n\n  \n\n\n\nPlotting That\n\nCodecrossing(\n  p_value = seq(0, 10, .1)\n  , age = mean(d1$age)\n) %>%\n  bind_cols(\n    .\n    , predict(m1, newdata = ., interval = \"prediction\")\n  ) %>%\n  ggplot(aes(x = p_value, y = fit)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"seagreen4\", alpha = .2) + \n    geom_line(color = \"seagreen4\", size = 2) + \n    theme_classic()\n\n\n\n\n\nThis is fine, but it could use some improvements:\n\nbetter scales\nraw data\nthe usual aesthetics\n\n\nBetter scales\n\nCodecrossing(\n  p_value = seq(0, 10, .1)\n  , age = mean(d1$age)\n) %>%\n  bind_cols(\n    .\n    , predict(m1, newdata = ., interval = \"prediction\")\n  ) %>%\n  ggplot(aes(x = p_value, y = fit)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"seagreen4\", alpha = .2) + \n    geom_line(color = \"seagreen4\", size = 2) + \n    scale_x_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    scale_y_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    theme_classic()\n\n\n\n\nRaw Data\n\nCodecrossing(\n  p_value = seq(0, 10, .1)\n  , age = mean(d1$age)\n) %>%\n  bind_cols(., predict(m1, newdata = ., interval = \"prediction\")) %>%\n  ggplot(aes(x = p_value, y = fit)) + \n    geom_point(\n      data = d1\n      , aes(x = p_value, y = SRhealth)\n      , alpha = .4\n      , color = \"seagreen4\"\n      ) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"seagreen4\", alpha = .2) + \n    geom_line(color = \"seagreen4\", size = 2) + \n    scale_x_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    scale_y_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    theme_classic()\n\n\n\n\nThe usual aesthetics\n\nCodecrossing(\n  p_value = seq(0, 10, .1)\n  , age = mean(d1$age)\n) %>%\n  bind_cols(., predict(m1, newdata = ., interval = \"prediction\")) %>%\n  ggplot(aes(x = p_value, y = fit)) + \n    geom_point(data = d1, aes(x = p_value, y = SRhealth)\n      , alpha = .4, color = \"seagreen4\") + \n    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"seagreen4\", alpha = .2) + \n    geom_line(color = \"seagreen4\", size = 2) + \n    scale_x_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    scale_y_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Predicted Self-Rated Health (POMP; 0-10)\"\n      , title = \"Conscientiousness and Self-Rated Health\\nWere Weakly Associated\"\n      ) + \n    theme_classic() + \n    theme(\n      axis.text = element_text(face = \"bold\", size = rel(1.1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.1))\n      , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n      )\n\n\n\n\nNow let’s do it for all of the samples\n\nCodepred_fun <- function(m){\n  d <- m$model\n\n  crossing(\n    p_value = seq(0, 10, length.out = 100)\n    , age = mean(d$age)\n  ) %>%\n    bind_cols(\n      .\n      , predict(m, newdata = ., interval = \"prediction\")\n    )\n}\n\nnested_lm <- nested_lm %>%\n  mutate(pred = map(m, pred_fun))\nnested_lm\n\n\n\n  \n\n\n\nNow let’s do it for all of the samples\n\nCodenested_lm %>%\n  select(study, pred) %>%\n  unnest(pred)\n\n\n\n  \n\n\n\n\nNow let’s do it for all of the samples\nVery close, but our intervals are cutoff\n\n\nCodenested_lm %>%\n  select(study, pred) %>%\n  unnest(pred) %>%\n  ggplot(aes(x = p_value, y = fit)) + \n    geom_point(data = d1, aes(x = p_value, y = SRhealth)\n      , alpha = .2, color = \"seagreen4\") + \n    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"seagreen4\", alpha = .2) + \n    geom_line(color = \"seagreen4\", size = 2) + \n    scale_x_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    scale_y_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Predicted Self-Rated Health (POMP; 0-10)\"\n      , title = \"Conscientiousness and Self-Rated Health\\nWere Weakly Associated In Most Samples\"\n      ) + \n    facet_wrap(~study, ncol = 2) + \n    theme_classic() + \n    theme(\n      axis.text = element_text(face = \"bold\", size = rel(1.1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.1))\n      , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n      )\n\n\n\n\n\nNow let’s do it for all of the samples\nVery close, but our intervals are cutoff\n\n\nCodenested_lm %>%\n  select(study, pred) %>%\n  unnest(pred) %>%\n  mutate(upr = ifelse(upr > 10, 10, upr)\n         , lwr = ifelse(lwr < 0, 0, lwr)) %>%\n  ggplot(aes(x = p_value, y = fit)) + \n    geom_point(data = d1, aes(x = p_value, y = SRhealth)\n      , alpha = .2, color = \"seagreen4\") + \n    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"seagreen4\", alpha = .2) + \n    geom_line(color = \"seagreen4\", size = 2) + \n    scale_x_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    scale_y_continuous(limits = c(0,10.2), breaks = seq(0,10,2)) + \n    labs(\n      x = \"Conscientiousness (POMP; 0-10)\"\n      , y = \"Predicted Self-Rated Health (POMP; 0-10)\"\n      , title = \"Conscientiousness and Self-Rated Health\\nWere Weakly Associated In Most Samples\"\n      ) + \n    facet_wrap(~study, ncol = 2) + \n    theme_classic() + \n    theme(\n      axis.text = element_text(face = \"bold\", size = rel(1.1))\n      , axis.title = element_text(face = \"bold\", size = rel(1.1))\n      , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n      , strip.background = element_rect(fill = \"darkseagreen4\")\n      , strip.text = element_text(face = \"bold\", color = \"white\")\n      )"
  },
  {
    "objectID": "associations.html#wrapping-up",
    "href": "associations.html#wrapping-up",
    "title": "Visualizing Associations and Models",
    "section": "Wrapping Up",
    "text": "Wrapping Up\n\nThis is a quick introduction to visualizing associations and working with models\nHere, we focused on doing things very manually to promote understanding\nBut there are lots of packages to automate much of this"
  }
]